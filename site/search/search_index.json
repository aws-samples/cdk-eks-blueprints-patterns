{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EKS Blueprints Patterns \u00b6 Welcome to the EKS Blueprints Patterns repository. This repository contains a number of samples for how you can leverage the Amazon EKS Blueprints . You can think of the patterns as \"codified\" reference architectures, which can be explained and executed as code in the customer environment. Patterns \u00b6 The individual patterns can be found in the lib directory. Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information. Documentation \u00b6 Please refer to the Amazon EKS Blueprints Quick Start documentation site for complete project documentation. Usage \u00b6 Before proceeding, make sure AWS CLI is installed on your machine. To use the eks-blueprints and patterns module, you must have Node.js and npm installed. You will also use make to simplify build and other common actions. Mac Setup: \u00b6 Follow the below steps to setup and leverage eks-blueprints and eks-blueprints-patterns in your local Mac laptop. Install make and node using brew brew install make brew install node Install npm sudo npm install -g n sudo n stable Make sure the following pre-requisites are met: Node version must be 16.x or above. $ node -v v18.12.1 Update (provided Node version manager is installed): n stable . May require sudo . NPM version must be 8.4 or above: $ npm -v 8 .19.2 Updating npm: sudo n stable where stable can also be a specific version above 8.4. May require sudo . Clone the eks-blueprints-patterns repository git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it. Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns. Install project dependencies. make deps To view patterns that are available to be deployed, execute the following: npm i make build To list the existing CDK EKS Blueprints patterns npx cdk list Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns. bottlerocket-blueprint emrOnEks-blueprint fargate-blueprint generic-cluster-blueprint kasten-blueprint keptn-blueprint kubecost-blueprint kubeflow-blueprint multi-team-blueprint newrelic-cluster-blueprint snyk-monitor-blueprint starter-blueprint Bootstrap your CDK environment. npx cdk bootstrap You can then deploy a specific pattern with the following: npx cdk deploy multi-team-blueprint Deploying Blueprints with External Dependency on AWS Resources \u00b6 There are cases when the blueprints defined in the patterns have dependencies on existing AWS Resources such as Secrets defined in the account/region. For such cases, you may see errors if such resources are not defined. For PipelineMultiEnvGitops please see instructions in this README . For MultiRegionConstruct the pattern relies on the following secrets defined: github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url . The secret is expected to be defined in us-east-1 and replicated to us-east-2 and us-west-2 regions. For more information on SSH credentials setup see ArgoCD Secrets Support . Example Structure: { \"sshPrivateKey\": \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\", \"url\": \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey. argo-admin-secret - must contain ArgoCD admin password in Plain Text. The secret is expected to be defined in us-east-1 and replicated to us-east-1 and us-west-2 regions. For `Dynatrace One Agent dynatrace-tokens - must contain API_URL , API_TOKEN and PAAS_TOKEN in Plain Text. The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication). For KeptnConstruct the pattern relies on the following secrets defined: keptn-secrets - must contain API_TOKEN and BRIDGE_PASSWORD password in Plain Text. The secret is expected to be defined in us-east-1 region. For NewRelicConstruct the pattern relies on the following secrets defined: newrelic-pixie-keys - must contain New Relic (required) and Pixie keys (optional). The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication). For more information on defining secrets for ArgoCD, please refer to Blueprints Documentation as well as known issues . For NginxIngressConstruct please see NGINX Blueprint documentation . For DatadogConstruct the pattern relies on the following secret defined: apiKeyAWSSecret - must contain the Datadog API key in Plain Text named datadog-api-key . The secret is expected to be defined in the target region. For KubeflowConstruct please see Kubeflow documentation . For SecureIngressCognitoConstruct please see Secure Ingress using Cognito Blueprint documentation . Security \u00b6 See CONTRIBUTING for more information. License \u00b6 This library is licensed under the MIT-0 License. See the LICENSE file.","title":"Overview"},{"location":"#eks-blueprints-patterns","text":"Welcome to the EKS Blueprints Patterns repository. This repository contains a number of samples for how you can leverage the Amazon EKS Blueprints . You can think of the patterns as \"codified\" reference architectures, which can be explained and executed as code in the customer environment.","title":"EKS Blueprints Patterns"},{"location":"#patterns","text":"The individual patterns can be found in the lib directory. Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information.","title":"Patterns"},{"location":"#documentation","text":"Please refer to the Amazon EKS Blueprints Quick Start documentation site for complete project documentation.","title":"Documentation"},{"location":"#usage","text":"Before proceeding, make sure AWS CLI is installed on your machine. To use the eks-blueprints and patterns module, you must have Node.js and npm installed. You will also use make to simplify build and other common actions.","title":"Usage"},{"location":"#mac-setup","text":"Follow the below steps to setup and leverage eks-blueprints and eks-blueprints-patterns in your local Mac laptop. Install make and node using brew brew install make brew install node Install npm sudo npm install -g n sudo n stable Make sure the following pre-requisites are met: Node version must be 16.x or above. $ node -v v18.12.1 Update (provided Node version manager is installed): n stable . May require sudo . NPM version must be 8.4 or above: $ npm -v 8 .19.2 Updating npm: sudo n stable where stable can also be a specific version above 8.4. May require sudo . Clone the eks-blueprints-patterns repository git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it. Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns. Install project dependencies. make deps To view patterns that are available to be deployed, execute the following: npm i make build To list the existing CDK EKS Blueprints patterns npx cdk list Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns. bottlerocket-blueprint emrOnEks-blueprint fargate-blueprint generic-cluster-blueprint kasten-blueprint keptn-blueprint kubecost-blueprint kubeflow-blueprint multi-team-blueprint newrelic-cluster-blueprint snyk-monitor-blueprint starter-blueprint Bootstrap your CDK environment. npx cdk bootstrap You can then deploy a specific pattern with the following: npx cdk deploy multi-team-blueprint","title":"Mac Setup:"},{"location":"#deploying-blueprints-with-external-dependency-on-aws-resources","text":"There are cases when the blueprints defined in the patterns have dependencies on existing AWS Resources such as Secrets defined in the account/region. For such cases, you may see errors if such resources are not defined. For PipelineMultiEnvGitops please see instructions in this README . For MultiRegionConstruct the pattern relies on the following secrets defined: github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url . The secret is expected to be defined in us-east-1 and replicated to us-east-2 and us-west-2 regions. For more information on SSH credentials setup see ArgoCD Secrets Support . Example Structure: { \"sshPrivateKey\": \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\", \"url\": \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey. argo-admin-secret - must contain ArgoCD admin password in Plain Text. The secret is expected to be defined in us-east-1 and replicated to us-east-1 and us-west-2 regions. For `Dynatrace One Agent dynatrace-tokens - must contain API_URL , API_TOKEN and PAAS_TOKEN in Plain Text. The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication). For KeptnConstruct the pattern relies on the following secrets defined: keptn-secrets - must contain API_TOKEN and BRIDGE_PASSWORD password in Plain Text. The secret is expected to be defined in us-east-1 region. For NewRelicConstruct the pattern relies on the following secrets defined: newrelic-pixie-keys - must contain New Relic (required) and Pixie keys (optional). The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication). For more information on defining secrets for ArgoCD, please refer to Blueprints Documentation as well as known issues . For NginxIngressConstruct please see NGINX Blueprint documentation . For DatadogConstruct the pattern relies on the following secret defined: apiKeyAWSSecret - must contain the Datadog API key in Plain Text named datadog-api-key . The secret is expected to be defined in the target region. For KubeflowConstruct please see Kubeflow documentation . For SecureIngressCognitoConstruct please see Secure Ingress using Cognito Blueprint documentation .","title":"Deploying Blueprints with External Dependency on AWS Resources"},{"location":"#security","text":"See CONTRIBUTING for more information.","title":"Security"},{"location":"#license","text":"This library is licensed under the MIT-0 License. See the LICENSE file.","title":"License"},{"location":"patterns/kubeflow/","text":"Kubeflow on EKS \u00b6 The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow. This pattern deploys the following resources: Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group Deploys supporting add-ons: ClusterAutoScaler, AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy, EbsCsiDriver, CertManagerAddOn, KubeStateMetricsAddOn, PrometheusNodeExporterAddOn, AdotCollectorAddOn, AmpAddOn, Deploy Kubeflow on the EKS cluster Prerequisites: \u00b6 Ensure that you have installed the following tools on your machine. aws cli kubectl cdk npm Deploy EKS Cluster with Amazon EKS Blueprints for CDK \u00b6 Clone the repository git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git Updating npm npm install -g npm@latest To view patterns and deploy kubeflow pattern cdk list cdk bootstrap cdk deploy kubeflow-blueprint Verify the resources \u00b6 Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access aws eks update-kubeconfig --name <your cluster name> --region <your region> --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO Let\u2019s verify the resources created by Steps above. kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | kubeflow # Output shows kubeflow namespace kubectl get pods --namespace = kubeflow-pipelines # Output shows kubeflow pods Execute Machine learning jobs on Kubeflow \u00b6 log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service kubectl port-forward svc/ml-pipeline-ui 9000 :80 -n = kubeflow-pipelines and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/tutorials/ Cleanup \u00b6 To clean up your EKS Blueprints, run the following commands: cdk destroy kubeflow-blueprint Disclaimer \u00b6 This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext If you have any questions about the npm package or find any defect, please post in the source repo at https://github.com/season1946/eks-blueprints-cdk-kubeflow-extension","title":"Kubeflow Pattern"},{"location":"patterns/kubeflow/#kubeflow-on-eks","text":"The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow. This pattern deploys the following resources: Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group Deploys supporting add-ons: ClusterAutoScaler, AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy, EbsCsiDriver, CertManagerAddOn, KubeStateMetricsAddOn, PrometheusNodeExporterAddOn, AdotCollectorAddOn, AmpAddOn, Deploy Kubeflow on the EKS cluster","title":"Kubeflow on EKS"},{"location":"patterns/kubeflow/#prerequisites","text":"Ensure that you have installed the following tools on your machine. aws cli kubectl cdk npm","title":"Prerequisites:"},{"location":"patterns/kubeflow/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","text":"Clone the repository git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git Updating npm npm install -g npm@latest To view patterns and deploy kubeflow pattern cdk list cdk bootstrap cdk deploy kubeflow-blueprint","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK"},{"location":"patterns/kubeflow/#verify-the-resources","text":"Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access aws eks update-kubeconfig --name <your cluster name> --region <your region> --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO Let\u2019s verify the resources created by Steps above. kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | kubeflow # Output shows kubeflow namespace kubectl get pods --namespace = kubeflow-pipelines # Output shows kubeflow pods","title":"Verify the resources"},{"location":"patterns/kubeflow/#execute-machine-learning-jobs-on-kubeflow","text":"log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service kubectl port-forward svc/ml-pipeline-ui 9000 :80 -n = kubeflow-pipelines and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/tutorials/","title":"Execute Machine learning jobs on Kubeflow"},{"location":"patterns/kubeflow/#cleanup","text":"To clean up your EKS Blueprints, run the following commands: cdk destroy kubeflow-blueprint","title":"Cleanup"},{"location":"patterns/kubeflow/#disclaimer","text":"This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext If you have any questions about the npm package or find any defect, please post in the source repo at https://github.com/season1946/eks-blueprints-cdk-kubeflow-extension","title":"Disclaimer"},{"location":"patterns/multi-account-monitoring/","text":"Multi Account Open Source Observability Pattern. \u00b6 Architecture \u00b6 The following figure illustrates the architecture of the pattern we will be deploying for Multi Account Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana : Objective \u00b6 Deploying two production grade Amazon EKS cluster across 2 AWS Accounts ( Prod1, Prod2 account ) through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in an another AWS account (pipeline account). Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote write metrics to AMP workspace in Prod 1 AWS Account. Deploying ADOT add-on, CloudWatch add-on to Prod 1 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account. Configuring GitOps tooling (ArgoCD addon) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects. Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (4th AWS account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account. Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2. GitOps confguration \u00b6 For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. You can find the team-geordie configuration for this pattern in the workload repository under the folder team-geordie . Prerequisites \u00b6 AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation , or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO. An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka prodEnv1) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka prodEnv2) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Pipeline Account (aka pipelineEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka monitoringEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. Deploying \u00b6 Fork this repository to your GitHub organisation/user Clone your forked repository Install the AWS CDK Toolkit globally on your machine using npm install -g aws-cdk github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url in pipelineEnv account. This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support . github-token secret must be stored in AWS Secrets Manager for the GitHub pipeline in pipelineEnv account. For more information on how to set it up, please refer to the docs . The GitHub Personal Access Token should have these scopes: repo - to read the repository admin:repo_hook - if you plan to use webhooks (enabled by default) cdk-context secret secret must be stored as a plain text in the following format in AWS Secrets Manager for cdk context for all the 4 AWS accounts used by the solution in pipelineEnv account.. { \"context\": { \"prodEnv1\": { \"account\": \"111111111111\", \"region\": \"your_region\" }, \"prodEnv2\": { \"account\": \"222222222222\", \"region\": \"your_region\" }, \"pipelineEnv\": { \"account\": \"333333333333\", \"region\": \"your_region\" }, \"monitoringEnv\": { \"account\": \"444444444444\", \"region\": \"your_region\" } } } 7. Create the following IAM users and attach administrator policy to required accounts. IAM user pipeline-admin with administrator in Pipeline AWS Account IAM user prod1-admin with administrator in Prod 1 AWS Account IAM user prod2-admin with administrator in Prod 2 AWS Account IAM user mon-admin with administrator in Monitoring AWS Account IAM user team-geordi in Prod 1 and Prod 2 AWS Account IAM user team-platform in Prod 1 and Prod 2 AWS Account Install project dependencies by running npm install in the main folder of this cloned repository Bootstrap your 4 AWS Accounts using [deploying pipelines approach] (https://aws-quickstart.github.io/cdk-eks-blueprints/pipelines/#deploying-pipelines) in this link. If you have bootstrap already done, please remove those before doing this step. Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattern file source code and look for the declared const of gitOwner . Change it to your GitHub username. Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: npm run build npx cdk deploy multi-account-central-pipeline Now you can go to AWS CodePipeline console , and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments. The deployment automation will create ampPrometheusDataSourceRole with permissions to retrieve metrics from AMP in Prod 1 Account, cloudwatchDataSourceRole with permissions to retrieve metrics from CloudWatch in Prod 2 Account and amgWorkspaceIamRole in monitoring account to assume roles in Prod 1 and Prod 2 account for retrieving and visualizing metrics in Grafana. Next, manually follow the following steps from AWS Open Source blog : AWS SSO in the management account Query metrics in Monitoring account from Amazon Managed Prometheus workspace in Prod 1 Account Query metrics in the Monitoring account from Amazon CloudWatch in Prod 1 Account Validating Custom Metrics and Traces from ho11y App \u00b6 Run the below command in both clusters to generate traces to X-Ray and Amazon Managed Grafana Console out the sample ho11y app : frontend_pod=`kubectl get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 5000 ] ; do kubectl exec -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local; echo ; loop_counter=$[$loop_counter+1]; done Traces and Service Map screenshots from X-Ray Console \u00b6 Custom Metrics from ho11y App on Amazon Managed Grafana Console using AMP as data source \u00b6 Custom Metrics from ho11y App on Amazon Managed Grafana Console using CloudWatch as data source \u00b6 Notes \u00b6 This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern in Prod 1 and Prod 2 AWS Accounts. Make sure your account limits for EIP are increased to support additional 9 EIPs (1 per Subnets).","title":"Multi Account Observability Pattern"},{"location":"patterns/multi-account-monitoring/#multi-account-open-source-observability-pattern","text":"","title":"Multi Account Open Source Observability Pattern."},{"location":"patterns/multi-account-monitoring/#architecture","text":"The following figure illustrates the architecture of the pattern we will be deploying for Multi Account Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :","title":"Architecture"},{"location":"patterns/multi-account-monitoring/#objective","text":"Deploying two production grade Amazon EKS cluster across 2 AWS Accounts ( Prod1, Prod2 account ) through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in an another AWS account (pipeline account). Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote write metrics to AMP workspace in Prod 1 AWS Account. Deploying ADOT add-on, CloudWatch add-on to Prod 1 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account. Configuring GitOps tooling (ArgoCD addon) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects. Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (4th AWS account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account. Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.","title":"Objective"},{"location":"patterns/multi-account-monitoring/#gitops-confguration","text":"For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. You can find the team-geordie configuration for this pattern in the workload repository under the folder team-geordie .","title":"GitOps confguration"},{"location":"patterns/multi-account-monitoring/#prerequisites","text":"AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation , or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO. An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka prodEnv1) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka prodEnv2) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Pipeline Account (aka pipelineEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization. An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka monitoringEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.","title":"Prerequisites"},{"location":"patterns/multi-account-monitoring/#deploying","text":"Fork this repository to your GitHub organisation/user Clone your forked repository Install the AWS CDK Toolkit globally on your machine using npm install -g aws-cdk github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url in pipelineEnv account. This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support . github-token secret must be stored in AWS Secrets Manager for the GitHub pipeline in pipelineEnv account. For more information on how to set it up, please refer to the docs . The GitHub Personal Access Token should have these scopes: repo - to read the repository admin:repo_hook - if you plan to use webhooks (enabled by default) cdk-context secret secret must be stored as a plain text in the following format in AWS Secrets Manager for cdk context for all the 4 AWS accounts used by the solution in pipelineEnv account.. { \"context\": { \"prodEnv1\": { \"account\": \"111111111111\", \"region\": \"your_region\" }, \"prodEnv2\": { \"account\": \"222222222222\", \"region\": \"your_region\" }, \"pipelineEnv\": { \"account\": \"333333333333\", \"region\": \"your_region\" }, \"monitoringEnv\": { \"account\": \"444444444444\", \"region\": \"your_region\" } } } 7. Create the following IAM users and attach administrator policy to required accounts. IAM user pipeline-admin with administrator in Pipeline AWS Account IAM user prod1-admin with administrator in Prod 1 AWS Account IAM user prod2-admin with administrator in Prod 2 AWS Account IAM user mon-admin with administrator in Monitoring AWS Account IAM user team-geordi in Prod 1 and Prod 2 AWS Account IAM user team-platform in Prod 1 and Prod 2 AWS Account Install project dependencies by running npm install in the main folder of this cloned repository Bootstrap your 4 AWS Accounts using [deploying pipelines approach] (https://aws-quickstart.github.io/cdk-eks-blueprints/pipelines/#deploying-pipelines) in this link. If you have bootstrap already done, please remove those before doing this step. Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattern file source code and look for the declared const of gitOwner . Change it to your GitHub username. Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: npm run build npx cdk deploy multi-account-central-pipeline Now you can go to AWS CodePipeline console , and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments. The deployment automation will create ampPrometheusDataSourceRole with permissions to retrieve metrics from AMP in Prod 1 Account, cloudwatchDataSourceRole with permissions to retrieve metrics from CloudWatch in Prod 2 Account and amgWorkspaceIamRole in monitoring account to assume roles in Prod 1 and Prod 2 account for retrieving and visualizing metrics in Grafana. Next, manually follow the following steps from AWS Open Source blog : AWS SSO in the management account Query metrics in Monitoring account from Amazon Managed Prometheus workspace in Prod 1 Account Query metrics in the Monitoring account from Amazon CloudWatch in Prod 1 Account","title":"Deploying"},{"location":"patterns/multi-account-monitoring/#validating-custom-metrics-and-traces-from-ho11y-app","text":"Run the below command in both clusters to generate traces to X-Ray and Amazon Managed Grafana Console out the sample ho11y app : frontend_pod=`kubectl get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 5000 ] ; do kubectl exec -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local; echo ; loop_counter=$[$loop_counter+1]; done","title":"Validating Custom Metrics and Traces from ho11y App"},{"location":"patterns/multi-account-monitoring/#traces-and-service-map-screenshots-from-x-ray-console","text":"","title":"Traces and Service Map screenshots from X-Ray Console"},{"location":"patterns/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-amp-as-data-source","text":"","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using AMP as data source"},{"location":"patterns/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-cloudwatch-as-data-source","text":"","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using CloudWatch as data source"},{"location":"patterns/multi-account-monitoring/#notes","text":"This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern in Prod 1 and Prod 2 AWS Accounts. Make sure your account limits for EIP are increased to support additional 9 EIPs (1 per Subnets).","title":"Notes"},{"location":"patterns/nginx/","text":"NGINX Pattern \u00b6 Objective \u00b6 When setting up a target platform across multiple dimensions that question of ingress must be solved. Ideally, it should work in such as way that workloads provisioned on the target environments could be accessible via internet exposing sub-domains of some predefined global domain name. Communication with the workloads should leverage secure TLS protected Load balancer with proper public (or private) certificate. A single cluster will deploy workloads from multiple teams and each of them should be able to expose workloads routed to their corresponding namespace. So, teams are expected to define ingress objects. In addition, this approach should work not only for a single cluster, but also across multiple regions and environments. Approach \u00b6 Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. Sub-domains are then defined in the target accounts (let's call them workload accounts). Our blueprint will then include the following: NGINX ingress controller to enable teams to create/configure their ingress objects. External DNS to integrate NGINX and public-facing NLB with Route53. AWS Loadbalancer controller to provision an NLB instance with each cluster fronting the NGINX ingress. Deployed with a public certificate that will also be provisioned as part of the blueprint. Team onboarding that leverage the ingress capabilities through ArgoCD. Other popular add-ons. Prerequisites \u00b6 argo-admin-password secret must be defined as plain text (not key/value) in us-west-2 region. The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT). The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it DomainOperatorRole but you can choose any arbitrary name for it. Policies: arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess or alternatively you can provide arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess and arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess . Trust relationship to allow workload accounts to create subdomains (replace <WORKLOAD_ACCOUNT> with the actual value): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<WORKLOAD_ACCOUNT>:root\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in ~/.cdk.json in your home directory. Example settings: { \"context\": { \"parent.dns.account\": \"<PARENT_ACCOUNT>\", \"parent.hostedzone.name\": \"mycompany.a2z.com\", \"dev.subzone.name\": \"dev.mycompany.a2z.com\", } } Deploying \u00b6 Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including workloads with an example of team-specific ingress objects.","title":"Nginx Pattern"},{"location":"patterns/nginx/#nginx-pattern","text":"","title":"NGINX Pattern"},{"location":"patterns/nginx/#objective","text":"When setting up a target platform across multiple dimensions that question of ingress must be solved. Ideally, it should work in such as way that workloads provisioned on the target environments could be accessible via internet exposing sub-domains of some predefined global domain name. Communication with the workloads should leverage secure TLS protected Load balancer with proper public (or private) certificate. A single cluster will deploy workloads from multiple teams and each of them should be able to expose workloads routed to their corresponding namespace. So, teams are expected to define ingress objects. In addition, this approach should work not only for a single cluster, but also across multiple regions and environments.","title":"Objective"},{"location":"patterns/nginx/#approach","text":"Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. Sub-domains are then defined in the target accounts (let's call them workload accounts). Our blueprint will then include the following: NGINX ingress controller to enable teams to create/configure their ingress objects. External DNS to integrate NGINX and public-facing NLB with Route53. AWS Loadbalancer controller to provision an NLB instance with each cluster fronting the NGINX ingress. Deployed with a public certificate that will also be provisioned as part of the blueprint. Team onboarding that leverage the ingress capabilities through ArgoCD. Other popular add-ons.","title":"Approach"},{"location":"patterns/nginx/#prerequisites","text":"argo-admin-password secret must be defined as plain text (not key/value) in us-west-2 region. The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT). The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it DomainOperatorRole but you can choose any arbitrary name for it. Policies: arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess or alternatively you can provide arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess and arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess . Trust relationship to allow workload accounts to create subdomains (replace <WORKLOAD_ACCOUNT> with the actual value): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<WORKLOAD_ACCOUNT>:root\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in ~/.cdk.json in your home directory. Example settings: { \"context\": { \"parent.dns.account\": \"<PARENT_ACCOUNT>\", \"parent.hostedzone.name\": \"mycompany.a2z.com\", \"dev.subzone.name\": \"dev.mycompany.a2z.com\", } }","title":"Prerequisites"},{"location":"patterns/nginx/#deploying","text":"Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including workloads with an example of team-specific ingress objects.","title":"Deploying"},{"location":"patterns/pipeline-multi-env-gitops/","text":"Pipeline Multi Environment Pattern \u00b6 Objective \u00b6 Deploying an EKS cluster across 3 environments( dev, test, and prod ), with a Continuous Deployment pipeline triggered upon a commit to the repository that holds the pipeline configuration. Configuring GitOps tooling (ArgoCD addon) to support multi-team and multi-repositories configuration, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects GitOps confguration \u00b6 For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. The pattern uses the ECSDEMO applications as sample applications to demonstrate how to setup a GitOps configuration with multiple teams and multiple applications. The pattern include the following configurations in terms io: Application team - it defines 3 application teams that corresponds with the 3 sample applications used ArgoCD bootstrap - the pattern configure the ArgoCD addon to point to the workload repository of the EKS Blueprints samples ArgoCD projects - as part of the ArgoCD addon bootstrap, the pattern generate an ArgoCD project for each application team. The ArgoCD are used in order to restrict the deployment of an application to a specific target namespace You can find the App of Apps configuration for this pattern in the workload repository under the folder multi-repo . Prerequisites \u00b6 Fork this repository to your GitHub organisation/user Clone your forked repository Install the AWS CDK Toolkit globally on your machine using npm install -g aws-cdk github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url . This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support . github-token secret must be stored in AWS Secrets Manager for the GitHub pipeline. For more information on how to set it up, please refer to the docs . The GitHub Personal Access Token should have these scopes: repo - to read the repository admin:repo_hook - if you plan to use webhooks (enabled by default) Create the relevant users that will be used by the different teams aws iam create-user --user-name frontend-user aws iam create-user --user-name nodejs-user aws iam create-user --user-name crystal-user aws iam create-user --user-name platform-user Install project dependencies by running npm install in the main folder of this cloned repository In case you haven't done this before, bootstrap your AWS Account for AWS CDK use using: cdk bootstrap Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattenrn file source code and look for the declared const of gitOwner . Change it to your GitHub username. OPTIONAL - As mentioned above, this pattern uses another repository for GitOps. This is the ArgoCD App of Apps configuration that resides in the aws-samples organisation. If you would like to modify the App of Apps configuration and customise it to your needs, then use the following instructions: Fork the App of Apps workloads repo to your GitHub username Modify the pattern code with the following changes: Change the consts of devArgoAddonConfig , testArgoAddonConfig , and prodArgoAddonConfig to point to your GitHub username In the createArgoAddonConfig function, look for the git@github.com:aws-samples/eks-blueprints-workloads.git code under the sourceRepos configurations, and add another reference to your forked workload repository Deploying \u00b6 Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: npx cdk deploy eks-blueprint-pipeline-stack Now you can go to AWS CodePipeline console , and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments. Notes \u00b6 In case your pipeline fails on the first run, it's because that the AWS CodeBuild step needs elevated permissions at build time. This is described in the official docs . To reolve this, locate AccessDeniedException in the CodeBuild build logs, and attach the following inline policy to it: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"sts:AssumeRole\" , \"secretsmanager:GetSecretValue\" , \"secretsmanager:DescribeSecret\" , \"cloudformation:*\" ], \"Resource\" : \"*\" } ] } This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern. Make sure your account limit for EIP are increased to support additional 9 EIPs (1 per Subnets)","title":"Multi environment Pipeline GitOps pattern Pattern"},{"location":"patterns/pipeline-multi-env-gitops/#pipeline-multi-environment-pattern","text":"","title":"Pipeline Multi Environment Pattern"},{"location":"patterns/pipeline-multi-env-gitops/#objective","text":"Deploying an EKS cluster across 3 environments( dev, test, and prod ), with a Continuous Deployment pipeline triggered upon a commit to the repository that holds the pipeline configuration. Configuring GitOps tooling (ArgoCD addon) to support multi-team and multi-repositories configuration, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects","title":"Objective"},{"location":"patterns/pipeline-multi-env-gitops/#gitops-confguration","text":"For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. The pattern uses the ECSDEMO applications as sample applications to demonstrate how to setup a GitOps configuration with multiple teams and multiple applications. The pattern include the following configurations in terms io: Application team - it defines 3 application teams that corresponds with the 3 sample applications used ArgoCD bootstrap - the pattern configure the ArgoCD addon to point to the workload repository of the EKS Blueprints samples ArgoCD projects - as part of the ArgoCD addon bootstrap, the pattern generate an ArgoCD project for each application team. The ArgoCD are used in order to restrict the deployment of an application to a specific target namespace You can find the App of Apps configuration for this pattern in the workload repository under the folder multi-repo .","title":"GitOps confguration"},{"location":"patterns/pipeline-multi-env-gitops/#prerequisites","text":"Fork this repository to your GitHub organisation/user Clone your forked repository Install the AWS CDK Toolkit globally on your machine using npm install -g aws-cdk github-ssh-key - must contain GitHub SSH private key as a JSON structure containing fields sshPrivateKey and url . This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support . github-token secret must be stored in AWS Secrets Manager for the GitHub pipeline. For more information on how to set it up, please refer to the docs . The GitHub Personal Access Token should have these scopes: repo - to read the repository admin:repo_hook - if you plan to use webhooks (enabled by default) Create the relevant users that will be used by the different teams aws iam create-user --user-name frontend-user aws iam create-user --user-name nodejs-user aws iam create-user --user-name crystal-user aws iam create-user --user-name platform-user Install project dependencies by running npm install in the main folder of this cloned repository In case you haven't done this before, bootstrap your AWS Account for AWS CDK use using: cdk bootstrap Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattenrn file source code and look for the declared const of gitOwner . Change it to your GitHub username. OPTIONAL - As mentioned above, this pattern uses another repository for GitOps. This is the ArgoCD App of Apps configuration that resides in the aws-samples organisation. If you would like to modify the App of Apps configuration and customise it to your needs, then use the following instructions: Fork the App of Apps workloads repo to your GitHub username Modify the pattern code with the following changes: Change the consts of devArgoAddonConfig , testArgoAddonConfig , and prodArgoAddonConfig to point to your GitHub username In the createArgoAddonConfig function, look for the git@github.com:aws-samples/eks-blueprints-workloads.git code under the sourceRepos configurations, and add another reference to your forked workload repository","title":"Prerequisites"},{"location":"patterns/pipeline-multi-env-gitops/#deploying","text":"Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: npx cdk deploy eks-blueprint-pipeline-stack Now you can go to AWS CodePipeline console , and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments.","title":"Deploying"},{"location":"patterns/pipeline-multi-env-gitops/#notes","text":"In case your pipeline fails on the first run, it's because that the AWS CodeBuild step needs elevated permissions at build time. This is described in the official docs . To reolve this, locate AccessDeniedException in the CodeBuild build logs, and attach the following inline policy to it: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"sts:AssumeRole\" , \"secretsmanager:GetSecretValue\" , \"secretsmanager:DescribeSecret\" , \"cloudformation:*\" ], \"Resource\" : \"*\" } ] } This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern. Make sure your account limit for EIP are increased to support additional 9 EIPs (1 per Subnets)","title":"Notes"},{"location":"patterns/secureingresscognito/","text":"Secure Ingress using Cognito Pattern \u00b6 Objective \u00b6 The objective of this pattern is to provide a secure authentication mechanism for customer applications using Amazon Cognito, ALB, and Route53, ensuring that only authorized users can access the application. The Kubecost tool is used as a reference or sample implementation to demonstrate the pattern's capabilities. To achieve this objective, the pattern utilizes Amazon Cognito to provide user authentication for the application's ingress, with ALB's built-in support for user authentication handling routine tasks such as user sign-up, sign-in, and sign-out. In addition to Amazon Cognito, ALB integrates with any OpenID Connect compliant identity provider (IdP) for a single sign-on experience across applications. ACM and Route53 provide SSL/TLS certificates to secure connections to ALB and authenticate users, preventing sensitive information from being intercepted or tampered with during transmission. The pattern also leverages Kubecost to provide real-time cost visibility and analysis for Kubernetes clusters, enabling customers to make informed decisions about resource allocation and utilization. This pattern can be easily adapted and extended to secure ingress for any application, providing a unified and secure solution for user authentication while optimizing costs. By implementing this solution, Amazon EKS customers can have a reliable, scalable, and secure authentication mechanism for their applications, with a cost optimization tool to manage and reduce the costs associated with their Kubernetes clusters. Architecture \u00b6 Approach \u00b6 This blueprint will include the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. External-DNS allows integration of exposed Kubernetes services and Ingresses with DNS providers Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and bootstraping your workloads from public and private Git repositories. Create the necessary Cognito resources like user pool, user pool client, domain etc.., and passed to the Argo CD app of apps pattern from which ingress resources can reference. GitOps confguration \u00b6 For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. Prerequisites \u00b6 The parent domain must exist. Deploying \u00b6 argo-admin-password secret must be defined as plain text (not key/value) in us-west-2 region. aws secretsmanager create-secret --name argo-admin-secret \\ --description \"Admin Password for ArgoCD\" \\ --secret-string \"password123$\" \\ --region \"us-west-2\" 2. The actual settings for the hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in ~/.cdk.json in your home directory. Example settings: Update the context in cdk.json file located in cdk-eks-blueprints-patterns directory \"context\": { \"parent.hostedzone.name\": \"mycompany.a2z.com\", \"dev.subzone.name\": \"dev.mycompany.a2z.com\" } Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: make build npx cdk deploy secure-ingress-blueprint Cluster Access \u00b6 Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: secure-ingress-blueprint.secureingressblueprintClusterNameD6A1BE5C = secure-ingress-blueprint secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 = aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 secure-ingress-blueprint.secureingressblueprintGetTokenCommand21BE2184 = aws eks get-token \u2014cluster-name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 Stack ARN: arn:aws:cloudformation:us-west-2:<ACCOUNT ID>:stack/secure-ingress-blueprint/64017120-91ce-11ed-93b2-0a67951f5d5d To update your Kubernetes config for your new cluster, copy and run the secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 command (the second command) in your terminal. aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 Validate that you now have kubectl access to your cluster via the following: kubectl get all -n kubecost You should see output that lists all namespaces in your cluster. Test authentication \u00b6 Point your browser to the URL of the Kubecost app in your cluster. You can get the URL from the cdk.json file using the below command. awk -F':' '/dev.subzone.name/ {print $2}' cdk.json | tr -d '\",' | xargs echo Your browser will be redirected to a sign-in page. This page is provided by Amazon Cognito hosted UI. Since this is your first time accessing the application, sign up as a new user. The data you input here will be saved in the Amazon Cognito user pool you created earlier in the post. Select \u201cSign up\u201d and use your email address and create a password Use the verification code received in your email and confirm the account. Once you sign in, ALB will send you to the Kubecost app\u2019s UI: Select the \u201cAWS Cluster #1\u201d to view the cost overview, savings and efficiency details.","title":"Secure Ingress and Authentication Pattern"},{"location":"patterns/secureingresscognito/#secure-ingress-using-cognito-pattern","text":"","title":"Secure Ingress using Cognito Pattern"},{"location":"patterns/secureingresscognito/#objective","text":"The objective of this pattern is to provide a secure authentication mechanism for customer applications using Amazon Cognito, ALB, and Route53, ensuring that only authorized users can access the application. The Kubecost tool is used as a reference or sample implementation to demonstrate the pattern's capabilities. To achieve this objective, the pattern utilizes Amazon Cognito to provide user authentication for the application's ingress, with ALB's built-in support for user authentication handling routine tasks such as user sign-up, sign-in, and sign-out. In addition to Amazon Cognito, ALB integrates with any OpenID Connect compliant identity provider (IdP) for a single sign-on experience across applications. ACM and Route53 provide SSL/TLS certificates to secure connections to ALB and authenticate users, preventing sensitive information from being intercepted or tampered with during transmission. The pattern also leverages Kubecost to provide real-time cost visibility and analysis for Kubernetes clusters, enabling customers to make informed decisions about resource allocation and utilization. This pattern can be easily adapted and extended to secure ingress for any application, providing a unified and secure solution for user authentication while optimizing costs. By implementing this solution, Amazon EKS customers can have a reliable, scalable, and secure authentication mechanism for their applications, with a cost optimization tool to manage and reduce the costs associated with their Kubernetes clusters.","title":"Objective"},{"location":"patterns/secureingresscognito/#architecture","text":"","title":"Architecture"},{"location":"patterns/secureingresscognito/#approach","text":"This blueprint will include the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. External-DNS allows integration of exposed Kubernetes services and Ingresses with DNS providers Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and bootstraping your workloads from public and private Git repositories. Create the necessary Cognito resources like user pool, user pool client, domain etc.., and passed to the Argo CD app of apps pattern from which ingress resources can reference.","title":"Approach"},{"location":"patterns/secureingresscognito/#gitops-confguration","text":"For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository.","title":"GitOps confguration"},{"location":"patterns/secureingresscognito/#prerequisites","text":"The parent domain must exist.","title":"Prerequisites"},{"location":"patterns/secureingresscognito/#deploying","text":"argo-admin-password secret must be defined as plain text (not key/value) in us-west-2 region. aws secretsmanager create-secret --name argo-admin-secret \\ --description \"Admin Password for ArgoCD\" \\ --secret-string \"password123$\" \\ --region \"us-west-2\" 2. The actual settings for the hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in ~/.cdk.json in your home directory. Example settings: Update the context in cdk.json file located in cdk-eks-blueprints-patterns directory \"context\": { \"parent.hostedzone.name\": \"mycompany.a2z.com\", \"dev.subzone.name\": \"dev.mycompany.a2z.com\" } Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack: make build npx cdk deploy secure-ingress-blueprint","title":"Deploying"},{"location":"patterns/secureingresscognito/#cluster-access","text":"Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: secure-ingress-blueprint.secureingressblueprintClusterNameD6A1BE5C = secure-ingress-blueprint secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 = aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 secure-ingress-blueprint.secureingressblueprintGetTokenCommand21BE2184 = aws eks get-token \u2014cluster-name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 Stack ARN: arn:aws:cloudformation:us-west-2:<ACCOUNT ID>:stack/secure-ingress-blueprint/64017120-91ce-11ed-93b2-0a67951f5d5d To update your Kubernetes config for your new cluster, copy and run the secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 command (the second command) in your terminal. aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::<ACCOUNT ID>:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0 Validate that you now have kubectl access to your cluster via the following: kubectl get all -n kubecost You should see output that lists all namespaces in your cluster.","title":"Cluster Access"},{"location":"patterns/secureingresscognito/#test-authentication","text":"Point your browser to the URL of the Kubecost app in your cluster. You can get the URL from the cdk.json file using the below command. awk -F':' '/dev.subzone.name/ {print $2}' cdk.json | tr -d '\",' | xargs echo Your browser will be redirected to a sign-in page. This page is provided by Amazon Cognito hosted UI. Since this is your first time accessing the application, sign up as a new user. The data you input here will be saved in the Amazon Cognito user pool you created earlier in the post. Select \u201cSign up\u201d and use your email address and create a password Use the verification code received in your email and confirm the account. Once you sign in, ALB will send you to the Kubecost app\u2019s UI: Select the \u201cAWS Cluster #1\u201d to view the cost overview, savings and efficiency details.","title":"Test authentication"}]}