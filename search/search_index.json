{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EKS Blueprints Patterns","text":"<p>Welcome to the <code>EKS Blueprints Patterns</code> repository.</p> <p>This repository contains a number of samples for how you can leverage the Amazon EKS Blueprints. You can think of the patterns as \"codified\" reference architectures, which can be explained and executed as code in the customer environment.</p>"},{"location":"#patterns","title":"Patterns","text":"<p>The individual patterns can be found in the <code>lib</code> directory. Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Please refer to the Amazon EKS Blueprints Patterns documentation site for complete list of Amazon EKS Blueprints patterns documentation.</p> <p>Please refer to the Amazon EKS Blueprints Quick Start documentation site for complete project documentation.</p>"},{"location":"#usage","title":"Usage","text":"<p>Before proceeding, make sure AWS CLI is installed on your machine.</p> <p>To use the eks-blueprints and patterns module, you must have Node.js and npm installed. You will also use <code>make</code> and <code>brew</code> to simplify build and other common actions.</p>"},{"location":"#workstation-setup-options","title":"Workstation Setup Options","text":""},{"location":"#devcontainer-setup","title":"DevContainer Setup","text":"<p>Users can choose this option, if you dont want to run this solution on a mac or ubuntu machine. Please use the dev container configuration in the <code>.devcontainer</code> folder with devpod or any other dev container environment to create a development environment with dependencies such as Node, NPM, aws-cli, aws-cdk, kubectl, helm dependencies for your local development with <code>cdk-eks-blueprints-patterns</code> solution. </p>"},{"location":"#rhel-setup","title":"RHEL Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your Amazon Linux/CentOS/RHEL Linux machine.</p> <ol> <li> <p>Update the package list</p> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo yum update\n</code></pre> </li> <li> <p>Install <code>make</code></p> <pre><code>sudo yum install make\n</code></pre> </li> <li> <p>Install <code>brew</code> by following instructions as detailed in docs.brew.sh</p> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Verify brew installation</p> <pre><code>brew -v\n</code></pre> </li> <li> <p>Install <code>Node.js</code> and <code>npm</code></p> <pre><code>Install Node.js v18 and npm using brew.\n\n```bash\nbrew install node@18\n```\n\nNote: Node.js package includes npm\n\nSet PATH for node@18\n\n```bash\ntest -r ~/.bash_profile &amp;&amp; echo 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.profile\nexport PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"\n```\n</code></pre> <p>Post completing the above, continue from Verify Node.js and npm Installation</p> </li> </ol>"},{"location":"#ubuntu-setup","title":"Ubuntu Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your Ubuntu Linux machine.</p> <ol> <li>Update the package list</li> </ol> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo apt update\n</code></pre> <ol> <li>Install <code>make</code></li> </ol> <pre><code>sudo apt install make\n</code></pre> <ol> <li>Install <code>brew</code> by following instructions as detailed in docs.brew.sh</li> </ol> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Verify brew installation</p> <pre><code>brew -v\n</code></pre> <ol> <li>Install <code>Node.js</code> and <code>npm</code></li> </ol> <p>Install Node.js v18 and npm using brew.</p> <pre><code>brew install node@18\n</code></pre> <p>Note: Node.js package includes npm</p> <p>Set PATH for node@18</p> <pre><code>test -r ~/.bash_profile &amp;&amp; echo 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.profile\nexport PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"\n</code></pre> <p>Post completing the above, continue from Verify Node.js and npm Installation</p>"},{"location":"#mac-setup","title":"Mac Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your local Mac laptop.</p> <ol> <li>Install <code>make</code>, <code>node</code> and <code>npm</code> using brew</li> </ol> <pre><code>brew install make\nbrew install node@18\n</code></pre> <p>Note: Node.js package includes npm</p> <p>Set PATH for node@18</p> <pre><code>echo 'export PATH=\"/opt/homebrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.zshrc\nexport PATH=\"/opt/homebrew/opt/node@18/bin:$PATH\"\n</code></pre>"},{"location":"#verify-nodejs-and-npm-installation","title":"Verify <code>Node.js</code> and <code>npm</code> Installation","text":"<ol> <li>Check the installed version of Node.js:</li> </ol> <pre><code>node -v\n</code></pre> <p>The output should be <code>v18.x.x</code>.</p> <ol> <li>Check the installed version of npm</li> </ol> <pre><code>npm -v\n</code></pre> <p>The output should be a version greater than <code>9.x.x</code>.</p> <p>If your npm version is not <code>9.x.x</code> or above, update npm with the following command:</p> <pre><code>sudo npm install -g npm@latest\n</code></pre> <p>Verify the installed version by running <code>npm -v</code>.</p>"},{"location":"#repo-setup","title":"Repo setup","text":"<ol> <li>Clone <code>cdk-eks-blueprints-patterns</code> repository</li> </ol> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it.</p> <ol> <li> <p>Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns.</p> </li> <li> <p>Install project dependencies.</p> </li> </ol> <pre><code>make deps\n</code></pre> <ul> <li>To view patterns that are available to be deployed, execute the following:</li> </ul> <pre><code>npm i\nmake build\n</code></pre> <ul> <li>To list the existing CDK EKS Blueprints patterns</li> </ul> <pre><code>make list\n</code></pre> <p>Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns.</p> <pre><code>To work with patterns use:\n        $ make pattern &lt;pattern-name&gt; &lt;list | deploy | synth | destroy&gt;\nExample:\n        $ make pattern fargate deploy\n\nPatterns:\n\n        bottlerocket\n        data-at-rest\n        datadog\n        dynatrace-operator\n        ecr-image-scanning\n        emr\n        fargate\n        generative-ai-showcase\n        generic-cluster-provider\n        guardduty\n        jupyterhub\n        kasten\n        keptn-control-plane\n        konveyor\n        kubecost\n        kubeflow\n        kubeshark\n        multi-region\n        multi-team\n        newrelic\n        nginx\n        pipeline-multienv-gitops\n        pipeline-multienv-monitoring\n        pipeline\n        rafay\n        secure-ingress-cognito\n        snyk\n        starter\n        gmaestro\n        workloads-codecommit\n</code></pre> <ul> <li>Bootstrap your CDK environment.</li> </ul> <pre><code>npx cdk bootstrap\n</code></pre> <ul> <li>You can then deploy a specific pattern with the following:</li> </ul> <pre><code>make pattern multi-team deploy\n</code></pre>"},{"location":"#developer-flow","title":"Developer Flow","text":""},{"location":"#modifications","title":"Modifications","text":"<p>All files are compiled to the dist folder including <code>lib</code> and <code>bin</code> directories. For iterative development (e.g. if you make a change to any of the patterns) make sure to run compile:</p> <pre><code>make compile\n</code></pre> <p>The <code>compile</code> command is optimized to build only modified files and is fast.</p>"},{"location":"#new-patterns","title":"New Patterns","text":"<p>To create a new pattern, please follow these steps:</p> <ol> <li>Under lib create a folder for your pattern, such as <code>&lt;pattern-name&gt;-construct</code>. If you plan to create a set of patterns that represent a particular subdomain, e.g. <code>security</code> or <code>hardening</code>, please create an issue to discuss it first. If approved, you will be able to create a folder with your subdomain name and group your pattern constructs under it.</li> <li>Blueprints generally don't require a specific class, however we use a convention of wrapping each pattern in a plain class like <code>&lt;Pattern-Name&gt;Construct</code>. This class is generally placed in <code>index.ts</code> under your pattern folder.</li> <li>Once the pattern implementation is ready, you need to include it in the list of the patterns by creating a file <code>bin/&lt;pattern-name&gt;.ts</code>. The implementation of this file is very light, and it is done to allow patterns to run independently.</li> </ol> <p>Example simple synchronous pattern:</p> <pre><code>import { configureApp } from \"../lib/common/construct-utils\";\nimport FargateConstruct from \"../lib/fargate-construct\";\n\nnew FargateConstruct(configureApp(), \"fargate\"); // configureApp() will create app and configure loggers and perform other prep steps\n</code></pre> <ol> <li>In some cases, patterns need to use async APIs. For example, they may rely on external secrets that you want to validate ahead of the pattern deployment.</li> </ol> <p>Example async pattern:</p> <pre><code>import { configureApp, errorHandler } from \"../lib/common/construct-utils\";\n\nconst app = configureApp();\n\nnew NginxIngressConstruct().buildAsync(app, \"nginx\").catch((e) =&gt; {\n  errorHandler(\n    app,\n    \"NGINX Ingress pattern is not setup. This maybe due to missing secrets for ArgoCD admin pwd.\",\n    e\n  );\n});\n</code></pre> <ol> <li>There are a few utility functions that can be used in the pattern implementation such as secret prevalidation. This function will fail if the corresponding secret is not defined, this preventing the pattern to deploy.</li> </ol> <pre><code>await prevalidateSecrets(\n  NginxIngressConstruct.name,\n  undefined,\n  SECRET_ARGO_ADMIN_PWD\n);\nawait prevalidateSecrets(\"my-pattern-name\", \"us-east-1\", \"my-secret-name\"); //\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>See Contributing guide for requirements on contribution.</p>"},{"location":"#deploying-blueprints-with-external-dependency-on-aws-resources","title":"Deploying Blueprints with External Dependency on AWS Resources","text":"<p>There are cases when the blueprints defined in the patterns have dependencies on existing AWS Resources such as Secrets defined in the account/region. For such cases, you may see errors if such resources are not defined.</p> <p>For <code>PipelineMultiEnvGitops</code> please see instructions in this README.</p> <p>For <code>MultiRegionConstruct</code> the pattern relies on the following secrets defined:</p> <ol> <li><code>github-ssh-key</code> - must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code>. The secret is expected to be defined in <code>us-east-1</code> and replicated to <code>us-east-2</code> and <code>us-west-2</code> regions. For more information on SSH credentials setup see ArgoCD Secrets Support.    Example Structure:</li> </ol> <pre><code>{\n    \"sshPrivateKey\": \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\",\n\n    \"url\": \"git@github\"\n}\n</code></pre> <p>Note: You can notice explicit \\n characters in the sshPrivateKey.</p> <ol> <li><code>argo-admin-secret</code> - must contain ArgoCD admin password in Plain Text. The secret is expected to be defined in <code>us-east-1</code> and replicated to <code>us-east-1</code> and <code>us-west-2</code> regions.</li> </ol> <p>For <code>`Dynatrace One Agent</code></p> <ul> <li><code>dynatrace-tokens</code> - must contain API_URL, API_TOKEN and PAAS_TOKEN in Plain Text. The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication).</li> </ul> <p>For <code>keptn-control-plane</code> the pattern relies on the following secrets defined:</p> <ul> <li><code>keptn-secrets</code> - must contain API_TOKEN and BRIDGE_PASSWORD password in Plain Text. The secret is expected to be defined in <code>us-east-1</code> region.</li> </ul> <p>For <code>newrelic</code> the pattern relies on the following secrets defined:</p> <ul> <li><code>newrelic-pixie-keys</code> - must contain New Relic (required) and Pixie keys (optional). The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication).</li> </ul> <p>For more information on defining secrets for ArgoCD, please refer to Blueprints Documentation as well as known issues.</p> <p>For <code>nginx</code> please see NGINX Blueprint documentation.</p> <p>For <code>datadog</code> the pattern relies on the following secret defined:</p> <ul> <li><code>apiKeyAWSSecret</code> - must contain the Datadog API key in Plain Text named <code>datadog-api-key</code>. The secret is expected to be defined in the target region.</li> </ul> <p>For <code>kubeflow</code> please see Kubeflow documentation.</p> <p>For <code>secure-ingress-cognito</code> please see Secure Ingress using Cognito Blueprint documentation.</p> <p>For <code>GmaestroConstruct</code> the pattern relies on the following secret defined:</p> <ul> <li><code>granulate-client-id</code> - must contain the client_id Plain Text. The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication).</li> </ul>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"patterns/backstage/","title":"Backstage on EKS","text":""},{"location":"patterns/backstage/#objective","title":"Objective","text":"<p>Backstage is an application that aims to facilitate introduction and maintenance of standards and best practices, across the organization, tying all infrastructure tooling, resources, owners, contributors, and administrators together in one place.</p> <p>The base functionality is provided by the Core component, which is assembled together with Plugins into an Application. Plugins extend the Core with additional functionalities that can be open source, or proprietary to a company.</p> <p>The objective of this pattern is to illustrate how to deploy a Backstage pre-built Docker image, using the Amazon EKS Blueprints Backstage add-on.</p>"},{"location":"patterns/backstage/#architecture","title":"Architecture","text":""},{"location":"patterns/backstage/#approach","title":"Approach","text":"<p>This blueprint will include the following:</p> <ul> <li>A new Well-Architected VPC with both Public and Private subnets</li> <li>A new Well-Architected EKS cluster in the region and account you specify</li> <li>An Application Load Balancer (ALB), implementing the Backstage Ingress rules</li> <li>An Amazon RDS for PostgreSQL instance</li> <li>A certificate, assigned to the ALB</li> <li>A Secret in AWS Secrets Manager, storing the database credentials, imported into the cluster via ExternalsSecretsAddOn</li> <li>Other popular add-ons</li> </ul>"},{"location":"patterns/backstage/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ul> <li>aws cli (also ensure it is configured)</li> <li>cdk</li> <li>npm</li> <li>tsc</li> <li>make</li> <li>Docker</li> </ul> <p>Let\u2019s start by setting the account and region environment variables:</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=$(aws configure get region)\n</code></pre> <p>Create the Backstage application, command reported here for your convenience:</p> <pre><code>npx @backstage/create-app@latest\n</code></pre> <p>Build the corresponding Docker image, commands reported here for your convenience:</p> <pre><code>cd ./backstage\nyarn install --frozen-lockfile\nyarn tsc\nyarn build:backend --config app-config.yaml\n</code></pre> <p>Note: if the above command throws an error caused by app-config.yaml not found, you can explicitly set the path to the file:</p> <p><pre><code>yarn build:backend --config $(pwd)/app-config.yaml\n</code></pre> Then you can progress with the docker image build:</p> <pre><code>docker image build . -f packages/backend/Dockerfile --tag backstage\n</code></pre> <p>Note: consider the platform you are building on, and the target platform the image will run on, you might want to use the --platform option, e.g.:</p> <pre><code>docker buildx build ... --platform=...\n</code></pre> <p>Note: If you are running a version of Docker Engine version earlier than 23.0, you might need to enable BuildKit manually, like explained in the Getting Started section of the BuildKit webpage.</p> <p>(Optional) to show examples on the UI, add to Docker file:</p> <pre><code>COPY --chown=node:node examples /examples\n</code></pre> <p>Create an Amazon Elastic Container Registry (ECR) repository, named backstage:</p> <pre><code>aws ecr create-repository --repository-name backstage\n</code></pre> <pre><code>DOCKER_IMAGE_ID=... #see output of image id from above image creation\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\ndocker tag $DOCKER_IMAGE_ID $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/backstage:latest\ndocker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/backstage:latest\n</code></pre> <p>Setup a Hosted Zone in Route 53, with your parent domain. The pattern will create a new subdomain with format {backstage subdomain label}.{parent domain}. The default value for {backstage subdomain label} is backstage (see parameters below).</p>"},{"location":"patterns/backstage/#deployment","title":"Deployment","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file (edit PARENT_DOMAIN_NAME as it fits):</p> <pre><code>PARENT_DOMAIN_NAME=example.com\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $PARENT_DOMAIN_NAME --query \"HostedZones[].Id\" --output text | xargs basename)\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"backstage.image.registry.name\": \"${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com\",\n        \"backstage.parent.domain.name\":\"${PARENT_DOMAIN_NAME}\",\n        \"backstage.hosted.zone.id\": \"${HOSTED_ZONE_ID}\"\n      }\n}\nEOF\n</code></pre> <p>(Optional) The full list of parameters you can set in the context is:</p> <pre><code>    \"context\": {\n        \"backstage.namespace.name\": ...,\n        \"backstage.image.registry.name\": ...,\n        \"backstage.image.repository.name\": ...,\n        \"backstage.image.tag.name\": ...,\n        \"backstage.parent.domain.name\": ...,\n        \"backstage.subdomain.label\": ...,\n        \"backstage.hosted.zone.id\": ...,\n        \"backstage.certificate.resource.name\": ...,\n        \"backstage.database.resource.name\": ...,\n        \"backstage.database.instance.port\": ...,\n        \"backstage.database.secret.resource.name\": ...,\n        \"backstage.database.username\": ...,\n        \"backstage.database.secret.target.name\": ...,\n      }\n</code></pre> <p>You can assign values to the above keys according to the following criteria (values are required where you don't see default mentioned):</p> <ul> <li>\"backstage.namespace.name\": Backstage's namespace, the default is \"backstage\"</li> <li>\"backstage.image.registry.name\": the image registry for the Backstage Helm chart in Amazon ECR, a value similar to \"youraccount.dkr.ecr.yourregion.amazonaws.com\"</li> <li>\"backstage.image.repository.name\": the image repository for the Backstage Helm chart, the default is \"backstage\"</li> <li>\"backstage.image.tag.name\": the image tag, the default is \"latest\"</li> <li>\"backstage.parent.domain.name\": the parent domain in your Hosted Zone</li> <li>\"backstage.subdomain.label\": to be used as {\"subdomain.label\"}.{\"parent.domain.name\"}, the default is \"backstage\"</li> <li>\"backstage.hosted.zone.id\": the Hosted zone ID (format: 20x chars/numbers)</li> <li>\"backstage.certificate.resource.name\": resource name of the certificate, registered by the resource provider, the default is \"backstage-certificate\"</li> <li>\"backstage.database.resource.name\": resource name of the database, registered by the resource provider, the default is \"backstage-database\"</li> <li>\"backstage.database.instance.port\": the port the database will use, the default is 5432</li> <li>\"backstage.database.secret.resource.name\": resource name of the database's Secret, registered by the resource provider, the default is \"backstage-database-credentials\"</li> <li>\"backstage.database.username\": the username for the database's credentials, the default is \"postgres\"</li> <li>\"backstage.database.secret.target.name\": the name to be used when creating the Secret, the default is \"backstage-database-secret\"</li> </ul> <p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Run the following commands:</p> <p><pre><code>make deps\nmake build\nmake pattern backstage deploy\n</code></pre> When deployment completes, the output will be similar to the following:</p> <p></p> <p>Navigate to the URL indicated by the first line in the output (_backstage-blueprint.BackstagebaseURL ...), you should see the screen below:</p> <p></p> <p>To see the deployed resources within the cluster, please run:</p> <pre><code>kubectl get pod,svc,secrets,ingress -A\n</code></pre> <p>A sample output is shown below:</p> <p></p>"},{"location":"patterns/backstage/#next-steps","title":"Next steps","text":"<p>You can go the AWS Blog to explore how to use Backstage e.g., as an API Developer Portal for Amazon API Gateway or to provision infrastructure using AWS Proton. On the Backstage website you can also see other examples of how to use and expand Backstage.</p>"},{"location":"patterns/backstage/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern backstage destroy \n</code></pre>"},{"location":"patterns/batch/","title":"AWS Batch on Amazon EKS Pattern","text":""},{"location":"patterns/batch/#objective","title":"Objective","text":"<p>AWS Batch helps you run batch computing workloads on AWS. Using Amazon EKS as the compute resource, you can now schedule and scale batch workloads into new or existing EKS cluster. As part of the deployment, AWS Batch doesn't create, administer, or perform lifecycle operations of the EKS cluster, but will only scale up and down the nodes maanged by AWS Batch and run pods on those nodes to complete batch jobs. </p> <p>The objective of this pattern is to deploy AWS Batch on Amazon EKS using EKS Blueprints with the following features in place: - Batch addon implemented - Batch Team defined with a sample compute environment and job queue (as defined under <code>lib/teams/team-batch</code>) - This can be customized based on your needs - Fluent Bit addon implemented to monitor AWS Batch on Amazon EKS jobs using CloudWatch, with the proper permissions for sending logs</p>"},{"location":"patterns/crossplane-argocd-gitops/","title":"GitOps based Multi-cluster add-on and Apps Management using Crossplane and ArgoCD","text":""},{"location":"patterns/crossplane-argocd-gitops/#objective","title":"Objective","text":"<p>The objective of this pattern is to provide centralized management of Amazon EKS add-ons, Kubernetes Applications and Helm charts in workload clusters. This approach consists of a Management Cluster and multiple workload clusters. The Management Cluster is created with ArgoCD and Crossplane add-ons. The platform team creates Crossplane Manifest files for Amazon EKS add-ons/Kubernetes Applications/Helm charts and pushes them to the GitOps Repo. The ArgoCD Application Controller in the Management Cluster reconciles these Crossplane Manifests and deploy them into Management Cluster.  The Crossplane Controller in the Management Cluster deploys the Amazon EKS add-ons/Kubernetes Applications/Helm charts into the Workload Clusters.</p> <p>This helps platform teams to simplify the process of deploying add-ons and Apps from a central Management Cluster. In this Solution, we use CDK to deploy AWS CodePipeline which monitors this platform repo and deploy the Management and Workload Clusters using CDK EKS Blueprints.</p>"},{"location":"patterns/crossplane-argocd-gitops/#architecture","title":"Architecture","text":""},{"location":"patterns/crossplane-argocd-gitops/#approach","title":"Approach","text":"<p>This blueprint will include the following:</p> <ul> <li>AWS CodePipeline which deploys the Management and Workload Clusters</li> <li>A new Well-Architected EKS cluster <code>eks-mgmt-cluster</code> and two workload EKS Clusters <code>workload-amd-1-29-blueprint</code> and <code>workload-arm-1-29-blueprint</code> in the region and account you specify.</li> <li>Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS.</li> <li>The Management Cluster is deployed with the following add-ons.<ul> <li>Upbound Universal Crossplane Provider</li> <li>Upbound AWS Family Crossplane Provider</li> <li>Upbound AWS EKS Crossplane Provider</li> <li>Kubernetes Crossplane Provider</li> <li>Helm Crossplane Provider</li> <li>Secrets Store AddOn</li> <li>ArgoCD add-on</li> </ul> </li> <li>The ArgoCD add-on is bootstrapped with GitOps which contains Crossplane Manifest files to deploy EKS add-ons, Kubernetes Manifests and also Helm Charts.</li> </ul>"},{"location":"patterns/crossplane-argocd-gitops/#gitops-configuration","title":"GitOps Configuration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD add-on and points to the EKS Blueprints Workload sample repository.</p>"},{"location":"patterns/crossplane-argocd-gitops/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>helm</li> <li>GitHub Access Token for this repo and AWS secret</li> </ol>"},{"location":"patterns/crossplane-argocd-gitops/#create-aws-secret-manager-secret","title":"Create AWS Secret Manager Secret","text":"<p>Create a plain-text Amazon secret to hold a fine-grained GitHub access token for this repo in the desired region, and set its name as a value to the GITHUB_SECRET environment variable. Default value is <code>cdk_blueprints_github_secret</code>.</p> <p>WARNING: When switching the CDK between region, remember to replicate this secret!!!!</p> <pre><code>export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)\nexport AWS_REGION=\"us-west-2\"\nexport CDK_REPO_GITHUB_PAT_TOKEN=&lt;set_token_here&gt;\nexport CDK_REPO_AWS_SECRET_NAME=\"cdk_blueprints_github_secret\"\naws secretsmanager create-secret --region $AWS_REGION \\\n    --name $CDK_REPO_AWS_SECRET_NAME \\\n    --description \"GitHub Personal Access Token for CodePipeline to access GitHub account\" \\\n    --secret-string $CDK_REPO_GITHUB_PAT_TOKEN\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#deploy","title":"Deploy","text":"<ol> <li>Clone the repository and install dependency packages. This repository contains CDK v2 code written in TypeScript.</li> </ol> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\nnpm i\n</code></pre> <ol> <li>Execute the commands below to bootstrap the AWS environment</li> </ol> <pre><code>cdk bootstrap aws://$ACCOUNT_ID/$AWS_REGION\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make clean\nmake build\nmake list\nmake pattern crossplane-argocd-gitops deploy\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#cluster-access","title":"Cluster Access","text":""},{"location":"patterns/crossplane-argocd-gitops/#view-the-codepipeline","title":"View the CodePipeline","text":""},{"location":"patterns/crossplane-argocd-gitops/#access-the-management-eks-cluster","title":"Access the Management EKS cluster","text":"<p>In this section, let us create a kube-context for the Management cluster and ensure that the ArgoCD and Crossplane add-ons are deployed successfully.</p> <ol> <li>Run the below command to get the AWS command from CloudFormation Stack <code>eks-mgmt-cluster-stage-eks-mgmt-cluster-stage-blueprint</code> outputs</li> </ol> <p>The example command looks like below.</p> <pre><code>export CFNOutputKey=$(aws cloudformation describe-stacks \\\n    --stack-name eks-mgmt-cluster-stage-eks-mgmt-cluster-stage-blueprint \\\n    --query 'Stacks[].Outputs[].OutputKey' | jq -r '.[]|select(. | startswith(\"mgmtclusterstageblueprintConfigCommand\"))')\necho $CFNOutputKey\n\nexport mgmtclusterstageblueprintConfigCommand=$(aws cloudformation describe-stacks \\\n    --stack-name eks-mgmt-cluster-stage-eks-mgmt-cluster-stage-blueprint \\\n    --query 'Stacks[].Outputs[?OutputKey==`'$CFNOutputKey'`].OutputValue' \\\n     --output text)\necho  $mgmtclusterstageblueprintConfigCommand\n</code></pre> <ol> <li>Run below command to create the kube-context for the Management cluster.</li> </ol> <pre><code>$mgmtclusterstageblueprintConfigCommand\n</code></pre> <p>The output will look like below.</p> <pre><code>Updated context arn:aws:eks:us-west-2:ACCOUNT_ID:cluster/eks-eks-mgmt-cluster in /Users/&lt;user_name&gt;/.kube/config\n</code></pre> <ol> <li>Copy the context in the output above and set an environment variable </li> </ol> <pre><code>export MANAGEMENT_CLUSTER_CONTEXT=\"arn:aws:eks:${AWS_REGION}:${ACCOUNT_ID}:cluster/eks-eks-mgmt-cluster\"\necho \"export  MANAGEMENT_CLUSTER_CONTEXT=${MANAGEMENT_CLUSTER_CONTEXT}\" &gt;&gt; ~/.bash_profile\n</code></pre> <ol> <li>Run below command to validate the access to the cluster</li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get node\n</code></pre> <p>The output will like below.</p> <pre><code>NAME                           STATUS   ROLES    AGE   VERSION\nip-10-0-137-3.ec2.internal     Ready    &lt;none&gt;   18h   v1.29.6-eks-1552ad0\nip-10-0-169-194.ec2.internal   Ready    &lt;none&gt;   18h   v1.29.6-eks-1552ad0\n</code></pre> <ol> <li>Run below command to get the list of Crossplane Providers deployed in the cluster</li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get providers.pkg.crossplane.io\n</code></pre> <p>The output will like below.</p> <p><pre><code>NAME                          INSTALLED   HEALTHY   PACKAGE                                                          AGE\nhelm-provider                 True        True      xpkg.upbound.io/crossplane-contrib/provider-helm:v0.19.0         18h\nkubernetes-provider           True        True      xpkg.upbound.io/crossplane-contrib/provider-kubernetes:v0.13.0   18h\nprovider-aws-eks              True        True      xpkg.upbound.io/upbound/provider-aws-eks:v1.1.0                  18h\nupbound-provider-family-aws   True        True      xpkg.upbound.io/upbound/provider-family-aws:v1.13.0 \n</code></pre> 6. Run below command to get the Crossplane Providers pods in the <code>upbound-system</code> Namespace.</p> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get pod -n upbound-system\n</code></pre> <p>The output will like below.</p> <p><pre><code>NAME                                                        READY   STATUS    RESTARTS   AGE\ncrossplane-594b65bfdb-pgkxf                                 1/1     Running   0          6d8h\ncrossplane-rbac-manager-86c74cf5d-tjcw8                     1/1     Running   0          6d8h\nhelm-provider-4d90a08b9ede-7c874b858b-pp26d                 1/1     Running   0          47h\nkubernetes-provider-a3cbbe355fa7-55846cfbfb-6tpcl           1/1     Running   0          25h\nprovider-aws-eks-23042d28ed58-66d9db8476-jr6mb              1/1     Running   0          6d8h\nupbound-provider-family-aws-bac5d48bd353-64845bdcbc-4vpn6   1/1     Running   0          6d8h            8d\n</code></pre> 7. Run below command to get the ArgoCD pods deployed in the <code>argocd</code> Namespace.</p> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get pod -n argocd\n</code></pre> <p>The output will like below.</p> <pre><code>NAME                                                              READY   STATUS    RESTARTS       AGE\nblueprints-addon-argocd-application-controller-0                  1/1     Running   0              24h\nblueprints-addon-argocd-applicationset-controller-7b78c7fc94ls9   1/1     Running   0              24h\nblueprints-addon-argocd-dex-server-6cf94ddc54-dfhv7               1/1     Running   0              24h\nblueprints-addon-argocd-notifications-controller-6f6b7d95cdd2tl   1/1     Running   0              24h\nblueprints-addon-argocd-redis-b8dbc7dc6-h4bs8                     1/1     Running   0              24h\nblueprints-addon-argocd-repo-server-fd57dc686-zkbsm               1/1     Running   0              4h15m\nblueprints-addon-argocd-server-84c8b597c9-98c95                   1/1     Running   0              24h\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#access-to-the-workload-clusters-using-iam-role-eks-workload-connector-role","title":"Access to the Workload clusters using IAM role <code>eks-workload-connector-role</code>","text":"<p>Note that we create and add an IAM role eks-workload-connector-role with system:masters RBAC access to both of the workload clusters i.e. workload-amd-1-29-blueprint and workload-arm-1-29-blueprint as part of the Stack creation.</p> <p>The Upbound AWS EKS Provider Pod will use its IRSA role to assume the <code>eks-workload-connector-role</code> to gain access to the workload clusters. The <code>sts:AssumeRole</code> IAM permission is already added to the IRSA role during the Management cluster creation.</p> <p>We will create two Crossplane objects of type <code>ClusterAuth</code> to create kube-context to access the Workload clusters using the IAM role <code>eks-workload-connector-role</code></p> <p>We will also create two Crossplane objects of type <code>Addon</code> to deploy Amazon EKS add-ons into the Workload clusters. To deploy add-ons, the AWS EKS Provider Pod needs <code>eks:*</code> IAM permissions, which are already added to <code>eks-workload-connector-role</code> during cluster creation.</p> <p>Note this IAM permissions can be made very granular to provide least privileged access to workload clusters.</p>"},{"location":"patterns/crossplane-argocd-gitops/#access-the-workload-eks-cluster-workload-amd-1-29-blueprint","title":"Access the Workload EKS cluster <code>workload-amd-1-29-blueprint</code>","text":"<p>In this section, let us create a kube-context and verify access to the Workload cluster <code>workload-amd-1-29-blueprint</code></p> <p>Note that we have added an IAM role eks-workload-connector-role with system:masters RBAC access to both of the workload clusters i.e. workload-amd-1-29-blueprint and workload-arm-1-29-blueprint.</p> <ol> <li>Run the command to create the kube-context for the cluster.</li> </ol> <pre><code>aws eks update-kubeconfig --name workload-amd-1-29-blueprint --region ${AWS_REGION} --role-arn \"arn:aws:iam::${ACCOUNT_ID}:role/eks-workload-connector-role\"\n</code></pre> <ol> <li>Copy the context in the output above and set an environment variable.</li> </ol> <p><pre><code>export WORKLOAD_CLUSTER1_CONTEXT=\"arn:aws:eks:${AWS_REGION}:${ACCOUNT_ID}:cluster/workload-amd-1-29-blueprint\"\necho \"export  WORKLOAD_CLUSTER1_CONTEXT=${WORKLOAD_CLUSTER1_CONTEXT}\" &gt;&gt; ~/.bash_profile\n</code></pre> 3. Run below command to validate the access to the cluster.</p> <pre><code>kubectl --context $WORKLOAD_CLUSTER1_CONTEXT get node\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#access-the-workload-eks-cluster-workload-arm-1-29-blueprint","title":"Access the Workload EKS cluster <code>workload-arm-1-29-blueprint</code>","text":"<p>In this section, let us create a kube-context and verify access to the Workload cluster <code>workload-arm-1-29-blueprint</code></p> <p>Note that we have added an IAM role eks-workload-connector-role with system:masters RBAC access to both of the workload clusters i.e. workload-amd-1-29-blueprint and workload-arm-1-29-blueprint.</p> <ol> <li>Run the command to create the kube-context for the cluster.</li> </ol> <p><pre><code>aws eks update-kubeconfig --name workload-arm-1-29-blueprint --region ${AWS_REGION} --role-arn \"arn:aws:iam::${ACCOUNT_ID}:role/eks-workload-connector-role\"\n</code></pre> 2. Copy the context in the output above and set an environment variable.</p> <p><pre><code>export WORKLOAD_CLUSTER2_CONTEXT=\"arn:aws:eks:${AWS_REGION}:${ACCOUNT_ID}:cluster/workload-arm-1-29-blueprint\"\necho \"export  WORKLOAD_CLUSTER2_CONTEXT=${WORKLOAD_CLUSTER1_CONTEXT}\" &gt;&gt; ~/.bash_profile\n</code></pre> 3. Run below command to validate the access to the cluster.</p> <pre><code>kubectl --context $WORKLOAD_CLUSTER1_CONTEXT get node\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#test","title":"Test","text":""},{"location":"patterns/crossplane-argocd-gitops/#install-the-argocd-cli","title":"Install the ArgoCD CLI","text":"<ol> <li> <p>Install the ArgoCD CLI as per the docs</p> </li> <li> <p>Get the ArgoCD Admin password using below command.</p> </li> </ol> <pre><code>kubectl --context $MANAGEMENT_CLUSTER_CONTEXT  -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <ol> <li>Open a New Terminal and Run a local proxy server for the ArgoCD Server.</li> </ol> <p><pre><code>kubectl --context $MANAGEMENT_CLUSTER_CONTEXT port-forward svc/blueprints-addon-argocd-server -n argocd 8080:443\n</code></pre> 4. In the current Terminal run the ArgoCD login command.</p> <pre><code>argocd login localhost:8080 --username admin --password &lt;admin_password&gt;\n</code></pre> <ol> <li>Add Management EKS cluster to ArgoCD.</li> </ol> <p><pre><code>argocd cluster add $MANAGEMENT_CLUSTER_CONTEXT\n</code></pre> The output will look like below.</p> <pre><code>WARNING: This will create a service account `argocd-manager` on the cluster referenced by context `arn:aws:eks:us-west-2:ACCOUNT_ID:cluster/eks-mgmt-cluster` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0004] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\" \nINFO[0004] ClusterRole \"argocd-manager-role\" updated    \nINFO[0005] ClusterRoleBinding \"argocd-manager-role-binding\" updated \nCluster 'https://0F745A41ECA76297CBF070C032932033.sk1.us-west-2.eks.amazonaws.com' added\n</code></pre> <ol> <li>Run the below command to get the list of ArgoCD Applications.</li> </ol> <pre><code>argocd app list\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                   CLUSTER                         NAMESPACE  PROJECT  STATUS  HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                        PATH                      TARGET\nargocd/bootstrap-apps  https://kubernetes.default.svc  argocd     default  Synced  Healthy  Auto-Prune  &lt;none&gt;      https://github.com/aws-samples/eks-blueprints-workloads  ./crossplane-argocd-gitops/envs/dev  main\nargocd/team-spock        https://kubernetes.default.svc  argocd     default  Synced  Healthy  Auto-Prune  &lt;none&gt;      https://github.com/aws-samples/eks-blueprints-workloads  ./teams/team-spock/dev        main\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#validate-eks-add-ons-deployment-in-workload-clusters","title":"Validate EKS add-ons deployment in Workload Clusters","text":"<ol> <li>Run the below command to get the list of <code>ProviderConfig</code> Crossplane CRD objects deployed in the Management cluster</li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get providerconfigs.aws.upbound.io\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                         AGE\ncommon-provider-config-aws   23h\n</code></pre> <ol> <li>Run the below command to get the list of <code>Addon</code> Objects deployed in the Management cluster. </li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get addons.eks.aws.upbound.io\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                    READY   SYNCED   EXTERNAL-NAME                               AGE\naddon-eks-pod-identity-agent-amd-1-29   True    True     workload-amd-1-29-blueprint:eks-pod-identity-agent   4h15m\naddon-eks-pod-identity-agent-arm-1-29   True    True     workload-arm-1-29-blueprint:eks-pod-identity-agent   4h15m\n</code></pre> <ol> <li>Go to the Workload EKS Clusters and Ensure that EKS add-on is deployed successfully.</li> </ol> <p></p> <p></p>"},{"location":"patterns/crossplane-argocd-gitops/#validate-kubernetes-manifests-deployment-in-workload-clusters","title":"Validate Kubernetes Manifests deployment in Workload clusters","text":"<ol> <li>Run the below command to get the list of Crossplane Kubernetes <code>ProviderConfig</code> objects deployed in the Management cluster.</li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get providerconfigs.kubernetes.crossplane.io\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                     AGE\nprovider-config-k8s-workload-amd-1-29-blueprint   4h31m\nprovider-config-k8s-workload-arm-1-29-blueprint   4h40m\n</code></pre> <ol> <li>Run the below command to get the list of Namespaces in the Workload cluster  <code>workload-amd-1-29-blueprint</code></li> </ol> <pre><code>kubectl  --context $WORKLOAD_CLUSTER1_CONTEXT get ns\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                STATUS   AGE\ndefault                             Active   8d\nexternal-secrets                    Active   8d\nkube-node-lease                     Active   8d\nkube-public                         Active   8d\nkube-system                         Active   8d\ntest-namespace-workload-amd-1-29-blueprint   Active   4h9m\n</code></pre> <ol> <li>Run the below command to get the list of Namespaces in the Workload cluster  <code>workload-arm-1-29-blueprint</code></li> </ol> <pre><code>kubectl  --context $WORKLOAD_CLUSTER2_CONTEXT get ns\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                STATUS   AGE\ndefault                             Active   8d\nexternal-secrets                    Active   8d\nkube-node-lease                     Active   8d\nkube-public                         Active   8d\nkube-system                         Active   8d\ntest-namespace-workload-arm-1-29-blueprint   Active   4h9m\n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#validate-helm-chart-deployment-in-workload-clusters","title":"Validate Helm Chart deployment in Workload clusters","text":"<ol> <li>Run the below command to get the list of Crossplane Helm Provider Objects deployed in the Management Cluster.</li> </ol> <pre><code>kubectl  --context $MANAGEMENT_CLUSTER_CONTEXT get providerconfigs.helm.crossplane.io\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                      AGE\nprovider-config-helm-workload-amd-1-29-blueprint   4h37m\nprovider-config-helm-workload-arm-1-29-blueprint   4h46m\n</code></pre> <ol> <li>Run the below command to get the list of helm charts in the Workload Cluster  <code>workload-amd-1-29-blueprint</code></li> </ol> <pre><code>helm  --kube-context $WORKLOAD_CLUSTER1_CONTEXT list -A\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                    NAMESPACE               REVISION        UPDATED                                  STATUS          CHART                   APP VERSION\nblueprints-addon-external-secrets       external-secrets        1               2024-05-07 05:25:31.465715836 +0000 UTC  deployed        external-secrets-0.9.9  v0.9.9     \ntest-helm-workload-amd-1-29-blueprint            default                 1               2024-05-15 06:39:17.325950143 +0000 UTC  deployed        nginx-17.0.1            1.26.0  \n</code></pre> <ol> <li>Run the below command to get the list of Helm Charts in the Workload cluster  <code>workload-arm-1-29-blueprint</code></li> </ol> <pre><code>helm  --kube-context $WORKLOAD_CLUSTER2_CONTEXT list -A\n</code></pre> <p>The output will look like below.</p> <pre><code>NAME                                    NAMESPACE               REVISION        UPDATED                                  STATUS          CHART                   APP VERSION\nblueprints-addon-external-secrets       external-secrets        1               2024-05-07 05:26:52.028907405 +0000 UTC  deployed        external-secrets-0.9.9  v0.9.9     \ntest-helm-workload-arm-1-29-blueprint            default                 1               2024-05-15 06:39:17.222351682 +0000 UTC  deployed        nginx-17.0.1            1.26.0   \n</code></pre>"},{"location":"patterns/crossplane-argocd-gitops/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <p><pre><code>make pattern crossplane-argocd-gitops destroy \n</code></pre> The above command deletes the AWS CodePipeline <code>crossplane-argocd-gitops</code>. However to complete the Cleanup, delete the following CloudFormation Stacks manually using AWS Console or AWS CLI using below commands.</p> <pre><code>aws cloudformation delete-stack --stack-name workload-amd-1-29-workload-amd-1-29-blueprint\naws cloudformation delete-stack --stack-name workload-arm-1-29-workload-arm-1-29-blueprint\naws cloudformation delete-stack --stack-name mgmt-cluster-stage-mgmt-cluster-stage-blueprint\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/","title":"Custom Networking on EKS","text":"<p>On Amazon EKS clusters, the default Container Networking Interface(CNI) is implemented by the Amazon VPC CNI plugin. When VPC CNI is used in EKS clusters, by default the VPC CNI assigns pods an IP address that's selected from the primary subnet of the VPC. The primary subnet is the subnet CIDR that the primary Elastic Network Interface(ENI) is attached to; usually it's the subnet of the worker node/host in the EKS cluster. If the primary subnet CIDR is too small, the CNI may not be able to have enough IP addresses to assign to the pods running in the cluster. This is a common challenge for EKS IPv4 clusters.</p> <p>Custom Networking provides a solution to the IP exhaustion issue by assigning the Pod IPs from secondary VPC address spaces(CIDR). When custom networking is enabled in VPC CNI, it creates secondary ENIs in the subnet defined under a custom resource named ENIConfig that includes an alternate subnet CIDR range (carved from a secondary VPC CIDR). The VPC CNI assigns pods IP addresses from the CIDR range defined in the ENIConfig Custom Resource Definition(CRD).</p> <p>Using the Custom Networking with IPv4 pattern, you should be able to stand up an EKS cluster with VPC CNI installed and configured with custom networking enabled.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with a managed node group </li> <li>Deploys supporting add-ons: VpcCni, CoreDns, KubeProxy, AWSLoadBalancerController</li> <li>Enables Custom Networking configuration in VpcCni AddOn </li> </ul>"},{"location":"patterns/custom-networking-with-ipv4/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>yq</li> <li><code>make</code></li> </ol> <p>Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.</p>"},{"location":"patterns/custom-networking-with-ipv4/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":""},{"location":"patterns/custom-networking-with-ipv4/#check-versions","title":"Check Versions","text":"<p>Make sure that, following versions are installed. Node version is a current stable node version 18.x.</p> <p><pre><code>node -v\nv18.12.1\n</code></pre> NPM version must be 8.4 or above:</p> <pre><code>npm -v\n8.19.2\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#clone-the-cdk-blueprints-patterns-github-repository","title":"Clone the cdk-blueprints-patterns github repository","text":"<pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#install-project-dependencies","title":"Install project dependencies","text":"<p>Once you have cloned the above repository, you can open it using your favourite IDE and run the below command to install the dependencies and build the existing patterns.</p> <p><code>make deps</code></p>"},{"location":"patterns/custom-networking-with-ipv4/#to-view-patterns-that-are-available-to-be-deployed-execute-the-following","title":"To view patterns that are available to be deployed, execute the following:","text":"<pre><code>npm i\nmake build\n</code></pre> <p>To list the existing CDK EKS Blueprints patterns, run</p> <p><code>make list</code></p>"},{"location":"patterns/custom-networking-with-ipv4/#bootstrap-your-cdk-environment","title":"Bootstrap your CDK environment","text":"<p><code>npx cdk bootstrap</code></p> <p>You can now proceed with deployment of the <code>custom-networking-ipv4</code> pattern.</p>"},{"location":"patterns/custom-networking-with-ipv4/#to-deploy-the-custom-networking-ipv4-pattern-run","title":"To deploy the custom-networking-ipv4 pattern, run","text":"<p><code>make pattern custom-networking-ipv4 deploy</code></p> <p>Once the deployment is successful, run <code>update-kubeconfig</code> command to update the kubeconfig file with required access. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name custom-networking-ipv4-blueprint --region $AWS_REGION --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/custom-networking-ipv4-bl-customnetworkingipv4blue-2SR7PW3UBLIH\n</code></pre> <p>You can verify the resources created by executing</p> <pre><code>kubectl get node -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                        STATUS   ROLES    AGE   VERSION                INTERNAL-IP   EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-18-208.us-east-2.compute.internal   Ready    &lt;none&gt;   70m   v1.24.11-eks-a59e1f0   10.0.18.208   18.116.23.237   Amazon Linux 2   5.10.173-154.642.amzn2.x86_64   containerd://1.6.19\nip-10-0-61-228.us-east-2.compute.internal   Ready    &lt;none&gt;   70m   v1.24.11-eks-a59e1f0   10.0\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#under-the-hood","title":"Under the Hood","text":"<p>This pattern first creates secondary CIDRs and secondary subnets with specified range of CIDRs as shown below in resourceProvider. Then the VPC CNI addon sets up custom networking based on the parameters <code>awsVpcK8sCniCustomNetworkCfg</code>, <code>eniConfigLabelDef: \"topology.kubernetes.io/zone\"</code> for your Amazon EKS cluster workloads with secondary subnet ranges.</p> <ul> <li>When the secondary CIDRs are passed to the VPC resource provider, the secondary subnets are created and registered under names <code>secondary-cidr-subnet-${order}</code> with the resource providers.</li> <li>We enable CNI plugin with custom pod networking with below environment variables:<ul> <li><code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</code> = <code>true</code></li> <li><code>ENI_CONFIG_LABEL_DEF</code> = <code>topology.kubernetes.io/zone</code></li> </ul> </li> </ul> <p>This deploys an ENIConfig custom resource for pod subnets (one per availability zone).</p> <p><pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nconst app = new cdk.App();\nconst addOn = new blueprints.addons.VpcCniAddOn({\n  customNetworkingConfig: {\n      subnets: [\n          blueprints.getNamedResource(\"secondary-cidr-subnet-0\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-1\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-2\"),\n      ]   \n  },\n  awsVpcK8sCniCustomNetworkCfg: true,\n  eniConfigLabelDef: 'topology.kubernetes.io/zone'\n});\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(undefined, {\n                primaryCidr: \"10.2.0.0/16\", \n                secondaryCidr: \"100.64.0.0/16\",\n                secondarySubnetCidrs: [\"100.64.0.0/24\",\"100.64.1.0/24\",\"100.64.2.0/24\"]\n            }))\n  .build(app, 'my-stack-name');\n  ```\n\nIn the diagram shown below, a secondary CIDR (100/64) is assigned to each private subnet that gets created in an availability zone. Worker nodes in the EKS cluster still gets an IP address from the Primary CIDRs(10.0) range whereas the pods get an IP address from the secondary CIDR range.\n\n![Custom-NW-IPv4](./images/custom-nw-mng.png)\n\nThis can be verified by issuing the following command\n</code></pre> kubectl get eniconfig <pre><code>Output:\n</code></pre> NAME         AGE us-east-2a   47m us-east-2b   47m us-east-2c   47m <pre><code>An ENIConfig custom resource is created in each AZ.  Number of secondary ENIs associated with the Worker node varies by instance type.\n\n![Custom-NW-MNG](./images/custom-nw-mng.png)\n\n## Additional Configuration Options\n\nVPC CNI AddOn provides some knobs to add additional advanced configuration on top of custom networking.\n\n### Prefix Delegation\n\nWhen using custom networking mode, since the node\u2019s primary ENI is no longer used to assign Pod IP addresses, there is a decrease in the number of Pods that can run on a given EC2 instance type. To work around this limitation you can use prefix delegation with custom networking. This is an important capability because when you use custom networking, only Pods that are configured to use hostNetwork are \u201cbound\u201d to the host\u2019s primary ENI. All other Pods are bound to secondary ENIs. However, with prefix delegation enabled, each secondary IP is replaced with a /28 prefix which negates the IP address loss when you use custom networking.\n\nBy default, Prefix Delegation is turned off in Vpc Cni. To check this, run the following command.\n</code></pre> kubectl get ds aws-node -o yaml -n kube-system | yq '.spec.template.spec.containers[].env' <pre><code>Output:\n</code></pre> [...]</p> <ul> <li>name: ENABLE_PREFIX_DELEGATION</li> </ul> <p>value: \"false\"</p> <p>[...] <pre><code>Consider the maximum number of Pods for an m5.large instance with custom networking.\nWhen using custom networking, the maximum number of Pods you can run without prefix delegation enabled is 20.\n\nDownload and run max-pods-calculator.sh script to calculate the maximum number of pods:\n</code></pre> curl -o max-pods-calculator.sh https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/files/max-pods-calculator.sh chmod +x max-pods-calculator.sh /max-pods-calculator.sh \\     --instance-type m5.large \\     --cni-version 1.12.5-eksbuild.2 \\     --cni-custom-networking-enabled <pre><code>Output:\n</code></pre> 20 <pre><code>To turn on `Prefix Delegation`, use the following command\n</code></pre> kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true <pre><code>Output:\n`110`\n\n![Custom-NW-Bar-Chart](./images/Custom-nw-bar-chart.png)\n\nThe reason we got max-pods is 110 instead of 290 is because the instance has a relatively low number of vCPUs. In addition the Kubernetes community recommends set max Pods no greater than 10 * number of cores, up to 110. Since Vpc Cni runs as a daemonset, you\u2019d need to create new nodes for this to take effect.\n\nThe number of ENIs and IP addresses in a pool are configured through environment variables called `WARM_ENI_TARGET`, `WARM_IP_TARGET`, `MINIMUM_IP_TARGET`. For more details on these options, please refer to [EKS Best Practices Networking](https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview) Guide.\n\n\n## Cleanup\n\nTo clean up your EKS Blueprints, run the following commands:\n\n\n```sh\nmake pattern custom-networking-ipv4 destroy \n</code></pre></p>"},{"location":"patterns/gmaestro/","title":"gMaestro on EKS pattern","text":"<p>gMaestro is a Kubernetes cost optimization solution that helps companies reduce spending on un-utilized resources. For additional information, visit gMaestro documentation.</p> <p>This pattern deploys the following resources: - Creates a single EKS cluster that includes a managed node group - Deploys supporting add-ons: ClusterAutoScaler and MetricsServer - Deploys a single granulate-gmaestro deployment with a single pod on the EKS cluster</p>"},{"location":"patterns/gmaestro/#prerequisites","title":"Prerequisites","text":"<p>Before using gMaestro, you need to: 1. Sign up to the gMaestro platform 2. Download a config YAML file - After signing up to gMaestro, navigate to the Deploy on the left-hand menu, fill in the required fields and click on \"Generate Config File\" as shown bellow:</p> <p></p> <p></p> <ol> <li> <p>Create a secret (as a plaintext, not key/value) in AWS Secrets Manager:     <pre><code>export MAESTRO_CLIENT_ID=\"&lt;MAESTRO_CLIENT_ID value from the deployment section in the downloaded config file&gt;\"\nexport MAESTRO_SECRET_NAME=\"&lt;MAESTRO_SECRET_NAME your preferred secret name&gt;\"\naws secretsmanager create-secret --name &lt;MAESTRO_SECRET_NAME&gt; --region $AWS_REGION \\\n    --description \"Encrypted client ID for Granulate gMaestro\" \\\n    --secret-string \"&lt;MAESTRO_CLIENT_ID&gt;\"\n</code></pre></p> </li> <li> <p>Follow the usage instructions to install the dependencies</p> </li> </ol>"},{"location":"patterns/gmaestro/#deployment","title":"Deployment","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Update <code>context</code> in <code>cdk.json</code> file located in the <code>cdk-eks-blueprints-patterns</code> directory as follows:</p> <pre><code>\"context\": {\n    \"clusterName\": \"&lt;MAESTRO_SERVICE_NAME value from the deployment section in the downloaded config file&gt;\",\n    \"namespace\": \"&lt;Where gMaestro will be installed&gt;\",  \n}\n</code></pre> <p>Run the following commands:</p> <pre><code>make deps\nmake build\nmake pattern gmaestro deploy\n</code></pre>"},{"location":"patterns/gmaestro/#verify-the-resources","title":"Verify the resources","text":"<p>Use the following command to validate that gMaestro installed successfully:</p> <pre><code>$ kubectl get pods -A | grep granulate-maestro\n\nNAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE\ndefault       granulate-maestro-6947dc87bc-k5nfc   1/1     Running   0          11m\n</code></pre> <p>After a few seconds, you will gain full visibility into your K8s cluster objects:</p> <p></p> <p>The first rightsizing recommendations may take up to 5 minutes to load.</p>"},{"location":"patterns/gmaestro/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern gmaestro destroy\n</code></pre>"},{"location":"patterns/gmaestro/#support","title":"Support","text":"<p>If you have questions about gMaestro, catch us on Slack!</p>"},{"location":"patterns/gmaestro/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open-source NPM package gmaestro-eks-blueprints-addon. Please refer to the package npm site for more information. https://www.npmjs.com/package/@granulate/gmaestro-eks-blueprints-addon</p> <p>If you have any questions about the npm package or find any defect, please post in the source repo at  https://github.com/Granulate/gmaestro-eks-blueprints-addon</p>"},{"location":"patterns/graviton/","title":"Graviton on EKS","text":"<p>AWS Graviton processors are designed by AWS to deliver the best price performance for your cloud workloads running in Amazon EC2. These processors are ARM chips running on aarch64 architecture.</p> <p>AWS Graviton processors are supported by many Linux operating systems including Amazon Linux 2, Red Hat Enterprise Linux, SUSE, and Ubuntu. Many popular applications and services for security, monitoring and management, containers, and continuous integration and delivery (CI/CD) from AWS and software partners also support AWS Graviton-based instances.</p> <p>AWS Graviton processors feature key capabilities that enable you to run cloud native applications securely, and at scale. EC2 instances powered by AWS Graviton processors are built on the AWS Nitro System that features the AWS Nitro security chip with dedicated hardware and software for security functions, and support for encrypted Amazon Elastic Block Store (EBS) volumes by default.</p>"},{"location":"patterns/graviton/#why-an-m7g-instance","title":"Why an M7g instance?","text":"<p>There are 7 families of Graviton instances split into 5 categories. General Purpose: M and T families Compute Optimized: C family Memory Optimized: R and X family Storage Optimized: I family Accelerated Computing: G family</p> <p>For a blueprint pattern, the General Purpose and Compute Optimized categories make the most sense, since they are the most common use cases and are all Nitro-Enabled instances.  Being Nitro-Enabled means that these instances provide better networking security as well as increased performance compared to non Nitro-Enabled instances.  In these categories, there are 7 different instance types: M7g, M6g, T4g, C7g, C7gn, C6g, and C6gn.  T4g instances are specialized for burstable workloads, and both T4g and M6g instances are Graviton2 chips. M7g instances are Graviton3 chips, which offer 25% better compute performace than Graviton2 and support DDR5 memory that provides 50% more bandwith compared to DDR4. C6g and C6gn instances are also Graviton2 chips, and C7g instances are specialized for high performance computing.  For this general blueprint pattern, the M7g instance is the best option due to the high compute power, memory bandwith, networking bandwith, and broad use cases.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with a managed node group running on an M family Graviton3 processor</li> </ul>"},{"location":"patterns/graviton/#addons","title":"Addons","text":"<p>Not all of the listed EKS addons support the Graviton processors. To find a list of supported addons, visit the documentation.</p>"},{"location":"patterns/graviton/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/graviton/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy graviton pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern graviton deploy\n</code></pre>"},{"location":"patterns/graviton/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name graviton-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/graviton-construct-bluepr-gravitonconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code>kubectl get nodes -o json | jq -r '.items[] | \"Name: \",.metadata.name,\"\\nInstance Type: \",.metadata.labels.\"beta.kubernetes.io/instance-type\",\"\\nArch: \",.metadata.labels.\"beta.kubernetes.io/arch\",\"\\n\"' # Output shows node on M famGraviton3 processor and ARM architecture\n</code></pre>"},{"location":"patterns/graviton/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprint, run the following command:</p> <pre><code>make pattern graviton destroy\n</code></pre>"},{"location":"patterns/instana/","title":"IBM Instana on EKS pattern","text":"<p>The IBM\u00ae Instana\u00ae Addon for Amazon EKS Blueprint is designed to enhance observability, monitoring, and management capabilities for applications running on Amazon Elastic Kubernetes Service (EKS). Instana Addon focuses on enhancing the user experience by reducing the complexity and time required to install and configure an Instana host agent on Amazon EKS cluster.</p> <p>This Addon will use IBM\u00ae Instana\u00ae Agent Operator in the namespace <code>instana-agent</code> to install and manage Instana Agent. It also configures custom resource values to configure the operator.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Install and set up Instana Agent for monitoring your EKS workloads. (by using the provided environment variable and additional configuration parameters)</li> </ul>"},{"location":"patterns/instana/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>Instana backend application - Use SaaS (eg aws) or Install self-hosted Instana backend (on-premises)</li> </ol>"},{"location":"patterns/instana/#project-setup","title":"Project Setup","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Go inside project directory (eg. cdk-eks-blueprints-patterns)</p> <pre><code>cd cdk-eks-blueprints-patterns\n</code></pre> <p>Install project dependencies.</p> <pre><code>make deps\n</code></pre>"},{"location":"patterns/instana/#instana-agent-configuration","title":"Instana Agent Configuration","text":"<p>Go to your Instana Backend application (Instana User Interface), click ... More &gt; Agents &gt; Installing Instana Agents and select 'Kubernetes' platform to get the Instana Agent Key, Instana Service Endpoint, Instana Service port. These steps are also described on the screenshot below.</p> <p>Instana Agent Configuration</p>"},{"location":"patterns/instana/#usage-using-aws-secret-manager-secrets","title":"Usage : Using AWS Secret Manager Secrets","text":""},{"location":"patterns/instana/#aws-secret-manager-secrets-optional","title":"AWS Secret Manager Secrets (Optional)","text":"<p>If you wish to use AWS Secret Manager Secrets to pass Instana props (key, endpoint, and port), then you will be required to setup Secrets first.</p> <p><pre><code>export SECRET_NAME=&lt;aws_secret_name&gt;\nexport INSTANA_AGENT_KEY=&lt;instana_key&gt;\nexport INSTANA_ENDPOINT_HOST_URL=&lt;instana_host_endpoint&gt;\nexport INSTANA_ENDPOINT_HOST_PORT=&lt;instana_port&gt;\"\naws secretsmanager create-secret \\\n  --name $SECRET_NAME \\\n  --secret-string \"{\\\"INSTANA_AGENT_KEY\\\":\\\"${INSTANA_AGENT_KEY}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_URL\\\":\\\"${INSTANA_ENDPOINT_HOST_URL}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_PORT\\\":\\\"${INSTANA_ENDPOINT_HOST_PORT}\\\"\n   }\"\n</code></pre> secret_name = AWS Secret Manager Secret name (eg. instana-secret-params).</p>"},{"location":"patterns/instana/#using-aws-secret-manager-secrets","title":"Using AWS Secret Manager Secrets","text":"<p>To use AWS Secret Manager Secrets follow these steps:</p> <ol> <li> <p>The actual settings for the secret name (<code>secretParamName</code>) are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory  <code>json \"context\": {      \"secretParamName\": \"instana-secret-params\"  }</code></p> </li> <li> <p>Go to project/lib/instana-construct/index.ts</p> </li> </ol> <pre><code>import { loadYaml } from \"@aws-quickstart/eks-blueprints/dist/utils\";\nimport * as cdk from \"aws-cdk-lib\";\nimport { InstanaOperatorAddon } from \"@instana/aws-eks-blueprint-addon\";\nimport { EksBlueprint, utils } from \"@aws-quickstart/eks-blueprints\";\nimport { prevalidateSecrets } from \"../common/construct-utils\";\n\nexport const instanaProps: { [key: string]: any } = {};\n\nexport default class InstanaConstruct {\n    async buildAsync(scope: cdk.App, id: string) {\n        try {\n            await prevalidateSecrets(InstanaConstruct.name, undefined, 'instana-secret-params');\n\n            const secretParamName: string = utils.valueFromContext(scope, \"secretParamName\", undefined);\n            if(secretParamName != undefined) {\n                instanaProps.secretParamName = secretParamName;\n            }\n            const yamlObject = loadYaml(JSON.stringify(instanaProps));\n            const stackId = `${id}-blueprint`;\n            const addOns = new InstanaOperatorAddon(yamlObject);\n            EksBlueprint.builder()\n                .account(process.env.CDK_DEFAULT_ACCOUNT!)\n                .region(process.env.CDK_DEFAULT_REGION!)\n                .addOns(addOns)\n                .build(scope, stackId);\n            console.log(\"Blueprint built successfully.\");\n        } catch (error) {\n            console.error(\"Error:\", error);\n            throw new Error(`environment variables must be setup for the instana-operator pattern to work`);\n        }\n    }\n}\n</code></pre>"},{"location":"patterns/instana/#usage-using-secrets-in-the-code","title":"Usage : Using Secrets in the Code","text":""},{"location":"patterns/instana/#setting-up-environment-variable","title":"Setting up environment variable","text":"<p>To set the following environment variables from the CLI, use the corresponding values obtained from the Instana Service Endpoint and Port (as shown in the above screenshot), and the Instana Application Key (also shown in the above screenshot):</p> <ul> <li>Set the value of INSTANA_ENDPOINT_HOST_URL to the Instana Service Endpoint.</li> <li>Set the value of INSTANA_ENDPOINT_HOST_PORT to the Instana Service Port.</li> <li>Set the value of INSTANA_AGENT_KEY to the Instana Application Key.</li> </ul> <p>Set the value of the following environment variable and run it on CLI to set those variables.</p> <p>For an example:</p> <pre><code>export INSTANA_AGENT_KEY=abc123\nexport INSTANA_ENDPOINT_HOST_URL=instana.example.com\nexport INSTANA_ENDPOINT_HOST_PORT=\"443\"\n</code></pre>"},{"location":"patterns/instana/#configure-additional-configuration-parameters","title":"Configure additional configuration parameters.","text":"<p>To configure additional parameters for Instana Agent according to your specific use case, follow these steps:</p> <ul> <li>Go to project/lib/instana-construct/index.ts</li> <li>Add the additional configuration parameters under <code>const instanaProps</code> variable.</li> </ul> <p>For an example:</p> <pre><code>export const instanaProps = {\n agent: {\n    key: process.env.INSTANA_AGENT_KEY,// Mandatory Parameter\n    endpointHost: process.env.INSTANA_ENDPOINT_HOST_URL,//Mandatory Parameter\n    endpointPort: process.env.INSTANA_ENDPOINT_HOST_PORT, // Mandatory Parameter,\n    env: {\n        INSTANA_AGENT_TAGS: \"staging\",\n    }\n  }\n};\n</code></pre>"},{"location":"patterns/instana/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>To view patterns and deploy <code>instana-operator</code> pattern</p> <pre><code>make deps\nmake build\ncdk bootstrap\nmake pattern instana-operator deploy\n</code></pre>"},{"location":"patterns/instana/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/eks-blue1-eksblue1AccessRole32C5DF05-1NBFCH8INI08A\n</code></pre></p> <p>Lets verify the resources created by Steps above. <pre><code>kubectl get pods -n instana-agent # Output shows the EKS Managed Node group nodes under instana-agent namespace\n</code></pre> Output of the above command will be silimar to below one:</p> <p><pre><code>NAMESPACE       NAME                                                READY       STATUS    RESTARTS   AGE\ninstana-agent   controller-manager-78479cb596-sktg9     1/1         Running                     0          56m\ninstana-agent   controller-manager-78479cb596-xz8kn     1/1         Running                     0          56m\ninstana-agent   instana-agent-gsqx8                                 1/1         Running                     0          56m\n</code></pre> Run following command to verify Instana Agent logs <pre><code>kubectl logs &lt;instana-agent-pod-name&gt; -n instana-agent # Output shows instana agent logs. pod name in this example is instana-agent-gsqx8\n</code></pre></p>"},{"location":"patterns/instana/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern instana-operator destroy \n</code></pre>"},{"location":"patterns/instana/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package aws-eks-blueprint-addon. Please refer to the package npm site for more information. <pre><code>https://www.npmjs.com/package/@instana/aws-eks-blueprint-addon'\n</code></pre> If you have any questions about the npm package or find any defect, please post in the source repo at: https://github.com/instana/instana-eks-blueprint-addon/issues</p>"},{"location":"patterns/jupyterhub/","title":"JupyterHub on EKS Pattern","text":""},{"location":"patterns/jupyterhub/#objective","title":"Objective","text":"<p>JupyterHub is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. The hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group.</p> <p>The objective of this pattern is to deploy JupyterHub on EKS using EKS Blueprints with the following features in place: - JupyterHub is hosted behind an ALB on EKS cluster across multiple AZs - JupyterHub allows for user friendly DNS name to route traffic to the load balancer, which is a subdomain of a parent domain in a separate account. This is representatitve of a typical global enterprise domain setup, where a central, global DNS account defines the parent domain (in Route53). The subdomain will be defined in Route53 from this account where the JupyterHub cluster is provisioned. - JupyterHub leverages an identity provider for user authentication. - JupyterHub uses persistent storage that is provided within a file system (i.e. EFS) when the user logs in - JupyterHub uses certificates to provide secured connection to the hub (the load balancer)  - The hub has a persistent storage with an EBS volume</p>"},{"location":"patterns/jupyterhub/#approach","title":"Approach","text":"<p>Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. </p> <p>Our blueprint will then include the following:</p> <ol> <li>AWS Loadbalancer controller to provision an ALB instance fronting the Kubernetes Ingress resource for the JupyterHub server. Deployed with a public certificate created from ACM (Certificate ARN must be provided post-creation via CDK context)</li> <li>External DNS to integrate ALB with Route53 and use custom domain to access the hub. </li> <li>Configurations to leverage existing user management via OAuth 2.0 protocol standard (i.e. Auth0).</li> <li>EFS file server for user persistent storage using the Blueprints.</li> <li>EBS volume for hub persistent storage.</li> </ol>"},{"location":"patterns/jupyterhub/#prerequisites","title":"Prerequisites","text":"<ol> <li>Identity Provider that can be leveraged using 0Auth 2.0 protocol. The actual settings are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"callbackUrl\": \"https://your.hub.domain.com/hub/oauth_callback\",\n    \"authUrl\": \"https://some.auth.address.com/authorize\",\n    \"tokenUrl\": \"https://some.auth.address.com/oauth/token\",\n    \"userDataUrl\": \"https://some.auth.address.com/userinfo\",\n    \"clientId\": \"someClientID\",\n    \"clientSecret\": \"someClientSecret\",\n    \"scope\": [\"openid\",\"name\",\"profile\",\"email\"],\n    \"usernameKey\": \"name\"\n  }\n}\n</code></pre></li> <li>The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT).</li> <li>The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it <code>DomainOperatorRole</code> but you can choose any arbitrary name for it.</li> <li>Policies:  <code>arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess</code> or alternatively you can provide <code>arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess</code> and <code>arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess</code>.</li> <li>Trust relationship to allow workload accounts to create subdomains (replace <code>&lt;WORKLOAD_ACCOUNT&gt;</code> with the actual value):     <pre><code>{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n     {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n             \"AWS\": \"arn:aws:iam::&lt;WORKLOAD_ACCOUNT&gt;:root\"\n         },\n         \"Action\": \"sts:AssumeRole\",\n         \"Condition\": {}\n     }\n ]\n}\n</code></pre></li> <li>The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name, subzone name, and the JupyterHub hub subdomain names are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"parent.dns.account\": \"&lt;PARENT_ACCOUNT&gt;\",\n    \"parent.hostedzone.name\": \"domain.com\",\n    \"dev.subzone.name\": \"hub.domain.com\",\n    \"jupyterhub.subzone.name\":\"your.hub.domain.com\",\n  }\n}\n</code></pre></li> </ol>"},{"location":"patterns/jupyterhub/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including a JupyterHub where users can log in using their credentials from the identity provider given.</p>"},{"location":"patterns/karpenter/","title":"Karpenter on EKS","text":"<p>Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application.</p> <p>Karpenter works by:</p> <ul> <li>Watching for pods that the Kubernetes scheduler has marked as unschedulable,</li> <li>Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods,</li> <li>Provisioning nodes that meet the requirements of the pods,</li> <li>Scheduling the pods to run on the new nodes, and</li> <li>Removing the nodes when the nodes are no longer needed</li> </ul> <p>To learn more about Karpenter add on usage, please visit the documentation here This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Deploys supporting add-ons: AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy,  CertManagerAddOn, KubeStateMetricsAddOn, MetricsServer</li> <li>Deploy Karpenter on the EKS cluster</li> </ul>"},{"location":"patterns/karpenter/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/karpenter/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy karpenter pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern karpenter deploy\n</code></pre>"},{"location":"patterns/karpenter/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name karpenter-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/karpenter-construct-bluepr-karpenterconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code># Assuming add-on is installed in the karpenter namespace.\n$ kubectl get po -n karpenter\nNAME                                          READY   STATUS    RESTARTS   AGE\nkarpenter-54fd978b89-hclmp   2/2     Running   0          99m\n</code></pre>"},{"location":"patterns/karpenter/#testing-with-a-sample-deployment","title":"Testing with a sample deployment","text":"<p>Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate\n  template:\n    metadata:\n      labels:\n        app: inflate\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: inflate\n          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n          resources:\n            requests:\n              cpu: 1\nEOF\n</code></pre> <p>Now scale the deployment:</p> <pre><code>kubectl scale deployment inflate --replicas 10\n</code></pre> <p>The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs,</p> <pre><code>kubectl logs -f -n karpenter karpenter-54fd978b89-hclmp\n</code></pre> <p>or, by looking at the nodes being created:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"patterns/konveyor/","title":"Konveyor Add-On for Amazon EKS Blueprints","text":"<p>Konveyor is an open-source application modernization platform that helps organizations safely and predictably modernize applications to new technologies, with an initial focus on accelerating the adoption of legacy applications to Kubernetes. Konveyor\u2019s goal is to deliver a Unified Experience to the organizations embarking on their modernization journey. It follows a simple yet effective approach of surfacing the information about the application to aid a \u2018Decision Maker\u2019 to make decisions about their modernization and migration needs, plan the work in the form of \u2018Migration waves\u2019 and provide guidance to the developers to complete the needed migration/modernization by providing assets as well as a catalog of integrated tools to aid specific workflows.</p> <p>Feature set</p> <ul> <li> <p>Konveyor Hub: Central interface from where you manage your application portfolio and integrate with other Konveyor tools.</p> </li> <li> <p>Categorize and group applications by different dimensions (pre-packaged or custom) aligned with technical criteria or your organization structure.</p> </li> <li> <p>Assess applications containerization suitablity and risks assessment.</p> </li> <li> <p>Assign priority, assess estimated migration effort, and define optimal migration strategy for individual applications.</p> </li> <li> <p>Evaluate required changes for Java applications containerization through automated analysis (pre-packaged or custom rules).</p> </li> <li> <p>Fully integrated Konveyor Data Intensive Validity Advisor (DiVA): Analyzes the data layer of applications and detect dependencies to different data stores and distributed transactions. Import target Java application source files to generate analysis results.</p> </li> </ul> <p>Konveyor is an Open Source software developed by the Konveyor Community, and is a CNCF Sandbox project.</p> <p>This Open Source solution is packaged by Claranet Switzerland GmbH.</p>"},{"location":"patterns/konveyor/#arhcitecture","title":"Arhcitecture","text":""},{"location":"patterns/konveyor/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ul> <li>AWS CLI (also ensure it is configured)</li> <li>cdk</li> <li>npm</li> <li>tsc</li> <li>make</li> </ul> <p>Let\u2019s start by setting the account and region environment variables:</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=$(aws configure get region)\n</code></pre>"},{"location":"patterns/konveyor/#deployment","title":"Deployment","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file (edit PARENT_DOMAIN_NAME as it fits):</p> <pre><code>PARENT_DOMAIN_NAME=example.com\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $PARENT_DOMAIN_NAME --query \"HostedZones[].Id\" --output text | xargs basename)\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"konveyor.parent.domain.name\":\"${PARENT_DOMAIN_NAME}\",\n        \"konveyor.hosted.zone.id\": \"${HOSTED_ZONE_ID}\"\n      }\n}\nEOF\n</code></pre> <p>(Optional) The full list of parameters you can set in the context is:</p> <pre><code>    \"context\": {\n        \"konveyor.namespace.name\": ...,\n        \"konveyor.parent.domain.name\": ...,\n        \"konveyor.subdomain.label\": ...,\n        \"konveyor.hosted.zone.id\": ...,\n        \"konveyor.certificate.resource.name\": ...,\n      }\n</code></pre> <p>You can assign values to the above keys according to the following criteria (values are required where you don't see default mentioned):</p> <ul> <li>\"konveyor.namespace.name\": Konveyor's namespace, the default is \"konveyor\"</li> <li>\"konveyor.parent.domain.name\": the parent domain in your Hosted Zone</li> <li>\"konveyor.subdomain.label\": to be used as {\"subdomain.label\"}.{\"parent.domain.name\"}, the default is \"backstage\"</li> <li>\"konveyor.hosted.zone.id\": the Hosted zone ID (format: 20x chars/numbers)</li> <li>\"konveyor.certificate.resource.name\": resource name of the certificate, registered by the resource provider, the default is \"konveyor-certificate\"</li> </ul> <p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Run the following commands:</p> <pre><code>make deps\nmake build\nmake pattern konveyor deploy\n</code></pre> <p>When deployment completes, the output will be similar to the following:</p> <p></p>"},{"location":"patterns/konveyor/#log-in","title":"Log in","text":"<p>Once the deployment ends navigate to</p> <p><code>https://&lt;subdomain&gt;.&lt;parent-domain&gt;</code></p> <p>And enter the default admin credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>Passw0rd!</code></li> </ul>"},{"location":"patterns/konveyor/#koveyor-ui","title":"Koveyor UI","text":"<p>Login page</p> <p></p> <p>Home Page</p> <p></p>"},{"location":"patterns/konveyor/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern konveyor destroy\n</code></pre>"},{"location":"patterns/kubeflow/","title":"Kubeflow on EKS","text":"<p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Deploys supporting add-ons: ClusterAutoScaler, AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy, EbsCsiDriver, CertManagerAddOn, KubeStateMetricsAddOn, PrometheusNodeExporterAddOn, AdotCollectorAddOn, AmpAddOn,</li> <li>Deploy Kubeflow on the EKS cluster</li> </ul>"},{"location":"patterns/kubeflow/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/kubeflow/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy kubeflow pattern</p> <pre><code>make list\ncdk bootstrap\nmake pattern kubeflow deploy\n</code></pre>"},{"location":"patterns/kubeflow/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO\n</code></pre></p> <p>Let\u2019s verify the resources created by Steps above. <pre><code>kubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | kubeflow # Output shows kubeflow namespace\n\nkubectl get pods --namespace=kubeflow-pipelines  # Output shows kubeflow pods\n</code></pre></p>"},{"location":"patterns/kubeflow/#execute-machine-learning-jobs-on-kubeflow","title":"Execute Machine learning jobs on Kubeflow","text":"<p>log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service</p> <p><pre><code>kubectl port-forward svc/ml-pipeline-ui 9000:80 -n =kubeflow-pipelines\n</code></pre> and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/legacy-v1/tutorials/</p>"},{"location":"patterns/kubeflow/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy kubeflow-blueprint \n</code></pre>"},{"location":"patterns/kubeflow/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext</p>"},{"location":"patterns/kubeshark/","title":"Kubeshark AddOn","text":"<p>kubeshark  is an API Traffic Analyzer for Kubernetes providing real-time, protocol-level visibility into Kubernetes\u2019 internal network, capturing and monitoring all traffic and payloads going in, out and across containers, pods, nodes and clusters.</p> <p>This pattern deploys the following resources: - Creates EKS Cluster Control plane with managed nodegroup - Install and set up kubeshark</p>"},{"location":"patterns/kubeshark/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/kubeshark/#project-setup","title":"Project Setup","text":"<p>1.) Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>2.) Go inside project directory (eg. cdk-eks-blueprints-patterns)</p> <pre><code>cd cdk-eks-blueprints-patterns\n</code></pre> <p>3.) Install project dependencies.</p> <pre><code>make deps\n</code></pre> <p>4.) import kubeshark <pre><code>npm i kubeshark\n</code></pre> 5.) To view patterns and deploy kubeshark pattern, run the below command. <pre><code>make list\ncdk bootstrap\nmake pattern kubeshark deploy\n</code></pre></p>"},{"location":"patterns/kubeshark/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/eks-blue1-eksblue1AccessRole32C5DF05-1NBFCH8INI08A\n</code></pre></p> <p>1.) verify the resources created by Steps above. <pre><code>$ kubectl get deployments -n kube-system\n\nNAME                                                          READY   UP-TO-DATE   AVAILABLE   AGE\nblueprints-addon-kubeshark                               1/1     1            1           20m\n</code></pre></p> <p>2.) Access to kubeshark.</p> <pre><code>$ kubectl -n kube-system port-forward svc/kubeshark-front 3000:80\n</code></pre> <p>Open the dashboard</p> <p>Then you should be able to see view like this </p> <p>3.) deploy nginx pod using the below command. <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\nEOF\n</code></pre></p> <p>4.) Try to access \"aws.com\" to generate traffic flow using the below command. <pre><code>kubectl exec nginx curl https://aws.com\n</code></pre></p> <p>5.) Access kubeshark using the below command. <pre><code>kubectl -n kube-system port-forward svc/kubeshark-front 3000:80\n</code></pre></p> <p>6.) Run Kubeshark query to identify the traffic flow. <pre><code>(src.pod.metadata.name == \"nginx\" or dst.pod.metadata name == \"nginx\") and request.questions[0].name == \"aws.com\" or (src.name == \"nginx\" and src.namespace == \"default\" and dst.name == \"kube-dns\" and dst.namespace == \"kube-system\")\n</code></pre> As shown below, the Kubeshark query used to identify the traffic flowing from the pod \"nginx\" in the \"default\" namespace to \"aws.com\" and \"coredns\". The query is writen by Kubeshark Filter Language (KFL) is the language implemented inside kubeshark/worker that enables the user to filter the traffic efficiently and precisely.</p> <p></p> <p>Also you can visualize the traffic flow and bandwidth using service map feature as shown below. </p>"},{"location":"patterns/kubeshark/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern kubeshark destroy \n</code></pre>"},{"location":"patterns/kubeshark/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package aws-eks-blueprint-addon. Please refer to the package npm site for more information. <pre><code>https://www.npmjs.com/package/kubeshark'\n</code></pre></p> <p>If you have any questions about the npm package or find any defect, please post in the source repo at: https://github.com/zghanem0/kubeshark/issues</p>"},{"location":"patterns/multi-cluster-conformitron/","title":"Multi-cluster pattern with observability, cost optimizations and metrics aggregation","text":""},{"location":"patterns/multi-cluster-conformitron/#objective","title":"Objective","text":"<p>This pattern was started to solve a problem faced at AWS. We often get third-party software for validation and need a consistent automated approach to run Kubernetes evaluator testing, deployment of containerized products, and validation in Kubernetes environments on a variety of Amazon EKS environments. </p> <p>In this pattern we:</p> <ol> <li> <p>Automate deployment of multiple EKS cluster in a region, with a Continuous Deployment pipeline triggered upon a commit to the GitHub repository that hosts the pipeline configuration.</p> </li> <li> <p>Configure the EKS clusters to deploy with different architectures (x86 or ARM or Bottlerocket) and different Kubernetes versions (3 most recent by default).</p> </li> <li> <p>Automate testing of all the available EKS Anywhere Addons, on each of the clusters, essentially testing their compatibility across all the potential architecture/version available today on AWS. </p> </li> <li> <p>Deploying this pattern 24x7 we observed high costs (300$ a day). By using the AWS Systems Manager Automations and AutoScaling Groups we scale-down to zero during non-business hours resulting in 60% cost savings. We also borrowed optimized OTEL collector configurations from  CDK Observability Accelerator to further reduce Prometheus storage costs. </p> </li> </ol> <p>To learn more about our EKS Addon validation checkout our blog</p> <p></p>"},{"location":"patterns/multi-cluster-conformitron/#gitops-confguration","title":"GitOps confguration","text":"<p>GitOps is a branch of DevOps that focuses on using Git code repositories to manage infrastructure and application code deployments.</p> <p>For this pattern there is a git driven deployment using GitHub and Codepipeline which automatically redploys the EKS Clusters when modifications are made to the GitHub repo. </p> <p>Secondly, for the deployment of workloads on the cluster we leverage FluxCD, this a GitOps approach for the workloads i.e. the third-party-software we want to validate on our hardware.</p> <p>We require some additional secrets to be created in Secrets Manager for the pattern to function properly</p> <ol> <li>AWS CodePipeline Bootstrap - The AWS CodePipeline points to the GitHub fork of this repository i.e [cdk-eks-blueprint-patterns] (https://github.com/aws-samples/cdk-eks-blueprints-patterns). </li> </ol> <p>A <code>github-token</code> secret must be stored as plaintext in AWS Secrets Manager for the CodePipeline to access the webhooks on GitHub. For more information on how/why to set it up, please refer to the docs. The GitHub Personal Access Token should have these scopes:    1. repo - to read your forked cdk-blueprint-patterns repostiory    1. admin:repo_hook - if you plan to use webhooks (enabled by default)</p> <ol> <li>FluxCD Bootstrap - The FluxCD points to the EKS Anywhere Addons repository. Since this is a public repository you will not need to add a github token to read it.</li> </ol> <p>As part of the FluxCD configuration, it uses Kustomize to apply all the addons that are in the repository along with deploying their functional tests and a custom validator cronJob.</p>"},{"location":"patterns/multi-cluster-conformitron/#prerequisites","title":"Prerequisites","text":"<p>Start by setting the account and region environment variables:</p> <p><pre><code>export ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nexport AWS_REGION=$(aws configure get region)\n</code></pre> 1. In case you haven't done this before, bootstrap your AWS Account for AWS CDK use using:</p> <pre><code>```bash\ncdk bootstrap\n```\n</code></pre> <ol> <li>Fork this repository (cdk-eks-blueprints-patterns) to your GitHub organisation/user</li> <li>Git clone your forked repository onto your machine</li> <li> <p>Install the AWS CDK Toolkit globally on your machine using</p> <pre><code>npm install -g aws-cdk@2.133.0\n</code></pre> </li> <li> <p>Increase AWS service quota for required resources, navigate to Service Quota Tutorial to learn more <pre><code>   SERVICE                                   | QUOTA NAME                         | REQUESTED QUOTA\n   Amazon Virtual Private Cloud (Amazon VPC) | NAT gateways per Availability Zone | 30 \n   Amazon Virtual Private Cloud (Amazon VPC) | VPCs per region                    | 30\n   Amazon Elastic Compute Cloud (Amazon EC2) | EC2-VPC Elastic IPs                | 30\n</code></pre> We are using seperate VPC as a best practice, but you can use default vpc if you prefer. Also, If you decide to use different regions for each cluster you dont need quota increase, please reach out if you have need for this use case.</p> </li> <li> <p>Amazon Managed Grafana Workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create environment variables <code>AMG_ENDPOINT_URL</code> as described below. </p> </li> </ol> <p>Else, to create a new workspace, visit and run our supporting example for Grafana Deployment</p> <pre><code>export AMG_ENDPOINT_URL=https://g-xxx.grafana-workspace.region.amazonaws.com\nexport AMG_WORKSPACE_ID=g-xxx\n</code></pre> <ol> <li> <p>Grafana API Key: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens. This allows programatic provisioning of Grafana dashboards using the EKS grafana operator.</p> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> </li> <li> <p>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access and provision Grafana dashboards from Amazon EKS monitoring Cluster</p> </li> </ol> <pre><code>aws ssm put-parameter --name \"/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li>Amazon Managed Prometheus Workspace: To store observability metrics from all clusters we will use Amazon Managed Prometheus due to it's ease of setup and easy integration with other AWS services. We recommend setting up a new seperate Prometheus workspace using the CLI commands below. The provisioning of a new AMP workspace can be automated by leveraging the <code>.resourceProvider</code> in our CDK blueprints. See Example. We intentionally left this out to allow to connecting with existing AMP deployments, but please reach out to us if you need guidance on automate this provisioning.</li> </ol> <pre><code>aws amp create-workspace --alias conformitron\n</code></pre> <p>Copy the <code>workspaceID</code> from the output and export it as a variable</p> <pre><code>export AMP_WS_ID=ws-xxxxxxx-xxxx-xxxx-xxxx-xxxxxx\n</code></pre> <ol> <li>Modify the code in your forked repo to point to your GitHub username/organisation. Open the pattern file source code and look for the declared const of <code>gitOwner</code>. Change it to your GitHub username.</li> </ol>"},{"location":"patterns/multi-cluster-conformitron/#deploying","title":"Deploying","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file (edit PARENT_DOMAIN_NAME as it fits): <pre><code>cat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"conformitron.amp.endpoint\": \"https://aps-workspaces.${AWS_REGION}.amazonaws.com/workspaces/${AMP_WS_ID}/\",\n        \"conformitron.amp.arn\":\"arn:aws:aps:${AWS_REGION}:${ACCOUNT_ID}:workspace/${AMP_WS_ID}\",\n        \"conformitron.amg.endpoint\": \"${AMG_ENDPOINT_URL}\",\n        \"conformitron.version\": [\"1.28\",\"1.29\",\"1.30\"],\n        \"fluxRepository\": {\n        \"name\": \"grafana-dashboards\",\n        \"namespace\": \"grafana-operator\",\n        \"repository\": {\n          \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n          \"name\": \"grafana-dashboards\",\n          \"targetRevision\": \"main\",\n          \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        \"values\": {\n          \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n          \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n          \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n          \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n          \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n          \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\"\n        },\n        \"kustomizations\": [\n          {\n            \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n          }\n        ]\n    }\n  }\n}\n\nEOF\n</code></pre></p> <p>You are now ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> <pre><code>make pattern multi-cluster-conformitron deploy multi-cluster-central-pipeline\n</code></pre> <p>Now you can go to AWS CodePipeline console, and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments.</p>"},{"location":"patterns/multi-cluster-conformitron/#grafana-dashboards","title":"Grafana Dashboards","text":""},{"location":"patterns/multi-cluster-conformitron/#ssm-cost-optimizations-for-conformitron-clusters","title":"SSM Cost Optimizations for conformitron clusters","text":"<p>Running all the clusters by default for 24 hours results in a daily spend of $300+</p> <p>To minimize these costs we have written a systems manager automation which automatically scales down autoscaling group to 0 desired nodes during off-business hours.</p> <p>On weekdays 5 PM PST clusters are scaled to 0 -&gt; CRON EXPRESSION:  <code>0 17 ? * MON-FRI *</code> On weekdays 5 AM PST clusters are scaled to 1 -&gt; CRON EXPRESSION:  <code>0 05 ? * MON-FRI *</code> On weekends clusters stay scaled to 0.</p> <p>These optimizations bring down the weekly cost to less than 1000$ essentially for a more than 60% cost savings.</p> <p>Please find the SSM Automation documents <code>lib/multi-cluster-construct/resources/cost-optimization/scaleDownEksToZero.yml</code> and <code>lib/multi-cluster-construct/resources/cost-optimization/scaleUpEksToOne.yml</code>. </p> <p>Lets take a look at one of the scripts <code>scaleDownEksToZero.yml</code></p> <p><pre><code>schemaVersion: '0.3'\n...\n...\nmainSteps:\n  ...\n  ...\n    inputs:\n      Service: eks\n      Api: UpdateNodegroupConfig      &lt;---- Update the managed node group \n      clusterName: arm-1-26-blueprint &lt;---- Modify according to your naming convention\n      nodegroupName: eks-blueprints-mng\n      scalingConfig:\n        minSize: 0                    &lt;---- New Scaling Configuration\n        maxSize: 1\n        desiredSize: 0                &lt;---- Scale To zero\n</code></pre> By triggering this automation at 5PM on Weekdays we automatically scale down clusters during off-hours.</p> <p>To run these scripts first you will have to modify update them with your own account_ID We will use <code>sed</code> command to automatically update the files <pre><code>sed \"s/ACCOUNT_ID/$ACCOUNT_ID/g\" scaleDownEksToZero.yml &gt; scaleDownEksToZeroNew.yml\nsed \"s/ACCOUNT_ID/$ACCOUNT_ID/g\" scaleUpEksToOne.yml &gt; scaleUpEksToOneNew.yml\n</code></pre></p> <ol> <li>Then navigate to the Systems Manager &gt; Documents and Create a new Automation.</li> </ol> <p></p> <ol> <li>Click on JSON and copy over the yml content to create a new runbook</li> </ol> <p></p> <ol> <li>Once saved, navigate to EventBridge &gt; Scheduler &gt; Schedules</li> </ol> <p></p> <ol> <li>Create a new schedule with the CRON expression specified aboce</li> </ol> <p></p> <ol> <li>For Target select \"StartAutomationExecution\" and type in the document name from step 2</li> </ol> <p></p>"},{"location":"patterns/nginx/","title":"NGINX Pattern","text":""},{"location":"patterns/nginx/#objective","title":"Objective","text":"<p>When setting up a target platform across multiple dimensions that question of ingress must be solved. Ideally, it should work in such as way that workloads provisioned on the target environments could be accessible via internet exposing sub-domains of some predefined global domain name. </p> <p>Communication with the workloads should leverage secure TLS protected Load balancer with proper public (or private) certificate.</p> <p>A single cluster will deploy workloads from multiple teams and each of them should be able to expose workloads routed to their corresponding namespace. So, teams are expected to define ingress objects. </p> <p>In addition, this approach should work not only for a single cluster, but also across multiple regions and environments. </p>"},{"location":"patterns/nginx/#approach","title":"Approach","text":"<p>Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. </p> <p>Sub-domains are then defined in the target accounts (let's call them workload accounts).</p> <p>Our blueprint will then include the following:</p> <ol> <li>NGINX ingress controller to enable teams to create/configure their ingress objects. </li> <li>External DNS to integrate NGINX and public-facing NLB with Route53. </li> <li>AWS Loadbalancer controller to provision an NLB instance with each cluster fronting the NGINX ingress. Deployed with a public certificate that will also be provisioned as part of the blueprint.</li> <li>Team onboarding that leverage the ingress capabilities through ArgoCD. </li> <li>Other popular add-ons.</li> </ol>"},{"location":"patterns/nginx/#prerequisites","title":"Prerequisites","text":"<ol> <li><code>argo-admin-password</code> secret must be defined as plain text (not key/value) in <code>us-west-2</code>  region.</li> <li>The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT).</li> <li>The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it <code>DomainOperatorRole</code> but you can choose any arbitrary name for it.</li> <li>Policies:  <code>arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess</code> or alternatively you can provide <code>arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess</code> and <code>arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess</code>.</li> <li>Trust relationship to allow workload accounts to create subdomains (replace <code>&lt;WORKLOAD_ACCOUNT&gt;</code> with the actual value):     <pre><code>{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n     {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n             \"AWS\": \"arn:aws:iam::&lt;WORKLOAD_ACCOUNT&gt;:root\"\n         },\n         \"Action\": \"sts:AssumeRole\",\n         \"Condition\": {}\n     }\n ]\n}\n</code></pre></li> <li>The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"parent.dns.account\": \"&lt;PARENT_ACCOUNT&gt;\",\n    \"parent.hostedzone.name\": \"mycompany.a2z.com\",\n    \"dev.subzone.name\": \"dev.mycompany.a2z.com\",\n  }\n}\n</code></pre></li> </ol>"},{"location":"patterns/nginx/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including workloads with an example of team-specific ingress objects. </p>"},{"location":"patterns/paralus/","title":"Paralus on EKS","text":"<p>The Paralus project is a free open-source tool that enables controlled audited access to Kubernetes infrastructure. It comes with just-in-time service account creation and user-level credential management that integrates with your existing RBAC and SSO providers of choice. Learn more by visiting the offical documentation page: https://www.paralus.io/</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates a single EKS cluster with a public endpoint (for demo purpose only) that includes a managed node group</li> <li>Deploys supporting AddOn:  AwsLoadBalancerController, VpcCni, KubeProxy, EbsCsiDriverAddOn</li> <li>Deploy Paralus on the EKS cluster</li> </ul> <p>NOTE: Paralus installs a few dependent modules such as Postgres, Kratos, and also comes with a built-in dashboard. At it's core, Paralus works atop domain-based routing, inter-service communication, and supports the AddOns mentioned above.</p>"},{"location":"patterns/paralus/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/paralus/#deploy-an-eks-cluster-using-amazon-eks-blueprints-for-cdk","title":"Deploy an EKS Cluster using Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Update FQDN information for your installation following the example below:</p> <pre><code>    fqdn: {\n        \"domain\": \"yourdomain.com\",\n        \"hostname\": \"console-eks\",\n        \"coreConnectorSubdomain\": \"*.core-connector.eks\",\n        \"userSubdomain\": \"*.user.eks\"\n    }\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy the Paralus pattern, run the commands below:</p> <pre><code>cdk list\ncdk bootstrap\nmake pattern paralus deploy\n</code></pre>"},{"location":"patterns/paralus/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command below. You should be able to get the command from the CDK output message once your cluster has been finished deploying. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::1234567890121:role/paralus-blueprint-paralusblueprintMastersRoleF3287-EI3XEBO1107B\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes # Output will provide list of running nodes in your cluster\n\nkubectl get ns | grep paralus # Output shows Paralus namespace\n\nkubectl get pods --namespace=paralus-system  # Output shows Paralus pods\n\nblueprints-addon-paralus-contour-contour-7857f4cd9-kqhgp   1/1     Running                 \nblueprints-addon-paralus-contour-envoy-mx8z7               2/2     Running                 \nblueprints-addon-paralus-fluent-bit-525tt                  1/1     Running                 \nblueprints-addon-paralus-kratos-588775bc47-wf5gf           2/2     Running                 \nblueprints-addon-paralus-kratos-courier-0                  2/2     Running                 \nblueprints-addon-paralus-postgresql-0                      1/1     Running                 \ndashboard-6d8b54d78b-d8cks                                 1/1     Running                 \nparalus-66d9bbf698-qznzl                                   2/2     Running                 \nprompt-54d45cff79-h9x95                                    2/2     Running   \nrelay-server-79448564cb-nf5tj                              2/2     Running              \n</code></pre> <p>Learn more about the various components that are deployed as part of Paralus.</p>"},{"location":"patterns/paralus/#configure-dns-settings","title":"Configure DNS Settings","text":"<p>Once Paralus is installed, continue with following steps to configure DNS settings, reset default password and start using Paralus.</p> <p>Obtain the external ip address by executing below command against the installation <code>kubectl get svc blueprints-addon-paralus-contour-envoy -n paralus-system</code></p> <pre><code>NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                         AGE\nblueprints-addon-paralus-contour-envoy         LoadBalancer   10.100.101.216   a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com   80:31810/TCP,443:30292/TCP      10m\n</code></pre> <p>Update the DNS settings to add CNAME records:</p> <pre><code>    name: console-eks \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.core-connector.eks  \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.user.eks \n    value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n</code></pre> <p>Obtain your default password and reset it upon first login</p> <p><code>kubectl logs -f --namespace paralus-system $(kubectl get pods --namespace paralus-system -l app.kubernetes.io/name='paralus' -o jsonpath='{ .items[0].metadata.name }') initialize | grep 'Org Admin default password:'</code></p> <p>You can now access dashboard with http://console-eks.yourdomain.com ( refers to the hostname.domain specified during installation ), start importing clusters and using paralus.</p> <p>Note: you can also refer to this paralus eks blogpost</p>"},{"location":"patterns/paralus/#paralus-features-usage","title":"Paralus Features &amp; Usage","text":"<p>https://www.paralus.io/docs/usage/</p>"},{"location":"patterns/paralus/#configuring-centralized-kubectl-access-to-clusters","title":"Configuring centralized kubectl access to clusters","text":"<p>Kubectl is one of the most widely used tools to interact with Kubernetes. The command line tool allows you to deploy applications, inspect, and manage resources. It authenticates with the control plane for your cluster and makes API calls to the Kubernetes API. In short if you are working with Kubernetes - you will use kubectl the most.</p> <p>In most modern day scenarios, there are multiple users who are accessing various clusters. This makes it all more important to ensure that every user or group has access to only those resources that they are allowed to. A couple different approaches to achieve this include using namespaces and role based access control. While these are good, most enterprise grade application deployments require something more robust.</p> <p>That\u2019s where Paralus comes in. It allows you to configure centralized kubectl access to multiple clusters all from a single dashboard. It allows you to create groups, assign projects and users, and provide access. Check out this blog post for a deep dive into how you can use Paralus to import different clusters to Paralus and configure access to them using zero trust principles built in. Read More</p>"},{"location":"patterns/paralus/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy paralus-blueprint \n</code></pre>"},{"location":"patterns/paralus/#troubleshooting","title":"Troubleshooting","text":"<p>If postgres pvc is not getting a volume allocated, it probably is due to the iam permissions. Please refer this https://docs.aws.amazon.com/eks/latest/userguide/csi-iam-role.html to assign approriate policies to kubernetes sa</p>"},{"location":"patterns/paralus/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open-source NPM package paralus-eks-blueprints-addon. Please refer to the package npm site for more information. https://www.npmjs.com/package/@paralus/paralus-eks-blueprints-addon</p> <p>If you have any questions about the npm package or find any defect, please post in the source repo at  https://github.com/paralus/eks-blueprints-addon</p>"},{"location":"patterns/pipeline-multi-env-gitops/","title":"Pipeline Multi Environment Pattern","text":""},{"location":"patterns/pipeline-multi-env-gitops/#objective","title":"Objective","text":"<ol> <li>Deploying an EKS cluster across 3 environments( dev, test, and prod ), with a Continuous Deployment pipeline triggered upon a commit to the repository that holds the pipeline configuration.</li> <li>Configuring GitOps tooling (ArgoCD addon) to support multi-team and multi-repositories configuration, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects</li> </ol>"},{"location":"patterns/pipeline-multi-env-gitops/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. The pattern uses the ECSDEMO applications as sample applications to demonstrate how to setup a GitOps configuration with multiple teams and multiple applications. The pattern include the following configurations in terms io:</p> <ol> <li>Application team - it defines 3 application teams that corresponds with the 3 sample applications used</li> <li>ArgoCD bootstrap - the pattern configure the ArgoCD addon to point to the workload repository of the EKS Blueprints samples</li> <li>ArgoCD projects - as part of the ArgoCD addon bootstrap, the pattern generate an ArgoCD project for each application team. The ArgoCD are used in order to restrict the deployment of an application to a specific target namespace</li> </ol> <p>You can find the App of Apps configuration for this pattern in the workload repository under the folder <code>multi-repo</code>.</p>"},{"location":"patterns/pipeline-multi-env-gitops/#prerequisites","title":"Prerequisites","text":"<ol> <li>Fork this repository to your GitHub organisation/user</li> <li>Clone your forked repository</li> <li> <p>Install the AWS CDK Toolkit globally on your machine using</p> <pre><code>npm install -g aws-cdk\n</code></pre> </li> <li> <p><code>github-ssh-key</code> - must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code>. This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support.</p> </li> <li> <p><code>github-token</code> secret must be stored in AWS Secrets Manager for the GitHub pipeline. For more information on how to set it up, please refer to the docs. The GitHub Personal Access Token should have these scopes:</p> </li> <li>repo - to read the repository</li> <li> <p>admin:repo_hook - if you plan to use webhooks (enabled by default)</p> </li> <li> <p>Create the relevant users that will be used by the different teams</p> <pre><code>aws iam create-user --user-name frontend-user\naws iam create-user --user-name nodejs-user\naws iam create-user --user-name crystal-user\naws iam create-user --user-name platform-user\n</code></pre> </li> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>In case you haven't done this before, bootstrap your AWS Account for AWS CDK use using:</p> <pre><code>cdk bootstrap\n</code></pre> </li> <li> <p>Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattenrn file source code and look for the declared const of <code>gitOwner</code>. Change it to your GitHub username.</p> </li> <li> <p>OPTIONAL - As mentioned above, this pattern uses another repository for GitOps. This is the ArgoCD App of Apps configuration that resides in the aws-samples organisation. If you would like to modify the App of Apps configuration and customise it to your needs, then use the following instructions:</p> <ol> <li> <p>Fork the App of Apps workloads repo to your GitHub username</p> </li> <li> <p>Modify the pattern code with the following changes:</p> </li> <li> <p>Change the consts of <code>devArgoAddonConfig</code>, <code>testArgoAddonConfig</code>, and <code>prodArgoAddonConfig</code> to point to your GitHub username</p> </li> <li> <p>In the <code>createArgoAddonConfig</code> function, look for the <code>git@github.com:aws-samples/eks-blueprints-workloads.git</code> code under the <code>sourceRepos</code> configurations, and add another reference to your forked workload repository</p> </li> </ol> </li> </ol>"},{"location":"patterns/pipeline-multi-env-gitops/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> <pre><code>make pattern pipeline-multienv-gitops deploy eks-blueprint-pipeline-stack\n</code></pre> <p>Now you can go to AWS CodePipeline console, and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments.</p>"},{"location":"patterns/pipeline-multi-env-gitops/#notes","title":"Notes","text":"<ol> <li> <p>In case your pipeline fails on the first run, it's because that the AWS CodeBuild step needs elevated permissions at build time. This is described in the official docs. To resolve this, locate <code>AccessDeniedException</code> in the CodeBuild build logs, and attach the following inline policy to it:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"cloudformation:*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> </li> </ol> <p>The above inconvenience has been fixed in the Blueprints framework as well as in the pattern, so please report such cases if you encounter them. This item is left here for reference in case customers modify the pattern to require additional permissions at build time. </p> <ol> <li>This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern. Make sure your account limit for EIP are increased to support additional 9 EIPs (1 per Subnets)</li> </ol>"},{"location":"patterns/secureingresscognito/","title":"Secure Ingress using Cognito Pattern","text":""},{"location":"patterns/secureingresscognito/#objective","title":"Objective","text":"<p>The objective of this pattern is to provide a secure authentication mechanism for customer applications using Amazon Cognito, ALB, and Route53, ensuring that only authorized users can access the application. The Kubecost tool is used as a reference or sample implementation to demonstrate the pattern's capabilities.</p> <p>To achieve this objective, the pattern utilizes Amazon Cognito to provide user authentication for the application's ingress, with ALB's built-in support for user authentication handling routine tasks such as user sign-up, sign-in, and sign-out. In addition to Amazon Cognito, ALB integrates with any OpenID Connect compliant identity provider (IdP) for a single sign-on experience across applications. ACM and Route53 provide SSL/TLS certificates to secure connections to ALB and authenticate users, preventing sensitive information from being intercepted or tampered with during transmission.</p> <p>The pattern also leverages Kubecost to provide real-time cost visibility and analysis for Kubernetes clusters, enabling customers to make informed decisions about resource allocation and utilization. This pattern can be easily adapted and extended to secure ingress for any application, providing a unified and secure solution for user authentication while optimizing costs. By implementing this solution, Amazon EKS customers can have a reliable, scalable, and secure authentication mechanism for their applications, with a cost optimization tool to manage and reduce the costs associated with their Kubernetes clusters.</p>"},{"location":"patterns/secureingresscognito/#architecture","title":"Architecture","text":""},{"location":"patterns/secureingresscognito/#approach","title":"Approach","text":"<p>This blueprint will include the following:</p> <ul> <li>A new Well-Architected VPC with both Public and Private subnets.</li> <li>A new Well-Architected EKS cluster in the region and account you specify.</li> <li>EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.</li> <li>AWS and Kubernetes resources needed to support AWS Load Balancer Controller.</li> <li>Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS.</li> <li>External-DNS allows integration of exposed Kubernetes services and Ingresses with DNS providers</li> <li>Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts</li> <li>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and bootstraping your workloads from public and private Git repositories.</li> <li>Create the necessary Cognito resources like user pool, user pool client, domain, Pre sign-up Lambda trigger and Pre authentication Lambda triggers  etc.., and passed to the Argo CD app of apps pattern from which ingress resources can reference.</li> </ul>"},{"location":"patterns/secureingresscognito/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p>"},{"location":"patterns/secureingresscognito/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/secureingresscognito/#deploy","title":"Deploy","text":"<ol> <li>Let\u2019s start by setting a few environment variables. Change the Region as needed.</li> </ol> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=us-west-2\n</code></pre> <ol> <li>Clone the repository and install dependency packages. This repository contains CDK v2 code written in TypeScript.</li> </ol> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\nnpm i\n</code></pre> <ol> <li>argo-admin-password secret must be defined as plain text (not key/value) in <code>us-west-2</code>  region.</li> </ol> <p><pre><code>aws secretsmanager create-secret --name argo-admin-secret \\\n    --description \"Admin Password for ArgoCD\" \\\n    --secret-string \"password123$\" \\\n    --region \"us-west-2\"\n</code></pre> 4. The CDK code expects the allowed domain and subdomain names in the CDK context file (cdk.json).</p> <p>Create two environment variables. The PARENT_HOSTED_ZONE variable contains your company\u2019s domain name. The DEV_SUBZONE_NAME will be the address for your Kubecost dashboard. </p> <p>Generate the cdk.json file:</p> <pre><code>PARENT_HOSTED_ZONE=mycompany.a2z.com\nDEV_SUBZONE_NAME=dev.mycompany.a2z.com\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"parent.hostedzone.name\": \"${PARENT_HOSTED_ZONE}\",\n        \"dev.subzone.name\": \"${DEV_SUBZONE_NAME}\"\n      }\n}\nEOF\n</code></pre> <ol> <li>In this solution, we\u2019ll allow access to the Kubecost dashboard based on user email addresses. You can control access to the dashboard by allow-listing an entire domain or individual email addresses. </li> </ol> <p>Users are required to sign-up before they can access the Kubecost dashboard. The pre sign-up Lambda trigger only allows sign-ups when user\u2019s email domain matches allow-listed domains. When users sign-up, Cognito sends a verification code to their email address. Users have to verify access (using the one time valid code) to their email before they get access to the dashboard. </p> <p>If you\u2019d like to limit access to the dashboard by email addresses, you can also create a parameter to store allowed email addresses and add a logic to the pre authentication Lambda trigger.</p> <p>Create below parameters with allowed email addresses and domains in the AWS Systems Manager Parameter Store:</p> <pre><code>export SSM_PARAMETER_KEY=\"/secure-ingress-auth-cognito/ALLOWED_DOMAINS\"\nexport SSM_PARAMETER_VALUE=\"emaildomain1.com,emaildomain2.com\"\n\naws ssm put-parameter \\\n  --name \"$SSM_PARAMETER_KEY\" \\\n  --value \"$SSM_PARAMETER_VALUE\" \\\n  --type \"String\" \\\n  --region $AWS_REGION\n</code></pre> <ol> <li>Execute the commands below to bootstrap the AWS environment in <code>us-west-2</code></li> </ol> <pre><code>cdk bootstrap aws://$ACCOUNT_ID/$AWS_REGION\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern secure-ingress-cognito deploy secure-ingress-blueprint\n</code></pre>"},{"location":"patterns/secureingresscognito/#cluster-access","title":"Cluster Access","text":"<p>Once the deploy completes, you will see output in your terminal window similar to the following:</p> <p><pre><code>Outputs:\nsecure-ingress-blueprint.secureingressblueprintClusterNameD6A1BE5C = secure-ingress-blueprint\nsecure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 =  aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\nsecure-ingress-blueprint.secureingressblueprintGetTokenCommand21BE2184 =  aws eks get-token \u2014cluster-name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\n</code></pre> <pre><code>Stack ARN:\narn:aws:cloudformation:us-west-2:&lt;ACCOUNT ID&gt;:stack/secure-ingress-blueprint/64017120-91ce-11ed-93b2-0a67951f5d5d\n</code></pre></p> <p>To update your Kubernetes config for your new cluster, copy and run the secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 command (the second command) in your terminal.</p> <pre><code>aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\n</code></pre> <p>Validate that you now have kubectl access to your cluster via the following:</p> <pre><code>kubectl get all -n kubecost\n</code></pre> <p>You should see output that lists all namespaces in your cluster.</p>"},{"location":"patterns/secureingresscognito/#test-authentication","title":"Test authentication","text":"<p>Point your browser to the URL of the Kubecost app in your cluster. You can get the URL from the cdk.json file using the below command.</p> <pre><code>awk -F':' '/dev.subzone.name/ {print $2}' cdk.json | tr -d '\",' | xargs echo\n</code></pre> <p>Your browser will be redirected to a sign-in page. This page is provided by Amazon Cognito hosted UI.</p> <p>Since this is your first time accessing the application, sign up as a new user. The data you input here will be saved in the Amazon Cognito user pool you created earlier in the post. </p> <p></p> <p>Select \u201cSign up\u201d and use your email address and create a password</p> <p></p> <p></p> <p>Use the verification code received in your email and confirm the account. Once you sign in, ALB will send you to the Kubecost app\u2019s UI:</p> <p></p> <p>Select the \u201cAWS Cluster #1\u201d to view the cost overview, savings and efficiency details.</p> <p></p>"},{"location":"patterns/windows/","title":"Windows Nodes on EKS","text":"<p>We (AWS) have received many requests to add windows node group support from the customers who run their workloads on Windows. Customers want to scale these workloads on Kubernetes alongside their Linux workloads. Amazon EKS supports windows node groups and you can Windows worker node group to an Amazon EKS cluster. This pattern Creates EKS Cluster Control plane with a managed node group running windows node. Please check our AWS doc on Enabling Windows support for your Amazon EKS cluster to learn more about considerations, prerequisites on running windows nodes with EKS cluster. Also please refer to this AWS doc to learn about Amazon EKS optimized Windows AMIs.</p>"},{"location":"patterns/windows/#addons","title":"Addons","text":"<p>Not all of the listed EKS addons support windows. We are currently working on a list of supported addons documentation which will be published here.</p>"},{"location":"patterns/windows/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/windows/#configuration-options","title":"Configuration Options","text":"<p>The pattern exposes the <code>WindowsBuilder</code> construct to build cluster with windows node groups. At the moment, adding windows nodes to the cluster requires at least one linux node group present to deploy core add-ons, such as VPC-CNI and CoreDNS. </p> <p>The <code>WindowsBuilder</code> provides a set of options, most of which are similar to managed node groups. </p> <p>In addition, it provides an attribute <code>noScheduleForWindowsNodes : true | false</code>. When set to <code>true</code> it will automatically add a <code>NoSchedule</code> taint to the Windows nodes. This approach is a safe way to disallow any application that does not provide proper tolerations to be scheduled on Windows nodes. </p> <p>In this scenario, in order to schedule a workload (application/add-on) on Windows nodes, customers can apply the following node selectors and tolerations to their deployments:</p> <pre><code>nodeSelector:\n  kubernetes.io/os: windows\ntolerations:\n  - key: \"os\"\n    operator: \"Equal\"\n    value: \"windows\"\n    effect: \"NoSchedule\"\n</code></pre>"},{"location":"patterns/windows/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy kubeflow pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern windows deploy\n</code></pre>"},{"location":"patterns/windows/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name windows-eks-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/windows-construct-bluepr-windowsconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code>kubectl get nodes -o json | jq -r '.items[] | \"Name: \",.metadata.name,\"\\nInstance Type: \",.metadata.labels.\"beta.kubernetes.io/instance-type\",\"\\nOS Type: \",.metadata.labels.\"beta.kubernetes.io/os\",\"\\n\"' # Output shows Windows and Linux Nodes\n</code></pre>"},{"location":"patterns/windows/#deploy-sample-windows-application","title":"Deploy sample windows application","text":"<p>Create a namespace for the windows app called windows</p> <pre><code>kubectl create ns windows\n</code></pre> <p>Create a yaml file for the app from the configuration below and save it as windows-server-2022.yaml</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: windows-server-iis-ltsc2022\n  namespace: windows\nspec:\n  selector:\n    matchLabels:\n      app: windows-server-iis-ltsc2022\n      tier: backend\n      track: stable\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: windows-server-iis-ltsc2022\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: windows-server-iis-ltsc2022\n        image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2022\n        ports:\n        - name: http\n          containerPort: 80\n        imagePullPolicy: IfNotPresent\n        command:\n        - powershell.exe\n        - -command\n        - \"Add-WindowsFeature Web-Server; Invoke-WebRequest -UseBasicParsing -Uri 'https://dotnetbinaries.blob.core.windows.net/servicemonitor/2.0.1.6/ServiceMonitor.exe' -OutFile 'C:\\\\ServiceMonitor.exe'; echo '&lt;html&gt;&lt;body&gt;&lt;br/&gt;&lt;br/&gt;&lt;H1&gt;Our first pods running on Windows managed node groups! Powered by Windows Server LTSC 2022.&lt;H1&gt;&lt;/body&gt;&lt;html&gt;' &gt; C:\\\\inetpub\\\\wwwroot\\\\iisstart.htm; C:\\\\ServiceMonitor.exe 'w3svc'; \"\n      nodeSelector:\n        kubernetes.io/os: windows\n      tolerations:\n          - key: \"os\"\n            operator: \"Equal\"\n            value: \"windows\"\n            effect: \"NoSchedule\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: windows-server-iis-ltsc2022-service\n  namespace: windows\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: windows-server-iis-ltsc2022\n    tier: backend\n    track: stable\n  sessionAffinity: None\n  type: LoadBalancer\n</code></pre> <p>Deploy the sample app</p> <pre><code>kubectl apply -f windows-server-2022.yaml\n</code></pre> <p>Verify the resources created successfully</p> <pre><code>kubectl get -n windows svc,deploy,pods\n</code></pre>"},{"location":"patterns/windows/#reference","title":"Reference","text":"<p>Please reference our blog on Deploying Amazon EKS Windows managed node groups to learn more about this topic.</p>"},{"location":"patterns/windows/#cleanup","title":"Cleanup","text":"<p>First delete the windows app</p> <pre><code>kubectl delete -f windows-server-2022.yaml\nkubectl delete ns windows\n</code></pre> <p>To clean up your EKS Blueprint, run the following command:</p> <pre><code>make pattern windows destroy\n</code></pre>"},{"location":"patterns/workloads-codecommit/","title":"EKS Cluster with ArgoCD and Workloads in private AWS CodeCommit repository","text":""},{"location":"patterns/workloads-codecommit/#objective","title":"Objective","text":"<p>This example shows how to provision an EKS cluster with:</p> <ul> <li>ArgoCD</li> <li>Workloads deployed by ArgoCD</li> <li>Private AWS CodeCommit repository to store the configurations of workloads</li> <li>Setup to trigger ArgoCD projects sync on git push to AWS CodeCommit repository</li> </ul> <p>Pattern source: /lib/workloads-codecommit-construct/index.ts</p>"},{"location":"patterns/workloads-codecommit/#architecture","title":"Architecture","text":"<p>To better understand how ArgoCD works with EKS Blueprints, read the EKS Blueprints ArgoCD Documentation</p> <ul> <li>After a push to AWS CodeCommit repository notification trigger calls AWS Lambda</li> <li>AWS Lambda calls ArgoCD webhook URL to trigger ArgoCD projects sync</li> </ul>"},{"location":"patterns/workloads-codecommit/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>jq</li> <li><code>make</code></li> </ol>"},{"location":"patterns/workloads-codecommit/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <ol> <li>Update npm</li> </ol> <pre><code>npm install -g npm@latest\n</code></pre> <ol> <li>View patterns and deploy workloads-codecommit pattern</li> </ol> <pre><code>make list\nnpx cdk bootstrap\nmake pattern workloads-codecommit deploy\n</code></pre>"},{"location":"patterns/workloads-codecommit/#verify-the-resources","title":"Verify the resources","text":"<ol> <li>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</li> </ol> <pre><code>aws eks update-kubeconfig --name workloads-codecommit-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/workloads-codecommit-blue-workloadscodecommitbluepr-VH6YOKWPAt5H\n</code></pre> <ol> <li>Verify the resources created from the steps above.</li> </ol> <pre><code>$ kubectl get po -n argocd\nNAME                                          READY   STATUS    RESTARTS   AGE\nblueprints-addon-argocd-application-controller-0                  1/1     Running   0          1h\nblueprints-addon-argocd-applicationset-controller-7b78c7fc5dmkx   1/1     Running   0          1h\nblueprints-addon-argocd-dex-server-6cf94ddc54-p68pl               1/1     Running   0          1h\nblueprints-addon-argocd-notifications-controller-6f6b7d95ckhf6p   1/1     Running   0          1h\nblueprints-addon-argocd-redis-b8dbc7dc6-dvbkr                     1/1     Running   0          1h\nblueprints-addon-argocd-repo-server-66df7f448f-kvwmw              1/1     Running   0          1h\nblueprints-addon-argocd-server-584db5f545-8xp48                   1/1     Running   0          1h\n</code></pre>"},{"location":"patterns/workloads-codecommit/#get-argocd-url-and-credentials","title":"Get ArgoCD Url and credentials","text":"<pre><code>until kubectl get svc blueprints-addon-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname' | grep -m 1 \"elb.amazonaws.com\"; do sleep 5 ; done;\nexport ARGOCD_SERVER=`kubectl get svc blueprints-addon-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`\nexport CC_REPO_NAME=eks-blueprints-workloads-cc\n\necho \"ArgoCD URL: https://$ARGOCD_SERVER\"\necho \"ArgoCD server user: admin\"\necho \"ArgoCD admin password: $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\"\n</code></pre>"},{"location":"patterns/workloads-codecommit/#create-notification-trigger-from-aws-codecommit-push-to-argocd-sync","title":"Create notification trigger from AWS CodeCommit push to ArgoCD Sync","text":"<pre><code>export LAMBDA_ARN=$(aws lambda get-function --function-name eks-blueprints-workloads-cc-webhook | jq -r .Configuration.FunctionArn)\n\ncat &gt; trigger.json &lt;&lt;EOF\n[\n  {\n      \"destinationArn\": \"${LAMBDA_ARN}\",\n      \"branches\": [],\n      \"name\": \"${CC_REPO_NAME}-trigger\",\n      \"customData\": \"${ARGOCD_SERVER}\",\n      \"events\": [\n          \"all\"\n      ]\n  }\n]\nEOF\n\naws codecommit put-repository-triggers --repository-name $CC_REPO_NAME --triggers file://trigger.json --no-cli-pager\nrm trigger.json\n</code></pre>"},{"location":"patterns/workloads-codecommit/#set-aws_region","title":"Set AWS_REGION","text":"<pre><code>export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')\necho $AWS_REGION\n</code></pre>"},{"location":"patterns/workloads-codecommit/#populate-aws-codecommit-with-blueprint-workloads-sample-repository","title":"Populate AWS CodeCommit with Blueprint workloads Sample repository","text":"<pre><code>pushd ..\ngit clone https://github.com/aws-samples/eks-blueprints-workloads.git\ngit clone codecommit::$AWS_REGION://$CC_REPO_NAME\ncd $CC_REPO_NAME\ngit checkout -b main\ncd ..\nrsync -av eks-blueprints-workloads/ $CC_REPO_NAME --exclude .git\ncd $CC_REPO_NAME\ngit add . &amp;&amp; git commit -m \"initial commit\" &amp;&amp; git push --set-upstream origin main\npopd\n</code></pre> <p>ArgoCD will receive notification and will start sync.</p> <p></p>"},{"location":"patterns/workloads-codecommit/#destroy","title":"Destroy","text":"<p>To teardown and remove the resources created in this example:</p> <ol> <li> <p>Delete \"bootstrap-apps\" project in ArgoCD UI and wait until ArgoCD delete workloads</p> </li> <li> <p>Delete deployed resources</p> </li> </ol> <pre><code>cd cdk-eks-blueprints-patterns\nmake pattern workloads-codecommit destroy\n</code></pre> <ol> <li>Delete cloned repositories (<code>if necessary</code>)</li> </ol> <pre><code>pushd ..\nrm -rf eks-blueprints-workloads-cc\nrm -rf eks-blueprints-workloads\npopd\n</code></pre>"},{"location":"patterns/generative-ai/showcase/","title":"Using Gen AI to run a prompt showcase with Bedrock and Amazon EKS","text":""},{"location":"patterns/generative-ai/showcase/#objective","title":"Objective","text":"<p>Amazon Bedrock is a fully managed service for using foundation models. It allows you to access models from Amazon and third parties with a single set of APIs for both text generation and image generation.</p> <p>LangChain provides convenient functions for interacting with Amazon Bedrock's models and related services like vector databases. LangChain offers Python and JavaScript libraries. For this workshop, we will use the Python version of LangChain.</p> <p>Streamlit allows us to quickly create web front ends for our Python code, without needing front-end development skills. Streamlit is great for creating proofs-of-concepts that can be presented to a wide audience of both technical and non-technical people.</p> <p>In this pattern we will demonstrate a prompt showcase use case with Gen AI using Bedrock and Amazon EKS. This usecase will demonstrate a prompt showcase which uses different prompt templates such as Summarization, Sentiment and Recommendation with user input to generate a response using Generative AI. In this model we will running a containerized application on Amazon EKS which integrates with Bedrock to provide required user response.</p>"},{"location":"patterns/generative-ai/showcase/#architecture","title":"Architecture","text":""},{"location":"patterns/generative-ai/showcase/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ul> <li>aws cli (also ensure it is configured)</li> <li>Bedrock is currently in preview. Please make sure your AWS account is enabled to use Bedrock</li> <li>cdk</li> <li>npm</li> <li>tsc</li> <li>make</li> <li>Docker</li> </ul> <p>Let\u2019s start by setting the account and region environment variables:</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=$(aws configure get region)\n</code></pre> <p>Clone the repository:</p> <p><pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns/lib/generative-ai-showcase/python\n</code></pre> Create the ECR image repository and push the docker image to ECR for your showcase app:</p> <pre><code>IMAGE_NAME=bedrock-showcase\nIMAGE_TAG=v2\naws ecr create-repository --repository-name $IMAGE_NAME\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\ndocker build -t $IMAGE_NAME .\ndocker tag bedrock-showcase:latest $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:$IMAGE_TAG\ndocker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:$IMAGE_TAG\ncd ../../../../\n</code></pre>"},{"location":"patterns/generative-ai/showcase/#deployment","title":"Deployment","text":"<p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file:</p> <pre><code>cat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"bedrock.pattern.name\": \"showcase\",\n        \"bedrock.pattern.namespace\": \"bedrock\",\n        \"bedrock.pattern.image.name\": \"${ACCOUNT_ID}.dkr.ecr.$AWS_REGION.amazonaws.com/${IMAGE_NAME}\",\n        \"bedrock.pattern.image.tag\": \"${IMAGE_TAG}\"\n      }\n}\nEOF\n</code></pre> <p>Run the following commands:</p> <p><pre><code>make deps\nmake build\nmake pattern generative-ai-showcase deploy\n</code></pre> When deployment completes, the output will be similar to the following:</p> <pre><code> \u2705  generative-ai-showcase-blueprint\n\n\u2728  Deployment time: 1287.16s\n\nOutputs:\ngenerative-ai-showcase-blueprint.generativeaishowcaseblueprintClusterNameA8D25DA0 = generative-ai-showcase-blueprint\ngenerative-ai-showcase-blueprint.generativeaishowcaseblueprintConfigCommandC6A8442C = aws eks update-kubeconfig --name generative-ai-showcase-blueprint --region us-east-1 --role-arn arn:aws:iam::XXXXXXXXXX:role/generative-ai-showcase-bl-generativeaishowcaseblue-L18IUPGQ8M2I\ngenerative-ai-showcase-blueprint.generativeaishowcaseblueprintGetTokenCommand5AE22878 = aws eks get-token --cluster-name generative-ai-showcase-blueprint --region us-east-1 --role-arn arn:aws:iam::XXXXXXXXXX:role/generative-ai-showcase-bl-generativeaishowcaseblue-L18IUPGQ8M2I\nStack ARN:\narn:aws:cloudformation:us-east-1:XXXXXXXXXX:stack/generative-ai-showcase-blueprint/cd2c4d90-5317-11ee-9c8d-0e69cfd9ba55\n\n\u2728  Total time: 1290.99s\n</code></pre> <p>To see the deployed resources within the cluster, please run:</p> <pre><code>kubectl get pod,svc,secrets,ingress -A\n</code></pre> <p>A sample output is shown below:</p> <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\npod/bedrock-showcase-model-586b558b46-bkwql   1/1     Running   0          60s\n\nNAME                                     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nservice/bedrock-showcase-model-service   NodePort   172.20.12.47   &lt;none&gt;        80:30451/TCP   10m\n\nNAME                                                       CLASS   HOSTS   ADDRESS                                                                 PORTS   AGE\ningress.networking.k8s.io/bedrock-showcase-model-ingress   alb     *       k8s-bedrock-bedrocks-63d6186d4e-765982776.us-east-1.elb.amazonaws.com   80      10m\n</code></pre> <p>Next, Navigate to the URL show under Ingress to see the below screen to interact with Generative AI showcase application by selecting different promptsand inputs and see the result :</p> <p></p>"},{"location":"patterns/generative-ai/showcase/#next-steps","title":"Next steps","text":"<p>You can go AWS Blogs to learn about New Tools for Building with Generative AI on AWS. Also check on another blog our on Enabling Foundation Models to Complete Tasks With Agents for Amazon Bedrock. </p>"},{"location":"patterns/generative-ai/showcase/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern generative-ai-showcase destroy \n</code></pre>"},{"location":"patterns/observability/existing-eks-apiserver-observability/","title":"OSS Apiserver Monitoring","text":""},{"location":"patterns/observability/existing-eks-apiserver-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and adds API server monitoring, with open source managed AWS services.</p>"},{"location":"patterns/observability/existing-eks-apiserver-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/observability/existing-eks-apiserver-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>!!! note For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/existing-eks-apiserver-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/observability/existing-eks-apiserver-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/observability/existing-eks-apiserver-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/observability/existing-eks-awsnative-observability/","title":"AWS Native Observability Pattern","text":""},{"location":"patterns/observability/existing-eks-awsnative-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Native Observability pattern, using AWS native tools such as CloudWatch and Logs and Open Source tools such as AWS Distro for OpenTelemetry (ADOT).</p> <p>[!NOTE] Currently, Xray AddOn is not supported for imported clusters. The Xray AddOn requires access to the nodegroup which is not available with the imported cluster and it does not support IRSA as of now and requires modification of the node instance role. Once it supports IRSA, we would update this pattern to work with the existing clusters.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with AWS services.</p>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-awsnative-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. <pre><code>aws eks update-kubeconfig --name single-new-eks-observability-accelerator --region &lt;your-region&gt; --role-arn arn:aws:iam::**************:role/single-new-eks-observabil-singleneweksobservabilit-CPAN247ASDF\n</code></pre> Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output: <pre><code>NAME                           STATUS   ROLES    AGE   VERSION\nip-10-0-145-216.ec2.internal   Ready    &lt;none&gt;   14m   v1.25.11-eks-a5565ad\n</code></pre></p> <p>Next, lets verify the namespaces in the cluster: <pre><code>kubectl get ns # Output shows all namespace\n</code></pre></p> <p>Output: <pre><code>NAME                       STATUS   AGE\namazon-cloudwatch          Active   5h36m\ncert-manager               Active   5h36m\ndefault                    Active   5h46m\nkube-node-lease            Active   5h46m\nkube-public                Active   5h46m\nkube-system                Active   5h46m\nprometheus-node-exporter   Active   5h36m\n</code></pre></p>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#visualization","title":"Visualization","text":"<ul> <li>Navigate to CloudWatch &gt; Insights &gt; Container Insights and select cluster, select <code>single-new-eks-cluster</code> if you created cluster with pattern mentioned from above guide, otherwise select relevant cluster.</li> <li>Now select <code>amazon-metrics</code> namepsace </li> <li> <p>On a same view, select 'EKS Pods', which provides insights overview of all the pods as shown below </p> </li> <li> <p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p> </li> </ul>"},{"location":"patterns/observability/existing-eks-awsnative-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-awsnative-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/observability/existing-eks-mixed-observability/","title":"Mixed Observability Pattern","text":""},{"location":"patterns/observability/existing-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Mixed Observability pattern, using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer, while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture, AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics, to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/observability/existing-eks-mixed-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with a mixture of AWS native and open source managed AWS services.</p>"},{"location":"patterns/observability/existing-eks-mixed-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/observability/existing-eks-mixed-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit <code>lib/existing-eks-mixed-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/observability/existing-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Cluster AWS Mixed Observability Accelerator.</p>"},{"location":"patterns/observability/existing-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-mixed-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/observability/existing-eks-nginx-observability/","title":"OSS Nginx Monitoring","text":""},{"location":"patterns/observability/existing-eks-nginx-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster NGINX pattern, using Open Source tools such as AWS Distro for OpenTelemetry (ADOT), Amazon Managed Grafana workspace and Prometheus.</p> <p>The current example deploys the AWS Distro for OpenTelemetry Operator for Amazon EKS with its requirements and make use of an existing Amazon Managed Grafana workspace. It creates a new Amazon Managed Service for Prometheus workspace. And You will gain both visibility on the cluster and NGINX based applications.</p>"},{"location":"patterns/observability/existing-eks-nginx-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and NGINX workloads, with open source managed AWS services.</p>"},{"location":"patterns/observability/existing-eks-nginx-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/observability/existing-eks-nginx-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>!!! note For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/existing-eks-nginx-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/existing-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern existing-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=$(kubectl get svc blueprints-addon-nginx-ingress-nginx-controller -n nginx-ingress-sample --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/k8s-deployment-manifest-templates/nginx/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre></p> </li> </ol>"},{"location":"patterns/observability/existing-eks-nginx-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/observability/existing-eks-nginx-observability/#visualization","title":"Visualization","text":"<ol> <li>Prometheus datasource on Grafana</li> <li> <p>After a successful deployment, this will open the Prometheus datasource configuration on Grafana. You should see a notification confirming that the Amazon Managed Service for Prometheus workspace is ready to be used on Grafana.</p> </li> <li> <p>Grafana dashboards</p> </li> <li>Go to the Dashboards panel of your Grafana workspace. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code>.</li> </ol> <p> </p> <ol> <li>Amazon Managed Service for Prometheus rules and alerts</li> <li>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the Rules management tab, you should find new rules deployed.</li> </ol> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"patterns/observability/existing-eks-nginx-observability/#verify-the-resources_1","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/observability/existing-eks-nginx-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/observability/existing-eks-opensource-observability/","title":"OSS Observability Pattern","text":""},{"location":"patterns/observability/existing-eks-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster Open Source Observability pattern, using AWS native tools such as CloudWatch, Amazon Managed Service for Prometheus, Amazon Managed Grafana, and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/observability/existing-eks-opensource-observability/#objective","title":"Objective","text":"<p>Configure the existing Amazon EKS cluster with below Observability components; - AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces - Logs with AWS for FluentBit - Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana. - Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this. - Installs External Secrets Operator to retrieve and Sync the Grafana API keys. - Amazon Managed Grafana Dashboard and data source - Alerts and recording rules with Amazon Managed Service for Prometheus</p>"},{"location":"patterns/observability/existing-eks-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/observability/existing-eks-opensource-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>!!! note For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/existing-eks-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Open Source Observability Accelerator.</p>"},{"location":"patterns/observability/existing-eks-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/","title":"Multi-Cluster Multi-Region Monitoring","text":""},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Multi-Account Multi-Region Mixed Observability (M3) Accelerator using both AWS native tooling such as: CloudWatch ContainerInsights, CloudWatch logs and Open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :</p> <p></p>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#objective","title":"Objective","text":"<ol> <li>Deploying two production grade Amazon EKS cluster with control plane logging across two AWS Accounts (Prod1, Prod2 account) in two different regions through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in another AWS account (pipeline account).</li> <li>Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote-write metrics to AMP workspace in Prod 1 AWS Account.</li> <li>Deploying ADOT add-on, CloudWatch add-on to Prod 2 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account.</li> <li>Configuring GitOps tooling (Argo CD add-on) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using Argo CD projects.</li> <li>Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (mon-account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account.</li> <li>Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.</li> <li>Installing Grafana Operator in Monitoring account (mon-account) to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installing External Secrets Operator in Monitoring account (mon-account) to retrieve and Sync the Grafana API keys.</li> </ol>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#gitops-configuration","title":"GitOps configuration","text":"<ul> <li>For GitOps, this pattern bootstraps Argo CD add-on and points to sample applications in AWS Observability Accelerator.</li> <li>You can find the team-geordie configuration for this pattern in the workload repository under the folder <code>team-geordie</code>.</li> <li>GitOps based management of Amazon Grafana resources (like: Datasources and Dashboards) is achieved using Argo CD application <code>grafana-operator-app</code>. Grafana Operator resources are deployed using <code>grafana-operator-chart</code>.</li> </ul>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure following tools are installed in host machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>argocd</li> <li>jq</li> </ol>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#aws-accounts","title":"AWS Accounts","text":"<ol> <li>AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation, or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO.</li> <li>An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka <code>prodEnv1</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka <code>prodEnv2</code>) provisioned using the AWS Service Catalog Account Factory] product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Pipeline Account (aka <code>pipelineEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka <code>monitoringEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An existing Amazon Managed Grafana Workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. Enable Data sources AWS X-Ray, Amazon Managed Service for Prometheus and Amazon Cloudwatch.</li> <li>If you are bringing new AWS accounts to deploy this pattern, then create a free-tier EC2 instance and let it run for 15-30 minutes in order to complete validation of account.</li> </ol> <p>NOTE: This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created in <code>ProdEnv1</code>, <code>ProdEnv2</code> and <code>monitoringEnv</code> AWS accounts. Make sure your account limits for EIP are increased to support additional 3 EIPs per account.</p>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#clone-repository","title":"Clone Repository","text":"<p>Clone <code>cdk-aws-observability-accelerator</code> repository, if not done already.</p> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\ncd cdk-aws-observability-accelerator\n</code></pre> <p>Pro Tip: This document is compatible to run as Notebook with RUNME for VS Code. There's no need to manually copy and paste commands. You can effortlessly execute them directly from this markdown file. Feel free to give it a try.</p> <p>Here is a sample usage of this document using RUNME:</p> <p></p>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#sso-profile-setup","title":"SSO Profile Setup","text":"<ol> <li>You will be accessing multiple accounts during deployment of this pattern. It is recommended to configure the AWS CLI to authenticate access with AWS IAM Identity Center (successor to AWS Single Sign-On). Let's configure Token provider with automatic authentication refresh for AWS IAM Identity Center. Ensure Prerequisites mentioned here are complete before proceeding to next steps.</li> <li>Create and use AWS IAM Identity Center login with <code>AWSAdministratorAccess</code> Permission set assigned to all AWS accounts required for this pattern (prodEnv1, prodEnv2, pipelineEnv and monitoringEnv).</li> <li>Configure AWS profile with sso for <code>pipelineEnv</code> account:</li> </ol> <pre><code>aws configure sso --profile pipeline-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# SSO start URL [None]: https://d-XXXXXXXXXX.awsapps.com/start\n# SSO region [None]: us-west-2\n# SSO registration scopes [sso:account:access]:sso:account:access\n\n# Attempting to automatically open the SSO authorization page in your default browser.\n# If the browser does not open or you wish to use a different device to authorize this request, open the following URL:\n\n# https://device.sso.us-west-2.amazonaws.com/\n\n# Then enter the code:\n\n# XXXX-XXXX\n# There are 7 AWS accounts available to you.\n# Using the account ID 111122223333\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile pipeline-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv1</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod1-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# There are 7 AWS accounts available to you.\n# Using the account ID 444455556666\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv2</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>monitoringEnv</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile monitoring-account\n</code></pre> <ol> <li>Login to required SSO profile using <code>aws sso login --profile &lt;profile name&gt;</code>. Let's now log in to <code>pipelineEnv</code> account. When SSO login expires, you can use this command to re-login.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\n</code></pre> <ol> <li>Export required environment variables for further use. If not available already, you will be prompted for name of Amazon Grafana workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. And, then its endpoint URL, ID, Role ARN will be captured as environment variables.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\n</code></pre> <ol> <li>Create SSM SecureString Parameter <code>/cdk-accelerator/cdk-context</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This parameter contains account ID and region of all four AWS accounts used in this Observability Accelerator pattern.</li> </ol> <pre><code>aws ssm put-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --type \"SecureString\" \\\n    --overwrite \\\n    --name \"/cdk-accelerator/cdk-context\" \\\n    --description \"AWS account details of different environments used by Multi-Account Multi-Region Mixed Observability (M3) Accelerator pattern\" \\\n    --value '{\n        \"context\": {\n            \"pipelineEnv\": {\n                \"account\": \"'$COA_PIPELINE_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PIPELINE_REGION'\"\n\n            },\n            \"prodEnv1\": {\n                \"account\": \"'$COA_PROD1_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD1_REGION'\"\n            },\n            \"prodEnv2\": {\n                \"account\": \"'$COA_PROD2_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD2_REGION'\"\n            },\n            \"monitoringEnv\": {\n                \"account\": \"'$COA_MON_ACCOUNT_ID'\",\n                \"region\": \"'$COA_MON_REGION'\"\n            }\n        }\n    }'\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#amazon-grafana-configuration","title":"Amazon Grafana Configuration","text":"<ol> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString parameter <code>/cdk-accelerator/amg-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This will be used by CDK for Grafana Operator resources configuration.</p> </li> <li>create Grafana workspace API key.</li> <li>create SSM SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. This will be used by External Secrets Operator.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#github-sources-configuration","title":"GitHub Sources Configuration","text":"<ol> <li> <p>Following GitHub sources used in this pattern:</p> </li> <li> <p>Apps Git Repo - This repository serves as the source for deploying and managing applications in <code>prodEnv1</code> and <code>prodEnv2</code> using GitOps by Argo CD. Here, it is configured to sample-apps from aws-observability-accelerator.</p> </li> <li>Source for CodePipeline - This repository serves as the CodePipeline source stage for retrieving and providing source code to downstream pipeline stages, facilitating automated CI/CD processes. Whenever a change is detected in the source code, the pipeline is initiated automatically. This is achieved using GitHub webhooks. We are using CodePipeline to deploy multi-account multi-region clusters.</li> <li>Source for <code>monitoringEnv</code> Argo CD - This repository serves as the source for deploying and managing applications in the <code>monitoringEnv</code> environment using GitOps by Argo CD. Here, it is configured to grafana-operator-app from aws-observability-accelerator, using which Grafana Datasoures and Dashboards are deployed.</li> </ol> <p>NOTE: Argo CD source repositories used here for <code>prodEnv1</code>, <code>prodEnv2</code> and <code>monitoringEnv</code> are public. If you need to use private repositories, create secret called <code>github-ssh-key</code> in respective accounts and region. This secret should contain your GitHub SSH private key as a JSON structure with fields <code>sshPrivateKey</code> and <code>url</code> in AWS Secrets Manager. Argo CD add-on will use this secret for authentication with private GitHub repositories. For more details on setting up SSH credentials, please refer to Argo CD Secrets Support.</p> <ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your GitHub account.</li> <li> <p>Create GitHub Personal Access Token (PAT) for your CodePipeline GitHub source. For more information on how to set it up, please refer here. The GitHub Personal Access Token should have these scopes:</p> </li> <li> <p>repo - to read the repository</p> </li> <li> <p>admin:repo_hook - to use webhooks</p> </li> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString Parameter <code>/cdk-accelerator/pipeline-git-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account which contains details of CodePipeline source. This parameter contains GitHub owner name where you forked <code>cdk-aws-observability-accelerator</code>, repository name (<code>cdk-aws-observability-accelerator</code>) and branch (<code>main</code>).</p> </li> <li>create AWS Secret Manager secret <code>github-token</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account to hold GitHub Personal Access Token (PAT).</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#deployment","title":"Deployment","text":"<ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your CodePioeline source GitHub organization/user.</li> <li>Install the AWS CDK Toolkit globally on host machine.</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Install project dependencies.</li> </ol> <pre><code>cd `git rev-parse --show-toplevel`\nnpm i\nmake build\n</code></pre> <ol> <li>Bootstrap all 4 AWS accounts using step mentioned for different environment for deploying CDK applications in Deploying Pipelines. If you have bootstrapped earlier, please remove them before proceeding with this step. Remember to set <code>pipelineEnv</code> account number in <code>--trust</code> flag. You can also refer to commands mentioned below:</li> </ol> <pre><code># bootstrap pipelineEnv account WITHOUT explicit trust\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile pipeline-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://${COA_PIPELINE_ACCOUNT_ID}/${COA_PIPELINE_REGION}\n\n# bootstrap prodEnv1 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod1-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD1_ACCOUNT_ID}/${COA_PROD1_REGION}\n\n# bootstrap prodEnv2 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod2-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD2_ACCOUNT_ID}/${COA_PROD2_REGION}\n\n# bootstrap monitoringEnv account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile monitoring-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_MON_ACCOUNT_ID}/${COA_MON_REGION}\n</code></pre> <ol> <li>Once all pre-requisites are set, you are ready to deploy the pipeline. Run the following command from the root of cloned repository to deploy the pipeline stack in <code>pipelineEnv</code> account. This step may require approximately 20 minutes to finish.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\nexport AWS_REGION=${COA_PIPELINE_REGION}\ncd `git rev-parse --show-toplevel`\n\nmake pattern multi-acc-new-eks-mixed-observability deploy multi-account-COA-pipeline\n</code></pre> <ol> <li> <p>Check status of pipeline that deploys multiple Amazon EKS clusters through CloudFromation stacks in respective accounts. This deployment also creates</p> </li> <li> <p><code>ampPrometheusDataSourceRole</code> with permissions to retrieve metrics from AMP in <code>ProdEnv1</code> account,</p> </li> <li><code>cloudwatchDataSourceRole</code> with permissions to retrieve metrics from CloudWatch in <code>ProdEnv2</code> account and</li> <li>Updates Amazon Grafana workspace IAM role in <code>monitoringEnv</code> account to assume roles in <code>ProdEnv1</code> and <code>ProdEnv2</code> accounts for retrieving and visualizing metrics in Grafana</li> </ol> <p>This step may require approximately 50 minutes to finish. You may login to <code>pipelineEnv</code> account and navigate to AWS CodePipeline console at <code>pipelineEnv</code> region to check the status.</p> <pre><code># script to check status of codepipeline\ndots=\"\"; while true; do status=$(aws codepipeline --profile pipeline-account list-pipeline-executions --pipeline-name multi-account-COA-pipeline --query 'pipelineExecutionSummaries[0].status' --output text); [ $status == \"Succeeded\" ] &amp;&amp; echo -e \"Pipeline execution SUCCEEDED.\" &amp;&amp; break || [ \"$status\" == \"Failed\" ] &amp;&amp; echo -e \"Pipeline execution FAILED.\" &amp;&amp; break ||  printf \"\\r\" &amp;&amp; echo -n \"Pipeline execution status: $status$dots\" &amp;&amp; dots+=\".\" &amp;&amp; sleep 10; done\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#post-deployment","title":"Post Deployment","text":"<ol> <li> <p>Once all steps of <code>multi-acc-stages</code> in <code>multi-account-COA-pipeline</code> are complete, run script to</p> </li> <li> <p>create entries in kubeconfig with contexts of newly created EKS clusters.</p> </li> <li>export cluster specific and kubecontext environment variables (like: <code>COA_PROD1_CLUSTER_NAME</code> and <code>COA_PROD1_KUBE_CONTEXT</code>).</li> <li>get Amazon Prometheus Endpoint URL from <code>ProdEnv1</code> account and export to environment variable <code>COA_AMP_ENDPOINT_URL</code>.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/post-deployment-source-envs.sh\n</code></pre> <ol> <li>Then, update parameter <code>AMP_ENDPOINT_URL</code> of Argo CD bootstrap app in <code>monitoringEnv</code> with Amazon Prometheus endpoint URL from <code>ProdEnv1</code> account (<code>COA_AMP_ENDPOINT_URL</code>) and sync Argo CD apps.</li> </ol> <pre><code>if [[ `lsof -i:8080 | wc -l` -eq 0 ]]\nthen\n    export ARGO_SERVER=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\n    export ARGO_PASSWORD=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\n    echo \"ARGO PASSWORD:: \"$ARGO_PASSWORD\n    kubectl --context ${COA_MON_KUBE_CONTEXT} port-forward $ARGO_SERVER -n argocd 8080:443 &gt; /dev/null 2&gt;&amp;1 &amp;\n    argocdPid=$!\n    echo pid: $argocdPid\n    sleep 5s\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} login localhost:8080 --insecure --username admin --password $ARGO_PASSWORD\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app set argocd/bootstrap-apps --helm-set AMP_ENDPOINT_URL=$COA_AMP_ENDPOINT_URL\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app sync argocd/bootstrap-apps\n\n    echo -e '\\033[0;33m' \"\\nConfirm update here.. You should see AMP endpoint URL and no error message.\" '\\033[0m'\n    kubectl --context ${COA_MON_KUBE_CONTEXT} get -n grafana-operator grafanadatasources grafanadatasource-amp -o jsonpath='{.spec.datasource.url}{\"\\n\"}{.status}{\"\\n\"}'\n\n    kill -9 $argocdPid\nelse\n    echo \"Port 8080 is already in use by PID `lsof -i:8080 -t`. Please terminate it and rerun this step.\"\nfi\n</code></pre> <p>NOTE: You can access Argo CD Admin UI using port-forwading. Here are commands to access <code>prodEnv1</code> Argo CD:</p> <pre><code>export PROD1_ARGO_SERVER=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\nexport PROD1_ARGO_PASSWORD=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\necho \"PROD1 ARGO PASSWORD:: \"$PROD1_ARGO_PASSWORD\nnohup kubectl --context ${COA_PROD1_KUBE_CONTEXT} port-forward $PROD1_ARGO_SERVER -n argocd 8081:443 &gt; /dev/null 2&gt;&amp;1 &amp;\nsleep 5\ncurl localhost:8081\n</code></pre> <ol> <li>Datasource <code>grafana-operator-amp-datasource</code> created by Grafana Operator needs to reflect AMP Endpoint URL. There is a limitation with Grafana Operator (or Grafana) which doesn't sync updated <code>grafana-datasources</code> to Grafana. To overcome this issue, we will simply delete Datasource and Grafana Operator syncs up with the latest configuration in 5 minutes. This is achieved using Grafana API and key stored in SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> account.</li> </ol> <pre><code>export COA_AMG_WORKSPACE_URL=$(aws ssm get-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --name \"/cdk-accelerator/amg-info\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text | jq -r \".[] | .workspaceURL\")\n\nexport COA_AMG_API_KEY=$(aws ssm get-parameter --profile monitoring-account --region ${COA_MON_REGION} \\\n    --name \"/cdk-accelerator/grafana-api-key\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text)\n\nexport COA_AMP_DS_ID=$(curl -s -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources \\\n    | jq -r \".[] |  select(.name==\\\"grafana-operator-amp-datasource\\\") | .id\")\n\necho \"Datasource Name:: grafana-operator-amp-datasource\"\necho \"Datasource ID:: \"$COA_AMP_DS_ID\n\ncurl -X DELETE -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources/${COA_AMP_DS_ID}\n</code></pre> <ol> <li>Then, deploy ContainerInsights in <code>ProdEnv2</code> account.</li> </ol> <pre><code>prod2NGRole=$(aws cloudformation describe-stack-resources --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --stack-name \"coa-eks-prod2-${COA_PROD2_REGION}-coa-eks-prod2-${COA_PROD2_REGION}-blueprint\" \\\n    --query \"StackResources[?ResourceType=='AWS::IAM::Role' &amp;&amp; contains(LogicalResourceId,'NodeGroupRole')].PhysicalResourceId\" \\\n    --output text)\n\naws iam attach-role-policy --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name ${prod2NGRole} \\\n    --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n\naws iam list-attached-role-policies --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name $prod2NGRole | grep CloudWatchAgentServerPolicy || echo 'Policy not found'\n\nFluentBitHttpPort='2020'\nFluentBitReadFromHead='Off'\n[[ ${FluentBitReadFromHead} = 'On' ]] &amp;&amp; FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'\n[[ -z ${FluentBitHttpPort} ]] &amp;&amp; FluentBitHttpServer='Off' || FluentBitHttpServer='On'\ncurl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/'${COA_PROD2_CLUSTER_NAME}'/;s/{{region_name}}/'${COA_PROD2_REGION}'/;s/{{http_server_toggle}}/\"'${FluentBitHttpServer}'\"/;s/{{http_server_port}}/\"'${FluentBitHttpPort}'\"/;s/{{read_from_head}}/\"'${FluentBitReadFromHead}'\"/;s/{{read_from_tail}}/\"'${FluentBitReadFromTail}'\"/' | kubectl --context ${COA_PROD2_KUBE_CONTEXT} apply -f -\n</code></pre>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#validating-grafana-dashboards","title":"Validating Grafana Dashboards","text":"<ol> <li>Run the below command in <code>ProdEnv1</code> cluster to generate test traffic to sample application and let us visualize traces to X-Ray and Amazon Managed Grafana Console out the sample <code>ho11y</code> app :</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD1_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD1_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Dashboards &gt; Observability Accelerator Dashboards &gt; Kubernetes / Compute Resources / Namespace (Workloads)</li> </ol> <p>Please also have a look at other Dashboards created using Grafana Operator under folder Observability Accelerator Dashboards.</p> <ol> <li>Run the below command in <code>ProdEnv2</code> cluster to generate test traffic to sample application.</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD2_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD2_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-cloudwatch-datasource &gt; Explore. Set values as highlighted in the snapshot and 'Run query'.</li> </ol> <p></p> <ol> <li>Then, let us look at X-Ray traces in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-xray-datasource &gt; Explore. Set Query Type = Service Map and 'Run query'.</li> </ol> <p></p>"},{"location":"patterns/observability/multi-acc-new-eks-mixed-observability/#clean-up","title":"Clean up","text":"<ol> <li>Run this command to destroy this pattern. This will delete pipeline.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\ncd `git rev-parse --show-toplevel`\n\nsource `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\nmake pattern multi-acc-new-eks-mixed-observability destroy multi-account-COA-pipeline\n</code></pre> <ol> <li>Next, run this script to clean up resources created in respective accounts. This script deletes Argo CD apps, unsets kubeconfig entries, initiates deletion of CloudFormation stacks, secrets, SSM parameters and Amazon Grafana Workspace API key from respective accounts.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/clean-up.sh\n</code></pre> <ol> <li>In certain scenarios, CloudFormation stack deletion might encounter issues when attempting to delete a nodegroup IAM role. In such situations, it's recommended to first delete the relevant IAM role and then proceed with deleting the CloudFormation stack.</li> <li>Delete Dashboards and Data sources in Amazon Grafana.</li> </ol>"},{"location":"patterns/observability/multi-account-monitoring/","title":"Multi Account Open Source Observability Pattern.","text":""},{"location":"patterns/observability/multi-account-monitoring/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Multi Account Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :</p> <p></p>"},{"location":"patterns/observability/multi-account-monitoring/#objective","title":"Objective","text":"<ol> <li> <p>Deploying two production grade Amazon EKS cluster across 2 AWS Accounts ( Prod1, Prod2 account ) through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in an another AWS account (pipeline account).</p> </li> <li> <p>Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote write metrics to AMP workspace in Prod 1 AWS Account. Deploying ADOT add-on, CloudWatch add-on to Prod 1 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account.</p> </li> <li> <p>Configuring GitOps tooling (ArgoCD addon) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects.</p> </li> <li> <p>Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (4th AWS account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account.</p> </li> <li> <p>Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.</p> </li> </ol>"},{"location":"patterns/observability/multi-account-monitoring/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>You can find the team-geordie configuration for this pattern in the workload repository under the folder <code>team-geordie</code>.</p>"},{"location":"patterns/observability/multi-account-monitoring/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation, or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO.</p> </li> <li> <p>An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka prodEnv1) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka prodEnv2) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Pipeline Account (aka pipelineEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka monitoringEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> </ol>"},{"location":"patterns/observability/multi-account-monitoring/#deploying","title":"Deploying","text":"<ol> <li> <p>Fork this repository to your GitHub organisation/user.</p> </li> <li> <p>Clone your forked repository.</p> </li> <li> <p>Set environment variable <code>AWS_REGION</code> with region from where <code>pipelineEnv</code> account will be bootstrapped.</p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\n</code></pre> </li> <li> <p>Install the AWS CDK Toolkit globally on your machine using</p> <pre><code>npm install -g aws-cdk\n</code></pre> </li> <li> <p>Create secret <code>github-ssh-key</code> in <code>AWS_REGION</code> of <code>pipelineEnv</code> account. This secret must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code> in <code>pipelineEnv</code> account. This will be used by ArgoCD addon to authenticate against any GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support.</p> <pre><code>aws secretsmanager create-secret --region $AWS_REGION \\\n--name github-ssh-key \\\n--description \"SSH private key for ArgoCD authentication to GitHub repository\" \\\n--secret-string '{\n    \"sshPrivateKey\":\"&lt;SSH private key&gt;\",\n    \"url\":\"git@github\"\n}'\n</code></pre> </li> <li> <p>Create <code>github-token</code> secret in <code>AWS_REGION</code> of <code>pipelineEnv</code> account. This secret must be stored as a plain text in AWS Secrets Manager for the GitHub pipeline in <code>pipelineEnv</code> account. For more information on how to set it up, please refer to the docs. The GitHub Personal Access Token should have these scopes:</p> <ol> <li> <p>repo - to read the repository</p> </li> <li> <p>admin:repo_hook - if you plan to use webhooks (enabled by default)</p> </li> </ol> <pre><code>aws secretsmanager create-secret --region $AWS_REGION \\\n--name github-token \\\n--description \"GitHub Personal Access Token for CodePipeline to access GitHub account\" \\\n--secret-string \"&lt;GitHub Personal Access Token&gt;\"\n</code></pre> </li> <li> <p>Create secret <code>cdk-context</code> in <code>us-east-1</code> region as a plain text in AWS Secrets Manager for the GitHub pipeline in <code>pipelineEnv</code> account. <code>cdk-context</code> secret must be stored as a plain text in the following format in AWS Secrets Manager for cdk context for all the 4 AWS accounts used by the solution in <code>pipelineEnv</code> account. This secret must be created in <code>us-east-1</code> region.</p> <pre><code>aws secretsmanager create-secret --region us-east-1 \\\n--name cdk-context \\\n--description \"AWS account details of different environments used by Multi account open source Observability pattern\" \\\n--secret-string '{\n\"context\": {\n    \"prodEnv1\": {\n        \"account\": \"&lt;prodEnv1 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"prodEnv2\": {\n        \"account\": \"&lt;prodEnv2 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"pipelineEnv\": {\n        \"account\": \"&lt;pipelineEnv account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"monitoringEnv\": {\n        \"account\": \"&lt;prodmonitoringEnvEnv1 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    }\n}\n}'\n</code></pre> </li> <li> <p>Create the following IAM users and attach <code>administrator</code> policy to required accounts.</p> <ol> <li> <p>IAM user <code>pipeline-admin</code> with <code>administrator</code> policy in Pipeline AWS Account</p> <pre><code>aws iam create-user \\\n[--profile pipelineEnv-admin-profile] \\\n--user-name pipeline-admin\n\naws iam attach-user-policy \\\n[--profile pipelineEnv-admin-profile] \\\n--user-name pipeline-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>prod1-admin</code> with <code>administrator</code> policy in Prod 1 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name prod1-admin\n\naws iam attach-user-policy \\\n[--profile prodEnv1-admin-profile] \\\n--user-name prod1-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>prod2-admin</code> with <code>administrator</code> policy in Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name prod2-admin\n\naws iam attach-user-policy \\\n[--profile prodEnv2-admin-profile] \\\n--user-name prod2-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>mon-admin</code> with <code>administrator</code> policy in Monitoring AWS Account</p> <pre><code>aws iam create-user \\\n[--profile monitoringEnv-admin-profile] \\\n--user-name mon-admin\n\naws iam attach-user-policy \\\n[--profile monitoringEnv-admin-profile] \\\n--user-name mon-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>team-geordi</code> in Prod 1 and Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name team-geordi\n\naws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name team-geordi        \n</code></pre> </li> <li> <p>IAM user <code>team-platform</code> in Prod 1 and Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name team-platform\n\naws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name team-platform     \n</code></pre> </li> </ol> </li> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Bootstrap all 4 AWS accounts using step mentioned for different environment for deploying CDK applications in Deploying Pipelines. If you have bootstrapped earlier, please remove them before proceeding with this step. Remember to set <code>pipelineEnv</code> account number in <code>--trust</code> flag. You can also refer to commands mentioned below:</p> <pre><code># bootstrap prodEnv1 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile prodEnv1-admin-profile] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust &lt;pipelineEnv account number&gt; \\\n    aws://&lt;prodEnv1 account number&gt;/$AWS_REGION\n\n# bootstrap prodEnv2 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile prodEnv2-admin-profile] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust &lt;pipelineEnv account number&gt; \\\n    aws://&lt;prodEnv2 account number&gt;/$AWS_REGION\n\n# bootstrap pipelineEnv account WITHOUT explicit trust \nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile pipelineEnv-admin-profile] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://&lt;pipelineEnv account number&gt;/$AWS_REGION\n\n# bootstrap monitoringEnv account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n    [--profile monitoringEnv-admin-profile] \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust &lt;pipelineEnv account number&gt; \\\n    aws://&lt;monitoringEnv account number&gt;/$AWS_REGION\n</code></pre> </li> <li> <p>Modify the code of <code>lib/pipeline-multi-env-gitops/index.ts</code> and <code>lib/multi-account-monitoring/pipeline.ts</code> in your forked repo to point to your GitHub username/organisation. Look for the declared const of <code>gitOwner</code> and change it to your GitHub username and commit changes to your forked repo. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo.</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack in <code>pipelineEnv</code> account:</p> <pre><code>make build\nmake pattern pipeline-multienv-monitoring deploy multi-account-central-pipeline\n</code></pre> </li> <li> <p>Now you can go to AWS CodePipeline console, and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments. </p> </li> <li> <p>The deployment automation will create <code>ampPrometheusDataSourceRole</code> with permissions to retrieve metrics from AMP in Prod 1 Account, <code>cloudwatchDataSourceRole</code> with permissions to retrieve metrics from CloudWatch in Prod 2 Account and <code>amgWorkspaceIamRole</code> in monitoring account to assume roles in Prod 1 and Prod 2 account for retrieving and visualizing metrics in Grafana.</p> </li> <li> <p>Next, manually follow the following steps from AWS Open Source blog :</p> <ol> <li>AWS SSO in the management account</li> <li>Query metrics in Monitoring account from Amazon Managed Prometheus workspace in Prod 1 Account</li> <li>Query metrics in the Monitoring account from Amazon CloudWatch in Prod 1 Account</li> </ol> </li> </ol> <p></p> <p></p>"},{"location":"patterns/observability/multi-account-monitoring/#validating-custom-metrics-and-traces-from-ho11y-app","title":"Validating Custom Metrics and Traces from ho11y App","text":"<ol> <li> <p>Run the below command in both clusters to generate traces to X-Ray and Amazon Managed Grafana Console out the sample <code>ho11y</code> app :</p> <pre><code>frontend_pod=`kubectl get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> </li> </ol> <p></p> <p></p>"},{"location":"patterns/observability/multi-account-monitoring/#traces-and-service-map-screenshots-from-x-ray-console","title":"Traces and Service Map screenshots from X-Ray Console","text":""},{"location":"patterns/observability/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-amp-as-data-source","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using AMP as data source","text":""},{"location":"patterns/observability/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-cloudwatch-as-data-source","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using CloudWatch as data source","text":""},{"location":"patterns/observability/multi-account-monitoring/#notes","title":"Notes","text":"<p>This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern in Prod 1 and Prod 2 AWS Accounts. Make sure your account limits for EIP are increased to support additional 9 EIPs (1 per Subnets).</p>"},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/","title":"OSS Apiserver Monitoring","text":""},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with API Server monitoring.</p>"},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre>"},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-apiserver-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/","title":"Fargate Observability Pattern","text":""},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Fargate Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. With ADOT support for CloudWatch Container Insights, customers can collect system metrics such as CPU, memory, disk, and network usage from Amazon EKS clusters running on Amazon Elastic Cloud Compute (Amazon EC2), providing the same experience as Amazon CloudWatch agent. In EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet\u2019s cAdvisor metrics for workloads on that node. </p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS Fargate cluster</li> <li>Enables Control Plane Logging. </li> <li>Logs with CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter and Metrics Server for infrastructure metrics.</li> </ul>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-fargate-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-fargate-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>Output:\nNAME                                   STATUS   ROLES    AGE   VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nfargate-ip-10-0-102-84.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.102.84    &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-124-175.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.124.175   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-126-244.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.126.244   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-132-165.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.132.165   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-159-96.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.159.96    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-170-28.ec2.internal    Ready    &lt;none&gt;   14m   v1.27.1-eks-2f008fe   10.0.170.28    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-173-57.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.173.57    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-175-87.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.175.87    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-187-27.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.187.27    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-188-225.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.188.225   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-189-234.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.189.234   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-96-29.ec2.internal     Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.96.29     &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\n</code></pre></p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>NAMESPACE                       NAME                                                   READY   STATUS    RESTARTS       AGE   IP             NODE                                   NOMINATED NODE   READINESS GATES\ncert-manager                    cert-manager-875c7579b-5kzg5                           1/1     Running   0              17m   10.0.188.225   fargate-ip-10-0-188-225.ec2.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-7bb6786867-xrtbx               1/1     Running   0              17m   10.0.102.84    fargate-ip-10-0-102-84.ec2.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-79d574fbd5-9b7mx                  1/1     Running   0              17m   10.0.187.27    fargate-ip-10-0-187-27.ec2.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-cloudwatch-collector-65bb5d7cb6-x8gdl   1/1     Running   1 (114s ago)   14m   10.0.132.165   fargate-ip-10-0-132-165.ec2.internal   &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-xray-collector-796b57b657-tnx86         1/1     Running   0              14m   10.0.124.175   fargate-ip-10-0-124-175.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-6qgfn           1/1     Running   0              17m   10.0.96.29     fargate-ip-10-0-96-29.ec2.internal     &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-dgqn6           1/1     Running   0              17m   10.0.189.234   fargate-ip-10-0-189-234.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-6765c9bc59-v98h5       1/1     Running   0              17m   10.0.175.87    fargate-ip-10-0-175-87.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-7lf2g                               1/1     Running   0              17m   10.0.173.57    fargate-ip-10-0-173-57.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-wn8nc                               1/1     Running   0              17m   10.0.126.244   fargate-ip-10-0-126-244.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-7f4b8b9f5-g994r                     1/1     Running   0              17m   10.0.159.96    fargate-ip-10-0-159-96.ec2.internal    &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-5fbdd4f5f9-lm2nf                2/2     Running   0              16m   10.0.170.28    fargate-ip-10-0-170-28.ec2.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p><pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <pre><code>NAME                       STATUS   AGE\naws-for-fluent-bit              Active   17m\ncert-manager                    Active   17m\ndefault                         Active   27m\nkube-node-lease                 Active   27m\nkube-public                     Active   27m\nkube-system                     Active   27m\nopentelemetry-operator-system   Active   17m\n</code></pre></p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#viewing-logs","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#using-cloudwatch-logs-insights-to-query-logs","title":"Using CloudWatch Logs Insights to Query Logs","text":"<p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select any of the logs that begin with \"/aws/eks/single-new-eks-awsnative-fargate-observability-accelerator\" and run a query.</p> <p>Example with \"kubesystem\" log group:</p> <p></p> <p>Then you can view the results of your query:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#viewing-metrics","title":"Viewing Metrics","text":"<p>Metrics are collected by the cloudWatchAdotAddon as based on the metricsNameSelectors we defined (default <code>['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*']</code>). These metrics can be found in the Cloudwatch metrics dashboard. </p> <p>Navigate to Cloudwatch, then go to \"Metrics\"</p> <p>Select \"All Metrics\" from the dropdown and select any logs in the ContainerInsights namespace</p> <p>Example with \"EKS_Cluster\" metrics</p> <p></p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#monitoring-workloads-on-eks","title":"Monitoring workloads on EKS","text":"<p>Although the default metrics exposed by cloudWatchAdotAddon are useful for getting some standardized metrics from our application we often instrument our own application with OLTP to expose metrics. Fortunately, the otel-collector-cloudwatch-collector can be specified as the endpoint for collecting these metrics and getting metrics and logs to cloudwatch. </p> <p>We will be fetching metrics from <code>ho11y</code> a synthetic signal generator allowing you to test observability solutions for microservices. It emits logs, metrics, and traces in a configurable manner. </p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#deploying-workload","title":"Deploying Workload","text":"<pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ho11y\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/z0a4o2j5/ho11y:latest\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"frontend\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: HO11Y_INJECT_FAILURE\n          value: \"enabled\"\n        - name: DOWNSTREAM0\n          value: \"http://downstream0\"\n        - name: DOWNSTREAM1\n          value: \"http://downstream1\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream0\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream0\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream0\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream0\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://mhausenblas.info/\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream1\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream1\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream1\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://o11y.news/2021-03-01/\"\n        - name: DOWNSTREAM1\n          value: \"DUMMY:187kB:42ms\"\n        - name: DOWNSTREAM2\n          value: \"DUMMY:13kB:2ms\"\n        imagePullPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: frontend\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream0\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream0\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream1\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream1\n---\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>kubectl get pods\nNAME                                                   READY   STATUS    RESTARTS   AGE\ndownstream0-6b665bbfd-6zdsb                            1/1     Running   0          61s\ndownstream1-749d75f6c-9t5dl                            1/1     Running   0          61s\nfrontend-557fd48b4f-gx8ff                              1/1     Running   0          61s\n</code></pre> <p>Once deployed you will be able to monitor the Ho11y metrics in cloudwatch as shown:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-awsnative-fargate-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-fargate-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/","title":"OSS GPU Monitoring","text":"<p>Graphics Processing Units (GPUs) play an integral part in the Machine Learning (ML) workflow, by providing the scalable performance needed for fast ML training and cost-effective ML inference. On top of that, they are used in flexible remote virtual workstations and powerful HPC computations.</p> <p>This pattern shows you how to monitor the performance of the GPUs units, used in an Amazon EKS cluster leveraging GPU-based instances.</p> <p>Amazon Managed Service for Prometheus and Amazon Managed Grafana are open source tools used in this pattern to collect and visualise metrics respectively.</p> <p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale.</p> <p>Amazon Managed Grafana is a managed service for Grafana, a popular open-source analytics platform that enables you to query, visualize, and alert on your metrics, logs, and traces.</p>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#objective","title":"Objective","text":"<p>This pattern deploys an Amazon EKS cluster and a node group that includes instance types featuring NVIDIA GPUs.</p> <p>The AMI type of the node group is <code>AL2_x86_64_GPU AMI</code>, which uses the Amazon EKS-optimized Linux AMI with GPU support. In addition to the standard Amazon EKS-optimized AMI configuration, the GPU AMI includes the NVIDIA drivers.</p> <p>The NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA datacenter GPUs in cluster environments. It includes health monitoring, diagnostics, system alerts and governance policies. GPU metrics are exposed to Amazon Managed Service for Prometheus by the DCGM Exporter, that uses the Go bindings to collect GPU telemetry data from DCGM and then exposes the metrics for Amazon Managed Service for Prometheus to pull from, using an http endpoint (<code>/metrics</code>).</p> <p>The pattern deploys the NVIDIA GPU Operator add-on. The GPU Operator uses the NVIDIA DCGM Exporter to expose GPU telemetry to Amazon Managed Service for Prometheus.</p> <p>Data is visualised in Amazon Managed Grafana by the NVIDIA DCGM Exporter Dashboard.</p> <p>The rest of the setup to collect and visualise metrics with Amazon Managed Service for Prometheus and Amazon Managed Grafana, is similar to that used in other open-source based patterns included in this repository.</p> <p>It also enables control plane logging on the EKS cluster for a comprehensive overview of cluster health.</p>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Amazon Managed Grafana</li> </ol> <p>!!! note     For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning     Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/gpu\"\n        }\n      ]\n    },\n    \"gpuNodeGroup\": {\n      \"instanceType\": \"g4dn.xlarge\",\n      \"desiredSize\": 2, \n      \"minSize\": 2, \n      \"maxSize\": 3,\n      \"ebsSize\": 50\n    },\n  }\n</code></pre> <p>Note: insure your selected instance type is available in your region. To check that, you can run the following command (amend <code>Values</code> below as you see fit):</p> <pre><code>aws ec2 describe-instance-type-offerings \\\n    --filters Name=instance-type,Values=\"g4*\" \\\n    --query \"InstanceTypeOfferings[].InstanceType\" \\\n    --region us-east-2\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-gpu-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksgpuopensourc...\n</code></pre> <p>Let\u2019s verify the resources created by steps above:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Output:</p> <p></p> <p>Next, let's verify that each node has allocatable GPUs:</p> <pre><code>kubectl get nodes  \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n</code></pre> <p>Output:</p> <p></p> <p>We can now deploy the <code>nvidia-smi</code> binary, which shows diagnostic information about all GPUs visible to the container:</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: nvidia-smi\n    image: \"nvidia/cuda:11.0.3-base-ubuntu20.04\"\n    args:\n    - \"nvidia-smi\"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <p>Then request the logs from the Pod:</p> <pre><code>kubectl logs nvidia-smi\n</code></pre> <p>Output:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#grafana-nvidia-dcgm-exporter-dashboard","title":"Grafana NVIDIA DCGM Exporter Dashboard","text":"<p>Login to your Amazon Managed Grafana workspace and navigate to the Dashboards panel. You should see a dashboard named <code>NVIDIA DCGM Exporter Dashboard</code>.</p> <p>We will now generate some load, to see some metrics in the dashboard. Please run the following command from terminal:</p> <pre><code>cat &lt;&lt; EOF | kubectl create -f -\n apiVersion: v1\n kind: Pod\n metadata:\n   name: dcgmproftester\n spec:\n   restartPolicy: OnFailure\n   containers:\n   - name: dcgmproftester11\n     image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04\n     args: [\"--no-dcgm-validation\", \"-t 1004\", \"-d 120\"]\n     resources:\n       limits:\n          nvidia.com/gpu: 1\n     securityContext:\n       capabilities:\n          add: [\"SYS_ADMIN\"]\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <p>Expected output:</p> <p></p> <p>After a few minutes, looking into the <code>NVIDIA DCGM Exporter Dashboard</code>, you should see the gathered metrics, similar to: </p> <p></p> <p></p> <p></p> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/observability/single-new-eks-gpu-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-gpu-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/","title":"AWS Graviton Observability Pattern","text":""},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability on Graviton pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#graviton","title":"Graviton","text":"<p>AWS Graviton Processors are designed by AWS to deliver the best price to performance for your cloud workloads running in Amazon EC2.  These processors are ARM chips running on aarch64 architecture. These processors feature key capabilities, such as the AWS Nitro System, that allow you to securely run cloud native applications at scale.</p> <p>Visit our EKS Blueprints docs for a list of supported addons on Graviton.</p>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster running on a Graviton3 Processor</li> <li>Enables control plane logging</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>!!! note     For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning     Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS Secrets Manager for GRAFANA API KEY: Update the Grafana API key secret in AWS Secrets using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws secretsmanager create-secret \\\n    --name grafana-api-key \\\n    --description \"API Key of your Grafana Instance\" \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION \\\n    --query ARN \\\n    --output text\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-graviton-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-graviton-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-gravitonop-singleneweksgravitonopens-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.27.1-eks-2f008fe   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.aarch64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-graviton-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/observability/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport GO_AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS Secrets Manager using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws secretsmanager update-secret \\\n    --secret-id $API_KEY_SECRET_NAME \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/","title":"OSS Java Monitoring","text":""},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Java based workloads.</p> <p>It also enables control plane logging for comprehensive monitoring on the EKS cluster.</p>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Java workload.</p>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#deploy-an-example-java-application","title":"Deploy an example Java application","text":"<p>In this section we will reuse an example from the AWS OpenTelemetry collector repository. For convenience, the steps can be found below.</p> <ol> <li> <p>Clone this repository and navigate to the <code>sample-apps/jmx/</code> directory.</p> </li> <li> <p>Authenticate to Amazon ECR (AWS_REGION was set during the deployment stage)</p> </li> </ol> <pre><code>export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n</code></pre> <ol> <li>Create an Amazon ECR repository</li> </ol> <pre><code>aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\\n  --image-scanning-configuration scanOnPush=true \\\n  --region $AWS_REGION \n</code></pre> <ol> <li>Build Docker image and push to ECR.</li> </ol> <pre><code>docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest .\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest \n</code></pre> <ol> <li>Install the sample application in the cluster</li> </ol> <pre><code>SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\ncurl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml | \nsed \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" |\nsed \"s/{{region}}/$AWS_REGION/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" | \nkubectl apply -f -\n</code></pre>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                              READY   STATUS    RESTARTS   AGE\ntomcat-bad-traffic-generator      1/1     Running   0          90m\ntomcat-example-77b46cc546-z22jf   1/1     Running   0          25m\ntomcat-traffic-generator          1/1     Running   0          90m\n</code></pre>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>Java/JMX</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Java/JMX</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-java-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-mixed-observability/","title":"Mixed Observability Pattern","text":""},{"location":"patterns/observability/single-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Mixed Observability pattern using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry(ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch as a metric and log aggregation layer while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane logging</li> <li>AWS Distro For OpenTelemetry Operator and Collector configured to collect metrics and traces.</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Aggregate Metrics in CloudWatch</li> <li>Aggregate Traces in X-Ray</li> </ul> <p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-mixed-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-144-134.us-west-1.compute.internal   Ready    &lt;none&gt;   143m   v1.25.9-eks-0a21954   10.0.144.134   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\naws-for-fluent-bit              Active   142m\ncert-manager                    Active   142m\ndefault                         Active   148m\nexternal-secrets                Active   142m\nkube-node-lease                 Active   149m\nkube-public                     Active   149m\nkube-system                     Active   149m\nopentelemetry-operator-system   Active   142m\nprometheus-node-exporter        Active   142m\n</code></pre>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to Metrics -&gt; All Metrics.</p> <p>Select the metrics in the ContainerInsights/Prometheus Namespace:</p> <p></p> <p>View the graph of the selected metrics:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/observability/single-new-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-mixed-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-native/","title":"AWS Native Observability Pattern","text":""},{"location":"patterns/observability/single-new-eks-native/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/observability/single-new-eks-native/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane Logging.</li> <li>AWS Distro For OpenTelemetry Operator and Collector</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter for infrastructure metrics.</li> </ul>"},{"location":"patterns/observability/single-new-eks-native/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-native/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-native/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-JN3QM2KMBNCO\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                       STATUS   AGE\namazon-cloudwatch          Active   5h36m\ncert-manager               Active   5h36m\ndefault                    Active   5h46m\nkube-node-lease            Active   5h46m\nkube-public                Active   5h46m\nkube-system                Active   5h46m\nprometheus-node-exporter   Active   5h36m\n</code></pre>"},{"location":"patterns/observability/single-new-eks-native/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to \"Container Insights\".</p> <p>View the Container Map:</p> <p></p> <p>View the Resource List:</p> <p></p> <p>View the Performance Monitoring Dashboard:</p> <p></p>"},{"location":"patterns/observability/single-new-eks-native/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/observability/single-new-eks-native/#enabling-application-signals-for-your-services","title":"Enabling Application Signals for your services","text":"<p>Amazon CloudWatch Application Signals is a new integrated native APM experience in AWS. CloudWatch Application Signals supports Java and Python applications running on your Amazon EKS cluster.</p> <p>If you haven't enabled Application Signals in this account yet, follow steps 1 - 4 in our AWS documentation.</p> <p>Next, you have to update your Application to <code>Configure application metrics and trace sampling</code>. For this, you must add an annotation to a manifest YAML in your cluster. Adding this annotation auto-instruments the application to send metrics, traces, and logs to Application Signals. You have two options for the annotation:</p> <ol> <li> <p>Annotate Workload auto-instruments a single workload in the cluster.</p> <ul> <li>Paste the below line into the PodTemplate section of the workload manifest. <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    metadata:\n      # add this annotation under the pod template metadata of the services deployment YAML you want to monitor\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n        instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_deployment_yaml</code> to apply the change.</li> </ul> </li> <li> <p>Annotate Namespace auto-instruments all workloads deployed in the selected namespace.</p> <ul> <li>Paste the below line into the metadata section of the namespace manifest. <pre><code>annotations: instrumentation.opentelemetry.io/inject-java: \"true\"\napiVersion: apps/v1\nkind: Namespace\nmetadata:\n    name: &lt;your_namespace&gt;\n    # add this annotation under metadata of the namespace manifest you want to monitor\n    annotations:\n      instrumentation.opentelemetry.io/inject-java: \"true\"\n      instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_namespace_yaml</code> to apply the change.</li> <li>In your terminal, enter a command to restart all pods in the namespace. An example command to restart deployment workloads is <code>kubectl rollout restart deployment -n namespace_name</code></li> </ul> </li> </ol>"},{"location":"patterns/observability/single-new-eks-native/#visualization-of-cloudwatch-application-signals-data","title":"Visualization of CloudWatch Application Signals data","text":"<p>After enabling your Application to pass metrics and traces by following the steps provided above, open your Amazon CloudWatch console in the same region as your EKS cluster, then from the left hand side choose <code>Application Signals -&gt; Services</code> and you will see the metrics shown on the sample dashboard below:</p> <p></p> <p></p>"},{"location":"patterns/observability/single-new-eks-native/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/","title":"OSS Nginx Monitoring","text":""},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Nginx based workloads.</p> <p>It also enables control plane logging to provide comprehensive overview of cluster health.</p>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <p>!! warning This scenario might need larger worker node for the pod. </p> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Nginx workload.</p>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/single-new-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern single-new-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=$(kubectl get svc blueprints-addon-nginx-ingress-nginx-controller -n nginx-ingress-sample --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/k8s-deployment-manifest-templates/nginx/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre></p> </li> </ol>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>NGINX</code>, under <code>Observability Accelerator Dashboards</code>.</p> <p></p>"},{"location":"patterns/observability/single-new-eks-nginx-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-opensource/","title":"OSS Observability Pattern","text":""},{"location":"patterns/observability/single-new-eks-opensource/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/observability/single-new-eks-opensource/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane logging.</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/observability/single-new-eks-opensource/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/observability/single-new-eks-opensource/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>!!! note     For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>!!! warning     Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-opensource/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/observability/single-new-eks-opensource/#visualization","title":"Visualization","text":""},{"location":"patterns/observability/single-new-eks-opensource/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>EKS Scheduler</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>EKS Control Manager</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/observability/single-new-eks-opensource/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/observability/single-new-eks-opensource/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/observability/single-new-eks-opensource/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/observability/single-new-eks-opensource/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport GO_AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION \\\n    --overwrite\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"},{"location":"patterns/security/eks-config-rules/","title":"Security Best Practices for Amazon EKS","text":""},{"location":"patterns/security/eks-config-rules/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable AWS Config configuration recorder and AWS Config managed rules for EKS security best practices in your AWS account <code>CDK_DEFAULT_ACCOUNT</code> and region <code>CDK_DEFAULT_REGION</code> and verify the status of the rules.</p>"},{"location":"patterns/security/eks-config-rules/#prerequisites","title":"Prerequisites","text":"<ol> <li>Follow the usage instructions to install the dependencies and perform the repository setup.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/eks-config-rules/#deploy","title":"Deploy","text":"<p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#deploy-aws-config-configuration-recorder","title":"Deploy AWS Config Configuration Recorder","text":"<p>Use the AWS Config setup blueprints pattern to enable AWS Config in your account and region by running the following command:</p> <pre><code>make pattern eks-config-rules deploy eks-config-setup\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#deploy-config-rules-for-eks-security-best-practices","title":"Deploy Config Rules for EKS Security Best Practices","text":"<p>Now enable the AWS Config managed rules for EKS security best practices by running the following command:</p> <pre><code>make pattern eks-config-rules deploy eks-config-rules-setup\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#verify","title":"Verify","text":""},{"location":"patterns/security/eks-config-rules/#verify-the-status-of-the-aws-config-managed-rules-for-eks-security-best-practices","title":"Verify the status of the AWS Config managed rules for EKS security best practices","text":"<p>Using the following AWS CLI command, get a list of the AWS Config rules with their evaluation status.</p> <pre><code>aws configservice describe-config-rule-evaluation-status\n</code></pre> <p>The output will look something like the following.</p> <pre><code>{\n    \"ConfigRulesEvaluationStatus\": [\n        ...\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksEndpointNoPublicAccess49-37QJEXYZALLB\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-luqz0p\",\n            \"ConfigRuleId\": \"config-rule-luqz0p\",\n            \"LastSuccessfulInvocationTime\": \"2023-05-30T00:33:26.878000+00:00\",\n            \"LastSuccessfulEvaluationTime\": \"2023-05-30T00:33:27.539000+00:00\",\n            \"FirstActivatedTime\": \"2023-05-27T00:32:41.020000+00:00\",\n            \"FirstEvaluationStarted\": true\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksOldestSupportedVersionAD-Z65N0TEQSF96\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-psbc54\",\n            \"ConfigRuleId\": \"config-rule-psbc54\",\n            \"LastSuccessfulInvocationTime\": \"2023-05-27T07:56:05.182000+00:00\",\n            \"LastSuccessfulEvaluationTime\": \"2023-05-27T07:56:07.542000+00:00\",\n            \"FirstActivatedTime\": \"2023-05-25T22:44:21.666000+00:00\",\n            \"FirstEvaluationStarted\": true\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksSecretsEncrypted7566BFCD-HUQX4WXUDEFA\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-kzohng\",\n            \"ConfigRuleId\": \"config-rule-kzohng\",\n            \"LastSuccessfulInvocationTime\": \"2023-05-30T00:33:26.902000+00:00\",\n            \"LastSuccessfulEvaluationTime\": \"2023-05-30T00:33:27.616000+00:00\",\n            \"FirstActivatedTime\": \"2023-05-27T00:32:41.006000+00:00\",\n            \"FirstEvaluationStarted\": true\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksSupportedVersionCDB3159A-1VNH10LGMMJX\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-oaio54\",\n            \"ConfigRuleId\": \"config-rule-oaio54\",\n            \"LastSuccessfulInvocationTime\": \"2023-05-27T07:56:05.223000+00:00\",\n            \"LastSuccessfulEvaluationTime\": \"2023-05-27T07:56:05.420000+00:00\",\n            \"FirstActivatedTime\": \"2023-05-25T22:51:26.563000+00:00\",\n            \"FirstEvaluationStarted\": true\n        }\n        ...\n    ]\n}\n</code></pre> <p>You can search for the EKS specific rules. Make a note of the unique <code>ConfigRuleName</code> of each of the AWS Config rules for EKS security best practices.</p> <p>Using the unique names of the EKS Config rules from your account and region shown after running the previous AWS CLI command, you can verify each EKS Config rule configuration and state using the following AWS CLI command (remember to replace the rule names below with your rule names).</p> <pre><code>aws configservice describe-config-rules --config-rule-names \"eks-config-rules-setup-EksEndpointNoPublicAccess&lt;your rule id&gt;\" \"eks-config-rules-setup-EksOldestSupportedVersion&lt;your rule id&gt;\" \"eks-config-rules-setup-EksSecretsEncrypted&lt;your rule id&gt;\" \"eks-config-rules-set\nup-EksSupportedVersion&lt;your rule id&gt;\"\n</code></pre> <pre><code>{\n    \"ConfigRules\": [\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksEndpointNoPublicAccess49-37QJEXYZALLB\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-luqz0p\",\n            \"ConfigRuleId\": \"config-rule-luqz0p\",\n            \"Source\": {\n                \"Owner\": \"AWS\",\n                \"SourceIdentifier\": \"EKS_ENDPOINT_NO_PUBLIC_ACCESS\"\n            },\n            \"ConfigRuleState\": \"ACTIVE\",\n            \"EvaluationModes\": [\n                {\n                    \"Mode\": \"DETECTIVE\"\n                }\n            ]\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksOldestSupportedVersionAD-Z65N0TEQSF96\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-psbc54\",\n            \"ConfigRuleId\": \"config-rule-psbc54\",\n            \"Source\": {\n                \"Owner\": \"AWS\",\n                \"SourceIdentifier\": \"EKS_CLUSTER_OLDEST_SUPPORTED_VERSION\"\n            },\n            \"InputParameters\": \"{\\\"oldestVersionSupported\\\":\\\"1.25\\\"}\",\n            \"ConfigRuleState\": \"ACTIVE\",\n            \"EvaluationModes\": [\n                {\n                    \"Mode\": \"DETECTIVE\"\n                }\n            ]\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksSecretsEncrypted7566BFCD-HUQX4WXUDEFA\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-kzohng\",\n            \"ConfigRuleId\": \"config-rule-kzohng\",\n            \"Source\": {\n                \"Owner\": \"AWS\",\n                \"SourceIdentifier\": \"EKS_SECRETS_ENCRYPTED\"\n            },\n            \"ConfigRuleState\": \"ACTIVE\",\n            \"EvaluationModes\": [\n                {\n                    \"Mode\": \"DETECTIVE\"\n                }\n            ]\n        },\n        {\n            \"ConfigRuleName\": \"eks-config-rules-setup-EksSupportedVersionCDB3159A-1VNH10LGMMJX\",\n            \"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-oaio54\",\n            \"ConfigRuleId\": \"config-rule-oaio54\",\n            \"Source\": {\n                \"Owner\": \"AWS\",\n                \"SourceIdentifier\": \"EKS_CLUSTER_SUPPORTED_VERSION\"\n            },\n            \"InputParameters\": \"{\\\"oldestVersionSupported\\\":\\\"1.25\\\"}\",\n            \"ConfigRuleState\": \"ACTIVE\",\n            \"EvaluationModes\": [\n                {\n                    \"Mode\": \"DETECTIVE\"\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <p>Note that you can see the parameter value of the rules with required <code>InputParameters</code> (<code>EKS_CLUSTER_OLDEST_SUPPORTED_VERSION</code> and <code>EKS_CLUSTER_OLDEST_SUPPORTED_VERSION</code>), and the <code>ConfigRuleState</code> for each of the rules which is <code>ACTIVE</code>.</p>"},{"location":"patterns/security/encryption-at-rest/","title":"Data at Rest Encryption","text":""},{"location":"patterns/security/encryption-at-rest/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable encryption at rest for EKS cluster using EBS/EFS storage.</p> <p>To achieve this objective, the pattern utilizes EBS CSI Driver Amazon EKS Add-on to enable encryption-at-rest for EBS volumes and EFS CSI Driver Amazon EKS Add-on to enable encryption-at-rest for EFS volumes.</p> <p>The pattern also leverages KMS resource provider to create KMS keys for EBS/EFS encryption-at-rest and EFS File System resource provider to create an encrypted EFS file system.</p>"},{"location":"patterns/security/encryption-at-rest/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ol> <li>team-platform creates a storage class for EBS and EFS volumes.</li> <li>team-data creates a persistent volume claim for EBS and EFS volumes and a pod that mounts the volumes.</li> </ol>"},{"location":"patterns/security/encryption-at-rest/#prerequisites","title":"Prerequisites","text":"<ol> <li>Follow the usage instructions to install the dependencies and perform the repository setup.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/encryption-at-rest/#deploy","title":"Deploy","text":"<p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre> <p>To deploy the pattern, run the following command:</p> <pre><code>make pattern data-at-rest-encryption deploy\n</code></pre>"},{"location":"patterns/security/encryption-at-rest/#verify","title":"Verify","text":"<p>Now you can verify that the EBS and EFS volumes are encrypted.</p>"},{"location":"patterns/security/encryption-at-rest/#ebs","title":"EBS","text":"<p>To list all the PersistentVolumeClaims (PVCs) that exist in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pvc -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE\ngp2-encrypted-pvc   Bound    pvc-78bd070e-8eba-4b01-a378-462bb806beb3   10Gi       RWO            gp2-encrypted   14m\n</code></pre> <p>To describe an Amazon Elastic Block Store (EBS) volume that is associated with a PersistentVolume (PV) in Kubernetes, run the following command (please replace the PVC-IDENTIFIER with the PVC name from the previous step):</p> <pre><code>aws ec2 describe-volumes --region us-east-1 --filters \"Name=tag:kubernetes.io/created-for/pv/name,Values=&lt;PVC-IDENTIFIER&gt;\" --query 'Volumes[*].{VolumeId:VolumeId, Encrypted:Encrypted, KmsKeyId:KmsKeyId}'\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>[\n    {\n        \"VolumeId\": \"vol-09332f96a58e67385\",\n        \"Encrypted\": true,\n        \"KmsKeyId\": \"arn:aws:kms:us-east-1:111122223333:key/a8b9fa0b-955f-4f85-85c1-8f911003390e\"\n    }\n]\n</code></pre>"},{"location":"patterns/security/encryption-at-rest/#efs","title":"EFS","text":"<p>To list all the StorageClasses that are defined in the Kubernetes cluster, run the following command:</p> <pre><code>kubectl get storageclass\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                      PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nefs-encrypted (default)   efs.csi.aws.com         Delete          Immediate              false                  70m\n</code></pre> <p>To retrieve the KMS Key ID parameter of a specific StorageClass named \"efs-encrypted\" in the Kubernetes cluster, run the following command:</p> <pre><code>kubectl get storageclass efs-encrypted -o jsonpath='{.parameters.kmsKeyId}'\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>arn:aws:kms:us-east-1:111222333444:key/19f4f602-dcf3-42a5-8eef-38f2af4b3626%  \n</code></pre> <p>To list all the PersistentVolumeClaims (PVCs) that exist in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pvc -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE\nefs-encrypted-claim   Bound    pvc-06df2640-ae2f-44ae-8d5c-82c72e56a9ae   10Gi       RWX            efs-encrypted   63m\n</code></pre> <p>To list all the pods that are running in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pods -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                 READY   STATUS    RESTARTS   AGE\nefs-encryption-app   1/1     Running   0          63m\n</code></pre> <p>To get detailed information about a PersistentVolumeClaim (PVC) named \"efs-encrypted-claim\" in the \"data\" namespace of the Kubernetes cluster, run the following command:</p> <pre><code>kubectl describe pvc efs-encrypted-claim -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>Name:          efs-encrypted-claim\nNamespace:     data\nStorageClass:  efs-encrypted\nStatus:        Bound\nVolume:        pvc-06df2640-ae2f-44ae-8d5c-82c72e56a9ae\nLabels:        argocd.argoproj.io/instance=team-data\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\n               volume.beta.kubernetes.io/storage-provisioner: efs.csi.aws.com\n               volume.kubernetes.io/storage-provisioner: efs.csi.aws.com\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      10Gi\nAccess Modes:  RWX\nVolumeMode:    Filesystem\nUsed By:       efs-encryption-app\nEvents:        &lt;none&gt;\n</code></pre>"},{"location":"patterns/security/guardduty/","title":"Amazon GuardDuty Protection","text":""},{"location":"patterns/security/guardduty/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable Amazon GuardDuty Detector across your AWS accounts, use GuardDuty optional features, and how to automate notifications via Amazon SNS based on security findings generated by GuardDuty.</p> <p>Supported features:</p> <ul> <li>Foundational data sources - these data sources are enabled by default, no need to mention them in the pattern input</li> <li>EKS Audit Log Monitoring</li> <li>EKS Runtime Monitoring</li> <li>Malware Protection in Amazon GuardDuty</li> <li>GuardDuty RDS Protection</li> <li>Amazon S3 Protection in Amazon GuardDuty</li> </ul> <p>The pattern consists of two components:</p> <ul> <li><code>GuardDutySetupStack</code> - enables GuardDuty Detector for the account. The stack also creates an SNS topic, SNS Subscription, and Amazon EventBridge Rule.</li> <li>A blueprint that deploys a sample GitOps workload that triggers a GuardDuty finding.</li> </ul> <p>The list of optional features is adjustable via the <code>features</code> parameter in the GuardDutySetupStack stack.</p>"},{"location":"patterns/security/guardduty/#gitops-configuration","title":"GitOps configuration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ul> <li><code>team-danger</code> runs a pod in a privileged mode which is a security anti-pattern</li> <li><code>team-danger</code> runs a pod with a malicious file</li> </ul>"},{"location":"patterns/security/guardduty/#prerequisites","title":"Prerequisites","text":"<ol> <li>Follow the usage instructions to install the dependencies and perform the repository setup.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/guardduty/#deploy","title":"Deploy","text":"<p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/guardduty/#deploying-the-guarddutysetupstack-stack","title":"Deploying the <code>GuardDutySetupStack</code> stack","text":"<p>The <code>GuardDutySetupStack</code> stack enables GuardDuty Detector for the account with all the features of your choice enabled.</p> <p>To deploy the stack, run the following command:</p> <pre><code>make pattern guardduty deploy guardduty-setup\n</code></pre>"},{"location":"patterns/security/guardduty/#deploying-the-blueprint-workload","title":"Deploying the blueprint workload","text":"<p>The blueprint deploys a sample GitOps workload that triggers a GuardDuty finding.</p> <p>To deploy the blueprint, run the following command:</p> <pre><code>make pattern guardduty deploy guardduty-blueprint\n</code></pre>"},{"location":"patterns/security/guardduty/#verify","title":"Verify","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found here. Please replace <code>&lt;your cluster name&gt;</code>, <code>&lt;your region&gt;</code>, and <code>&lt;your cluster role arn&gt;</code> with the values from the CDK output message.</p> <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn &lt;your cluster role arn&gt;\n</code></pre>"},{"location":"patterns/security/guardduty/#verifying-that-the-guardduty-detector-is-enabled","title":"Verifying that the GuardDuty detector is enabled","text":"<p>Now you can check that the GuardDuty detector is successfully enabled with all the required data sources.</p> <p>To list all detectors in the region, run the following command:</p> <pre><code>aws guardduty list-detectors --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n    \"DetectorIds\": [\n        \"80c3c03d44819a984b035b000aa9b3da\"\n    ]\n}\n</code></pre> <p>To check the detector's configuration, run the following command (please replace <code>&lt;DETECTOR-ID&gt;</code> with the ID of the detector):</p> <pre><code>aws guardduty get-detector --detector-id &lt;DETECTOR-ID&gt; --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n    \"CreatedAt\": \"2023-04-14T15:55:27.088Z\",\n    \"FindingPublishingFrequency\": \"SIX_HOURS\",\n    \"ServiceRole\": \"arn:aws:iam::123456789012:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty\",\n    \"Status\": \"ENABLED\",\n    \"UpdatedAt\": \"2023-04-14T15:55:27.088Z\",\n    \"DataSources\": {\n        \"CloudTrail\": {\n            \"Status\": \"ENABLED\"\n        },\n        \"DNSLogs\": {\n            \"Status\": \"ENABLED\"\n        },\n        \"FlowLogs\": {\n            \"Status\": \"ENABLED\"\n        },\n        \"S3Logs\": {\n            \"Status\": \"ENABLED\"\n        },\n        \"Kubernetes\": {\n            \"AuditLogs\": {\n                \"Status\": \"ENABLED\"\n            }\n        },\n        \"MalwareProtection\": {\n            \"ScanEc2InstanceWithFindings\": {\n                \"EbsVolumes\": {\n                    \"Status\": \"ENABLED\"\n                }\n            },\n            \"ServiceRole\": \"arn:aws:iam::123456789012:role/aws-service-role/malware-protection.guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDutyMalwareProtection\"\n        }\n    },\n    \"Tags\": {},\n    \"Features\": [\n        {\n            \"Name\": \"CLOUD_TRAIL\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n        },\n        {\n            \"Name\": \"DNS_LOGS\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n        },\n        {\n            \"Name\": \"FLOW_LOGS\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n        },\n        {\n            \"Name\": \"S3_DATA_EVENTS\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n        },\n        {\n            \"Name\": \"EKS_AUDIT_LOGS\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n        },\n        {\n            \"Name\": \"EBS_MALWARE_PROTECTION\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n        },\n        {\n            \"Name\": \"RDS_LOGIN_EVENTS\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n        },\n        {\n            \"Name\": \"EKS_RUNTIME_MONITORING\",\n            \"Status\": \"ENABLED\",\n            \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\",\n            \"AdditionalConfiguration\": [\n                {\n                    \"Name\": \"EKS_ADDON_MANAGEMENT\",\n                    \"Status\": \"ENABLED\",\n                    \"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n                }\n            ]\n        }\n    ]\n}\n ```\n\n### Verifying that the GuardDuty findings are generated\n\nTo list all findings in the region, run the following command (please replace `&lt;DETECTOR-ID&gt;` with the ID of the detector):\n\n```bash\naws guardduty list-findings --detector-id &lt;DETECTOR-ID&gt; --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n    \"FindingIds\": [\n        \"f2c3859c6ca25b3057d13470a992bbd7\"\n    ]\n}\n</code></pre> <p>To check the finding's details, run the following command (please replace <code>&lt;DETECTOR-ID&gt;</code> and <code>&lt;FINDING-ID&gt;</code> with the ID of the detector and the ID of the finding):</p> <pre><code>aws guardduty get-findings --detector-id &lt;DETECTOR-ID&gt; --finding-ids &lt;FINDING-ID&gt; --region us-east-1\n</code></pre> <p>The list of findings contains <code>PrivilegeEscalation:Kubernetes/PrivilegedContainer</code> as expected:</p> <pre><code>{\n    \"Findings\": [\n        {\n            \"AccountId\": \"123456789012\",\n            \"Arn\": \"arn:aws:guardduty:us-east-1:123456789012:detector/94c3858788bc1444ceedab472bab5d7e/finding/f2c3859c6ca25b3057d13470a992bbd7\",\n            \"CreatedAt\": \"2023-03-22T21:28:07.748Z\",\n            \"Description\": \"A privileged container with root level access was launched on EKS Cluster guardduty-blueprint. If this behavior is not expected, it may indicate that your credentials are compromised.\",\n            \"Id\": \"f2c3859c6ca25b3057d13470a992bbd7\",\n            \"Partition\": \"aws\",\n            \"Region\": \"us-east-1\",\n            \"Resource\": {\n                \"EksClusterDetails\": {\n                    \"Name\": \"guardduty-blueprint\",\n                    \"Arn\": \"arn:aws:eks:us-east-1:123456789012:cluster/guardduty-blueprint\",\n                    \"VpcId\": \"vpc-02b68c9ddc1d403ab\",\n                    \"Status\": \"ACTIVE\",\n                    \"Tags\": [],\n                    \"CreatedAt\": \"2023-03-22T15:48:25.752000-05:00\"\n                },\n                \"KubernetesDetails\": {\n                    \"KubernetesUserDetails\": {\n                        \"Username\": \"system:serviceaccount:argocd:argocd-application-controller\",\n                        \"Uid\": \"1871d525-442e-487f-ae60-81336d1ff0cf\",\n                        \"Groups\": [\n                            \"system:serviceaccounts\",\n                            \"system:serviceaccounts:argocd\",\n                            \"system:authenticated\"\n                        ]\n                    },\n                    \"KubernetesWorkloadDetails\": {\n                        \"Name\": \"privileged-pod\",\n                        \"Type\": \"pods\",\n                        \"Uid\": \"33a3c89e-3280-474d-b8cb-fdf03394fc15\",\n                        \"Namespace\": \"argocd\",\n                        \"HostNetwork\": false,\n                        \"Containers\": [\n                            {\n                                \"Name\": \"app\",\n                                \"Image\": \"centos\",\n                                \"ImagePrefix\": \"\",\n                                \"SecurityContext\": {\n                                    \"Privileged\": true\n                                }\n                            }\n                        ]\n                    }\n                },\n                \"ResourceType\": \"EKSCluster\"\n            },\n            \"SchemaVersion\": \"2.0\",\n            \"Service\": {\n                \"Action\": {\n                    \"ActionType\": \"KUBERNETES_API_CALL\",\n                    \"KubernetesApiCallAction\": {\n                        \"RequestUri\": \"/api/v1/namespaces/argocd/pods\",\n                        \"Verb\": \"create\",\n                        \"UserAgent\": \"argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format\",\n                        \"RemoteIpDetails\": {\n                            \"City\": {\n                                \"CityName\": \"UNKNOWN\"\n                            },\n                            \"Country\": {},\n                            \"GeoLocation\": {\n                                \"Lat\": 0.0,\n                                \"Lon\": 0.0\n                            },\n                            \"IpAddressV4\": \"10.0.205.129\",\n                            \"Organization\": {\n                                \"Asn\": \"0\",\n                                \"AsnOrg\": \"UNKNOWN\",\n                                \"Isp\": \"UNKNOWN\",\n                                \"Org\": \"UNKNOWN\"\n                            }\n                        },\n                        \"StatusCode\": 201\n                    }\n                },\n                \"Archived\": false,\n                \"Count\": 1,\n                \"DetectorId\": \"94c3858788bc1444ceedab472bab5d7e\",\n                \"EventFirstSeen\": \"2023-03-22T21:27:18.186Z\",\n                \"EventLastSeen\": \"2023-03-22T21:27:18.630Z\",\n                \"ResourceRole\": \"TARGET\",\n                \"ServiceName\": \"guardduty\",\n                \"AdditionalInfo\": {\n                    \"Value\": \"{}\",\n                    \"Type\": \"default\"\n                }\n            },\n            \"Severity\": 5,\n            \"Title\": \"Privileged container with root level access launched on the EKS Cluster.\",\n            \"Type\": \"PrivilegeEscalation:Kubernetes/PrivilegedContainer\",\n            \"UpdatedAt\": \"2023-03-22T21:28:07.748Z\"\n        }\n    ]\n}\n</code></pre>"},{"location":"patterns/security/guardduty/#verifying-that-the-guardduty-runtime-monitoring-agents-are-automatically-deployed","title":"Verifying that the GuardDuty Runtime Monitoring agents are automatically deployed","text":"<p>To verify that the GuardDuty Runtime Monitoring agents are automatically deployed, run the following command:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>The output should look like this:</p> <pre><code>NAMESPACE          NAME                                                              READY   STATUS    RESTARTS        AGE\namazon-guardduty   aws-guardduty-agent-qrm22                                         1/1     Running   0               25m\nargocd             blueprints-addon-argocd-application-controller-0                  1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-applicationset-controller-7c4c75877579s   1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-dex-server-c6687d84f-q4697                1/1     Running   1 (3m21s ago)   3m25s\nargocd             blueprints-addon-argocd-notifications-controller-7c74f76c5wh4nb   1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-redis-595cc69fff-9985j                    1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-repo-server-7f75c7796c-229c4              1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-server-86867c9dd8-p6qk7                   1/1     Running   0               3m25s\nargocd             privileged-pod                                                    1/1     Running   0               115s\nkube-system        aws-node-4lhp7                                                    1/1     Running   0               26m\nkube-system        coredns-79989457d9-jncrb                                          1/1     Running   0               32m\nkube-system        coredns-79989457d9-l5jcg                                          1/1     Running   0               32m\nkube-system        kube-proxy-hwkwm                                                  1/1     Running   0               26m\n</code></pre> <p>As you can see, the GuardDuty Runtime Monitoring agent is deployed in the <code>amazon-guardduty</code> namespace.</p>"},{"location":"patterns/security/image-scanning/","title":"Amazon ECR Image Scanning","text":""},{"location":"patterns/security/image-scanning/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable and configure Amazon ECR image scanning.</p> <p>The following scanning types are offered:</p> <ul> <li>Enhanced scanning \u2014 Amazon ECR integrates with Amazon Inspector to provide automated, continuous scanning of your repositories. Your container images are scanned for both operating systems and programing language package vulnerabilities. As new vulnerabilities appear, the scan results are updated and Amazon Inspector emits an event to EventBridge to notify you.</li> <li>Basic scanning \u2014 Amazon ECR uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. With basic scanning, you configure your repositories to scan on push or you can perform manual scans and Amazon ECR provides a list of scan findings.</li> </ul> <p>The pattern consists of two components:</p> <ul> <li><code>ImageScanningSetupStack</code> - configures the Amazon ECR image scanning and the ECR automated re-scan duration in Inspector.</li> <li>A blueprint that deploys a sample GitOps workload that pushes images to Amazon ECR and triggers the image scanning.</li> </ul>"},{"location":"patterns/security/image-scanning/#configuration","title":"Configuration","text":"<p>You can configure the following parameters in the ImageScanningSetupStack stack:</p> <ul> <li><code>scanType</code> - The type of scan to perform. Valid values are <code>BASIC</code> and <code>ENHANCED</code>.</li> <li>Enhanced scanning only:</li> <li><code>enhancedContinuousScanDuration</code> - the Amazon ECR automated re-scan duration setting determines how long Amazon Inspector continuously monitors images pushed into repositories. When the number of days from when an image is first pushed exceeds the automated re-scan duration configuration, Amazon Inspector stops monitoring the image. When Amazon Inspector stops monitoring an image, the scan status of the image is changed to inactive with a reason code of expired, and all associated findings for the image are scheduled to be closed. Valid values are <code>LIFETIME</code>, <code>DAYS_30</code>, and <code>DAYS_180</code>.</li> <li><code>enhancedScanRules</code> - the scanning rules.</li> <li>Basic scanning only:</li> <li><code>basicScanRules</code> - the scanning rules.</li> </ul> <p>Please refer to the Amazon ECR Image Scanning documentation for more information and how to use filters.</p>"},{"location":"patterns/security/image-scanning/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ul> <li><code>team-scan</code> pushes a Docker image to Amazon ECR and triggers the image scanning.</li> </ul>"},{"location":"patterns/security/image-scanning/#prerequisites","title":"Prerequisites","text":"<ol> <li>Follow the usage instructions to install the dependencies and perform the repository setup.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/image-scanning/#deploy","title":"Deploy","text":"<p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/image-scanning/#deploying-the-imagescanningsetupstack-stack","title":"Deploying the <code>ImageScanningSetupStack</code> stack","text":"<p>The <code>ImageScanningSetupStack</code> configures the Amazon ECR image scanning and the ECR automated re-scan duration in Inspector.</p> <p>To deploy the stack, run the following command:</p> <pre><code>make pattern ecr-image-scanning deploy image-scanning-setup\n</code></pre>"},{"location":"patterns/security/image-scanning/#deploying-the-blueprint","title":"Deploying the blueprint","text":"<p>The blueprint deploys a sample GitOps workload that pushes images to Amazon ECR and triggers the image scanning.</p> <p>To deploy the blueprint, run the following command:</p> <pre><code>make pattern ecr-image-scanning deploy image-scanning-workload-blueprint\n</code></pre>"},{"location":"patterns/security/image-scanning/#verify","title":"Verify","text":""},{"location":"patterns/security/image-scanning/#verifying-that-the-image-scanning-is-enabled","title":"Verifying that the image scanning is enabled","text":"<p>To verify that the image scanning is enabled at the registry level, run the following command:</p> <pre><code>aws ecr get-registry-scanning-configuration\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n    \"registryId\": \"123456789012\",\n    \"scanningConfiguration\": {\n        \"scanType\": \"ENHANCED\",\n        \"rules\": [\n            {\n                \"scanFrequency\": \"CONTINUOUS_SCAN\",\n                \"repositoryFilters\": [\n                    {\n                        \"filter\": \"prod\",\n                        \"filterType\": \"WILDCARD\"\n                    }\n                ]\n            },\n            {\n                \"scanFrequency\": \"SCAN_ON_PUSH\",\n                \"repositoryFilters\": [\n                    {\n                        \"filter\": \"*\",\n                        \"filterType\": \"WILDCARD\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"patterns/security/image-scanning/#verifying-that-the-image-is-pushed-to-amazon-ecr","title":"Verifying that the image is pushed to Amazon ECR","text":"<p>To verify that the image is pushed to Amazon ECR, run the following command (please replace <code>&lt;REPOSITORY-NAME&gt;</code> with the repository name):</p> <pre><code>aws ecr describe-images --repository-name &lt;REPOSITORY-NAME&gt;\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n    \"imageDetails\": [\n        {\n            \"registryId\": \"123456789012\",\n            \"repositoryName\": \"image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw\",\n            \"imageDigest\": \"sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n            \"imageTags\": [\n                \"latest\"\n            ],\n            \"imageSizeInBytes\": 83520228,\n            \"imagePushedAt\": \"2023-04-17T17:22:33-05:00\",\n            \"imageManifestMediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"artifactMediaType\": \"application/vnd.docker.container.image.v1+json\",\n            \"lastRecordedPullTime\": \"2023-04-17T17:22:33.966000-05:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"patterns/security/image-scanning/#checking-the-image-scanning-findings","title":"Checking the image scanning findings","text":"<p>To check the image scanning findings, run the following command (please replace <code>&lt;REPOSITORY-NAME&gt;</code> with the repository name):</p> <pre><code>aws ecr describe-image-scan-findings --repository-name &lt;REPOSITORY-NAME&gt; --image-id imageTag=latest\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n    \"imageScanFindings\": {\n        \"enhancedFindings\": [\n            {\n                \"awsAccountId\": \"123456789012\",\n                \"description\": \"basic/unit-name.c in systemd prior to 246.15, 247.8, 248.5, and 249.1 has a Memory Allocation with an Excessive Size Value (involving strdupa and alloca for a pathname controlled by a local attacker) that results in an operating system crash.\",\n                \"findingArn\": \"arn:aws:inspector2:us-east-1:123456789012:finding/0407d7719da0fc8a8f44991f0bf524d6\",\n                \"firstObservedAt\": \"2023-04-17T17:40:39.940000-05:00\",\n                \"lastObservedAt\": \"2023-04-17T17:40:39.940000-05:00\",\n                \"packageVulnerabilityDetails\": {\n                    \"cvss\": [\n                        {\n                            \"baseScore\": 4.9,\n                            \"scoringVector\": \"AV:L/AC:L/Au:N/C:N/I:N/A:C\",\n                            \"source\": \"NVD\",\n                            \"version\": \"2.0\"\n                        },\n                        {\n                            \"baseScore\": 5.5,\n                            \"scoringVector\": \"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\",\n                            \"source\": \"NVD\",\n                            \"version\": \"3.1\"\n                        }\n                    ],\n                    \"referenceUrls\": [\n                        \"https://www.debian.org/security/2021/dsa-4942\",\n                        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/42TMJVNYRY65B4QCJICBYOEIVZV3KUYI/\",\n                        \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/2LSDMHAKI4LGFOCSPXNVVSEWQFAVFWR7/\",\n                        \"https://security.gentoo.org/glsa/202107-48\",\n                        \"https://cert-portal.siemens.com/productcert/pdf/ssa-222547.pdf\"\n                    ],\n                    \"relatedVulnerabilities\": [],\n                    \"source\": \"NVD\",\n                    \"sourceUrl\": \"https://nvd.nist.gov/vuln/detail/CVE-2021-33910\",\n                    \"vendorCreatedAt\": \"2021-07-20T14:15:00-05:00\",\n                    \"vendorSeverity\": \"MEDIUM\",\n                    \"vendorUpdatedAt\": \"2022-06-14T06:15:00-05:00\",\n                    \"vulnerabilityId\": \"CVE-2021-33910\",\n                    \"vulnerablePackages\": [\n                        {\n                            \"arch\": \"X86_64\",\n                            \"epoch\": 0,\n                            \"name\": \"systemd-pam\",\n                            \"packageManager\": \"OS\",\n                            \"release\": \"45.el8\",\n                            \"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n                            \"version\": \"239\"\n                        },\n                        {\n                            \"arch\": \"X86_64\",\n                            \"epoch\": 0,\n                            \"name\": \"systemd\",\n                            \"packageManager\": \"OS\",\n                            \"release\": \"45.el8\",\n                            \"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n                            \"version\": \"239\"\n                        },\n                        {\n                            \"arch\": \"X86_64\",\n                            \"epoch\": 0,\n                            \"name\": \"systemd-libs\",\n                            \"packageManager\": \"OS\",\n                            \"release\": \"45.el8\",\n                            \"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n                            \"version\": \"239\"\n                        },\n                        {\n                            \"arch\": \"X86_64\",\n                            \"epoch\": 0,\n                            \"name\": \"systemd-udev\",\n                            \"packageManager\": \"OS\",\n                            \"release\": \"45.el8\",\n                            \"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n                            \"version\": \"239\"\n                        }\n                    ]\n                },\n                \"remediation\": {\n                    \"recommendation\": {\n                        \"text\": \"None Provided\"\n                    }\n                },\n                \"resources\": [\n                    {\n                        \"details\": {\n                            \"awsEcrContainerImage\": {\n                                \"architecture\": \"amd64\",\n                                \"imageHash\": \"sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n                                \"imageTags\": [\n                                    \"latest\"\n                                ],\n                                \"platform\": \"CENTOS_8\",\n                                \"pushedAt\": \"2023-04-17T17:22:33-05:00\",\n                                \"registry\": \"123456789012\",\n                                \"repositoryName\": \"image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw\"\n                            }\n                        },\n                        \"id\": \"arn:aws:ecr:us-east-1:123456789012:repository/image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw/sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n                        \"tags\": {},\n                        \"type\": \"AWS_ECR_CONTAINER_IMAGE\"\n                    }\n                ],\n                \"score\": 5.5,\n                \"scoreDetails\": {\n                    \"cvss\": {\n                        \"adjustments\": [],\n                        \"score\": 5.5,\n                        \"scoreSource\": \"NVD\",\n                        \"scoringVector\": \"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\",\n                        \"version\": \"3.1\"\n                    }\n                },\n                \"severity\": \"MEDIUM\",\n                \"status\": \"ACTIVE\",\n                \"title\": \"CVE-2021-33910 - systemd-pam, systemd and 2 more\",\n                \"type\": \"PACKAGE_VULNERABILITY\",\n                \"updatedAt\": \"2023-04-17T17:40:39.940000-05:00\"\n            },\n        }\n...            \n</code></pre> <p>You can also check the findings in Inspector2.</p> <pre><code>aws inspector2 list-findings\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n    \"findings\": [\n        {\n            \"awsAccountId\": \"123456789012\",\n            \"description\": \"When curl is instructed to get content using the metalink feature, and a user name and password are used to download the metalink XML file, those same credentials are then subsequently passed on to each of the servers from which curl will download or try to download the contents from. Often contrary to the user's expectations and intentions and without telling the user it happened.\",\n            \"exploitAvailable\": \"NO\",\n            \"findingArn\": \"arn:aws:inspector2:us-east-1:123456789012:finding/006e8eac196bf27417099413ce74eb1a\",\n            \"firstObservedAt\": \"2023-04-14T21:03:02.932000-05:00\",\n            \"fixAvailable\": \"YES\",\n            \"inspectorScore\": 5.3,\n            \"inspectorScoreDetails\": {\n                \"adjustedCvss\": {\n                    \"adjustments\": [],\n                    \"cvssSource\": \"NVD\",\n                    \"score\": 5.3,\n                    \"scoreSource\": \"NVD\",\n                    \"scoringVector\": \"CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:N/A:N\",\n                    \"version\": \"3.1\"\n                }\n            },\n            \"lastObservedAt\": \"2023-04-17T13:34:41.687000-05:00\",\n            \"packageVulnerabilityDetails\": {\n                \"cvss\": [\n                    {\n                        \"baseScore\": 2.6,\n                        \"scoringVector\": \"AV:N/AC:H/Au:N/C:P/I:N/A:N\",\n                        \"source\": \"NVD\",\n                        \"version\": \"2.0\"\n                    },\n                    {\n                        \"baseScore\": 5.3,\n                        \"scoringVector\": \"CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:N/A:N\",\n                        \"source\": \"NVD\",\n                        \"version\": \"3.1\"\n                    }\n                ],\n                \"referenceUrls\": [\n                    \"https://hackerone.com/reports/1213181\",\n                    \"https://security.gentoo.org/glsa/202212-01\",\n                    \"https://cert-portal.siemens.com/productcert/pdf/ssa-389290.pdf\",\n                    \"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FRUCW2UVNYUDZF72DQLFQR4PJEC6CF7V/\",\n                    \"https://www.oracle.com/security-alerts/cpuoct2021.html\"\n                ],\n                \"relatedVulnerabilities\": [],\n                \"source\": \"NVD\",\n                \"sourceUrl\": \"https://nvd.nist.gov/vuln/detail/CVE-2021-22923\",\n                \"vendorCreatedAt\": \"2021-08-05T16:15:00-05:00\",\n                \"vendorSeverity\": \"MEDIUM\",\n                \"vendorUpdatedAt\": \"2023-01-05T12:17:00-06:00\",\n                \"vulnerabilityId\": \"CVE-2021-22923\",\n                \"vulnerablePackages\": [\n                    {\n                        \"arch\": \"AARCH64\",\n                        \"epoch\": 0,\n                        \"fixedInVersion\": \"0:7.61.1-18.el8_4.1\",\n                        \"name\": \"curl\",\n                        \"packageManager\": \"OS\",\n                        \"release\": \"18.el8\",\n                        \"remediation\": \"dnf update curl\",\n                        \"sourceLayerHash\": \"sha256:52f9ef134af7dd14738733e567402af86136287d9468978d044780a6435a1193\",\n                        \"version\": \"7.61.1\"\n                    },\n                    {\n                        \"arch\": \"AARCH64\",\n                        \"epoch\": 0,\n                        \"fixedInVersion\": \"0:7.61.1-18.el8_4.1\",\n                        \"name\": \"libcurl-minimal\",\n                        \"packageManager\": \"OS\",\n                        \"release\": \"18.el8\",\n                        \"remediation\": \"dnf update libcurl-minimal\",\n                        \"sourceLayerHash\": \"sha256:52f9ef134af7dd14738733e567402af86136287d9468978d044780a6435a1193\",\n                        \"version\": \"7.61.1\"\n                    }\n                ]\n            },\n            \"remediation\": {\n                \"recommendation\": {\n                    \"text\": \"None Provided\"\n                }\n            },\n            \"resources\": [\n                {\n                    \"details\": {\n                        \"awsEcrContainerImage\": {\n                            \"architecture\": \"arm64\",\n                            \"imageHash\": \"sha256:65a4aad1156d8a0679537cb78519a17eb7142e05a968b26a5361153006224fdc\",\n                            \"imageTags\": [\n                                \"latest\"\n                            ],\n                            \"platform\": \"CENTOS_8\",\n                            \"pushedAt\": \"2023-04-17T13:34:34-05:00\",\n                            \"registry\": \"123456789012\",\n                            \"repositoryName\": \"cdk-hnb659fds-container-assets-123456789012-us-east-1\"\n                        }\n                    },\n                    \"id\": \"arn:aws:ecr:us-east-1:123456789012:repository/cdk-hnb659fds-container-assets-123456789012-us-east-1/sha256:65a4aad1156d8a0679537cb78519a17eb7142e05a968b26a5361153006224fdc\",\n                    \"partition\": \"aws\",\n                    \"region\": \"us-east-1\",\n                    \"tags\": {},\n                    \"type\": \"AWS_ECR_CONTAINER_IMAGE\"\n                }\n            ],\n            \"severity\": \"MEDIUM\",\n            \"status\": \"CLOSED\",\n            \"title\": \"CVE-2021-22923 - curl, libcurl-minimal\",\n            \"type\": \"PACKAGE_VULNERABILITY\",\n            \"updatedAt\": \"2023-04-17T13:36:44.258000-05:00\"\n        },\n...\n</code></pre>"},{"location":"patterns/security/securityhub/","title":"AWS Security Hub Monitoring","text":""},{"location":"patterns/security/securityhub/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable AWS Security Hub and default security standards in your AWS account, verify that it is enabled, and get findings from AWS Security Hub.</p> <p>The pattern will enable AWS Security Hub in the <code>CDK_DEFAULT_ACCOUNT</code> and <code>CDK_DEFAULT_REGION</code>.</p>"},{"location":"patterns/security/securityhub/#prerequisites","title":"Prerequisites","text":"<ol> <li>Follow the usage instructions to install the dependencies and perform the repository setup.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> <li>Complete the steps to enable AWS Config and deploy the Security Best Practices for Amazon EKS AWS Config managed rules.</li> </ol> <p>Optional (but recommended):  If you have not done so already, follow the steps to deploy the Amazon GuardDuty stack and blueprint. Since Amazon GuardDuty automatically sends its findings to AWS Security Hub, the sample EKS finding will appear in AWS Security Hub about five minutes after it has been enabled in the same region.</p>"},{"location":"patterns/security/securityhub/#deploy","title":"Deploy","text":"<p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/securityhub/#deploy-aws-security-hub","title":"Deploy AWS Security Hub","text":"<p>To enable AWS Security Hub in the account and region deploy the stack, run the following command.</p> <pre><code>make pattern securityhub deploy securityhub-setup\n</code></pre> <p>Once deployed, AWS Security Hub will automatically enable all controls that are part of the default security standards. Currently, the default security standards that are automatically enabled are AWS Foundational Security Best Practices and the Center for Internet Security (CIS) AWS Foundations Benchmark v1.2.0.</p>"},{"location":"patterns/security/securityhub/#verify","title":"Verify","text":""},{"location":"patterns/security/securityhub/#verify-that-aws-security-hub-is-enabled","title":"Verify that AWS Security Hub is enabled","text":"<p>Now you can check that AWS Security Hub is successfully enabled by using the AWS CLI to query the same account and region.</p> <p>Using the AWS CLI run following command in the same account and region where you deployed the stack.</p> <pre><code>aws securityhub describe-hub\n</code></pre> <p>If you successfully enabled AWS Security Hub, you will see the following.</p> <pre><code>{\n    \"HubArn\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXXX:hub/default\",\n    \"SubscribedAt\": \"2021-08-18T00:52:40.624Z\",\n    \"AutoEnableControls\": true,\n    \"ControlFindingGenerator\": \"SECURITY_CONTROL\"\n}\n</code></pre>"},{"location":"patterns/security/securityhub/#view-findings-in-aws-security-hub","title":"View findings in AWS Security Hub","text":"<p>Use the following AWS CLI commands to view your findings in AWS Security Hub.</p> <p>To list critical findings and findings related to controls that have a failed status according to the enabled AWS Security Hub security standards in the same account and region, run the following command:</p> <pre><code>aws securityhub get-findings --filter 'SeverityLabel={Value=CRITICAL,Comparison=EQUALS},ComplianceStatus={Value=FAILED,Comparison=EQUALS}'\n</code></pre> <p>Below is an example of an IAM finding that relates to a failed IAM control that AWS Security Hub found related to the enabled security standards, and will likely be present in your list of findings if you or your organization are not using a hardware MFA device for your AWS root account:</p> <pre><code>{\n    \"Findings\": [\n        {\n            \"SchemaVersion\": \"2018-10-08\",\n            \"Id\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/IAM.6/finding/494ffa38-0b6e-46d1-98f4-e605ec09d045\",\n            \"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub\",\n            \"ProductName\": \"Security Hub\",\n            \"CompanyName\": \"AWS\",\n            \"Region\": \"us-east-1\",\n            \"GeneratorId\": \"security-control/IAM.6\",\n            \"AwsAccountId\": \"XXXXXXXXXXX\",\n            \"Types\": [\n                \"Software and Configuration Checks/Industry and Regulatory Standards\"\n            ],\n            \"FirstObservedAt\": \"2023-03-04T00:54:44.307Z\",\n            \"LastObservedAt\": \"2023-05-31T01:20:18.210Z\",\n            \"CreatedAt\": \"2023-03-04T00:54:44.307Z\",\n            \"UpdatedAt\": \"2023-05-31T01:20:05.845Z\",\n            \"Severity\": {\n                \"Label\": \"CRITICAL\",\n                \"Normalized\": 90,\n                \"Original\": \"CRITICAL\"\n            },\n            \"Title\": \"Hardware MFA should be enabled for the root user\",\n            \"Description\": \"This AWS control checks whether your AWS account is enabled to use a hardware multi-factor authentication (MFA) device to sign in with root user credentials.\",\n            \"Remediation\": {\n                \"Recommendation\": {\n                    \"Text\": \"For information on how to correct this issue, consult the AWS Security Hub controls documentation.\",\n                    \"Url\": \"https://docs.aws.amazon.com/console/securityhub/IAM.6/remediation\"\n                }\n            },\n            \"ProductFields\": {\n                \"RelatedAWSResources:0/name\": \"securityhub-root-account-hardware-mfa-enabled-24e3b344\",\n                \"RelatedAWSResources:0/type\": \"AWS::Config::ConfigRule\",\n                \"aws/securityhub/ProductName\": \"Security Hub\",\n                \"aws/securityhub/CompanyName\": \"AWS\",\n                \"Resources:0/Id\": \"arn:aws:iam::XXXXXXXXXXX:root\",\n                \"aws/securityhub/FindingId\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub/arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/IAM.6/finding/494ffa38-0b6e-46d1-98f4-e605ec09d045\"\n            },\n            \"Resources\": [\n                {\n                    \"Type\": \"AwsAccount\",\n                    \"Id\": \"AWS::::Account:XXXXXXXXXXX\",\n                    \"Partition\": \"aws\",\n                    \"Region\": \"us-east-1\"\n                }\n            ],\n            \"Compliance\": {\n                \"Status\": \"FAILED\",\n                \"RelatedRequirements\": [\n                    \"CIS AWS Foundations Benchmark v1.2.0/1.14\",\n                    \"CIS AWS Foundations Benchmark v1.4.0/1.6\",\n                    \"NIST.800-53.r5 AC-2(1)\",\n                    \"NIST.800-53.r5 AC-3(15)\",\n                    \"NIST.800-53.r5 IA-2(1)\",\n                    \"NIST.800-53.r5 IA-2(2)\",\n                    \"NIST.800-53.r5 IA-2(6)\",\n                    \"NIST.800-53.r5 IA-2(8)\",\n                    \"PCI DSS v3.2.1/8.3.1\"\n                ],\n                \"SecurityControlId\": \"IAM.6\",\n                \"AssociatedStandards\": [\n                    {\n                        \"StandardsId\": \"ruleset/cis-aws-foundations-benchmark/v/1.2.0\"\n                    },\n                    {\n                        \"StandardsId\": \"standards/aws-foundational-security-best-practices/v/1.0.0\"\n                    },\n                    {\n                        \"StandardsId\": \"standards/cis-aws-foundations-benchmark/v/1.4.0\"\n                    },\n                    {\n                        \"StandardsId\": \"standards/nist-800-53/v/5.0.0\"\n                    },\n                    {\n                        \"StandardsId\": \"standards/pci-dss/v/3.2.1\"\n                    }\n                ]\n            },\n            \"WorkflowState\": \"NEW\",\n            \"Workflow\": {\n                \"Status\": \"NEW\"\n            },\n            \"RecordState\": \"ACTIVE\",\n            \"FindingProviderFields\": {\n                \"Severity\": {\n                    \"Label\": \"CRITICAL\",\n                    \"Original\": \"CRITICAL\"\n                },\n                \"Types\": [\n                    \"Software and Configuration Checks/Industry and Regulatory Standards\"\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <p>To search for findings related to the Security Best Practices for Amazon EKS Config managed rules, run the following AWS CLI command:</p> <pre><code>aws securityhub get-findings --filters 'GeneratorId={Value=\"security-control/EKS.1\", Comparison=\"EQUALS\"}'\n</code></pre> <p>You might see a finding such as the following:</p> <pre><code>{\n    \"Findings\": [\n        {\n            \"SchemaVersion\": \"2018-10-08\",\n            \"Id\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/EKS.1/finding/931a06d9-1b1d-431b-8b91-1ff86829b400\",\n            \"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub\",\n            \"ProductName\": \"Security Hub\",\n            \"CompanyName\": \"AWS\",\n            \"Region\": \"us-east-1\",\n            \"GeneratorId\": \"security-control/EKS.1\",\n            \"AwsAccountId\": \"XXXXXXXXXXX\",\n            \"Types\": [\n                \"Software and Configuration Checks/Industry and Regulatory Standards\"\n            ],\n            \"FirstObservedAt\": \"2023-05-09T10:34:36.736Z\",\n            \"LastObservedAt\": \"2023-05-30T10:27:41.205Z\",\n            \"CreatedAt\": \"2023-05-09T10:34:36.736Z\",\n            \"UpdatedAt\": \"2023-05-30T10:27:34.574Z\",\n            \"Severity\": {\n                \"Label\": \"HIGH\",\n                \"Normalized\": 70,\n                \"Original\": \"HIGH\"\n            },\n            \"Title\": \"EKS cluster endpoints should not be publicly accessible\",\n            \"Description\": \"This control checks whether an Amazon EKS cluster endpoint is publicly accessible. The control fails if an EKS cluster has an endpoint that is publicly accessible.\",\n            \"Remediation\": {\n                \"Recommendation\": {\n                    \"Text\": \"For information on how to correct this issue, consult the AWS Security Hub controls documentation.\",\n                    \"Url\": \"https://docs.aws.amazon.com/console/securityhub/EKS.1/remediation\"\n                }\n            },\n            \"ProductFields\": {\n                \"RelatedAWSResources:0/name\": \"securityhub-eks-endpoint-no-public-access-f5aecad6\",\n                \"RelatedAWSResources:0/type\": \"AWS::Config::ConfigRule\",\n                \"aws/securityhub/ProductName\": \"Security Hub\",\n                \"aws/securityhub/CompanyName\": \"AWS\",\n                \"aws/securityhub/annotation\": \"Cluster Endpoint of starter-blueprint is Publicly accessible\",\n                \"Resources:0/Id\": \"arn:aws:eks:us-east-1:XXXXXXXXXXX:cluster/starter-blueprint\",\n                \"aws/securityhub/FindingId\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub/arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/EKS.1/finding/931a06d9-1b1d-431b-8b91-1ff86829b400\"\n            },\n            \"Resources\": [\n                {\n                    \"Type\": \"AwsEksCluster\",\n                    \"Id\": \"arn:aws:eks:us-east-1:XXXXXXXXXXX:cluster/starter-blueprint\",\n                    \"Partition\": \"aws\",\n                    \"Region\": \"us-east-1\"\n                }\n            ],\n            \"Compliance\": {\n                \"Status\": \"FAILED\",\n                \"RelatedRequirements\": [\n                    \"NIST.800-53.r5 AC-21\",\n                    \"NIST.800-53.r5 AC-3\",\n                    \"NIST.800-53.r5 AC-3(7)\",\n                    \"NIST.800-53.r5 AC-4\",\n                    \"NIST.800-53.r5 AC-4(21)\",\n                    \"NIST.800-53.r5 AC-6\",\n                    \"NIST.800-53.r5 SC-7\",\n                    \"NIST.800-53.r5 SC-7(11)\",\n                    \"NIST.800-53.r5 SC-7(16)\",\n                    \"NIST.800-53.r5 SC-7(20)\",\n                    \"NIST.800-53.r5 SC-7(21)\",\n                    \"NIST.800-53.r5 SC-7(3)\",\n                    \"NIST.800-53.r5 SC-7(4)\",\n                    \"NIST.800-53.r5 SC-7(9)\"\n                ],\n                \"SecurityControlId\": \"EKS.1\",\n                \"AssociatedStandards\": [\n                    {\n                        \"StandardsId\": \"standards/aws-foundational-security-best-practices/v/1.0.0\"\n                    },\n                    {\n                        \"StandardsId\": \"standards/nist-800-53/v/5.0.0\"\n                    }\n                ]\n            },\n            \"WorkflowState\": \"NEW\",\n            \"Workflow\": {\n                \"Status\": \"NEW\"\n            },\n            \"RecordState\": \"ACTIVE\",\n            \"FindingProviderFields\": {\n                \"Severity\": {\n                    \"Label\": \"HIGH\",\n                    \"Original\": \"HIGH\"\n                },\n                \"Types\": [\n                    \"Software and Configuration Checks/Industry and Regulatory Standards\"\n                ]\n            }\n        }\n\n    ]\n}\n</code></pre> <p>To see any findings generated by Amazon GuardDuty in AWS Security Hub, run the following command:</p> <pre><code>aws securityhub get-findings --filters 'ProductName={Value=\"GuardDuty\",Comparison=\"EQUALS\"}'\n</code></pre> <pre><code>{\n    \"Findings\": [\n        {\n            \"SchemaVersion\": \"2018-10-08\",\n            \"Id\": \"arn:aws:guardduty:us-east-1:XXXXXXXXXXX:detector/68b6db88cfef1e59333ecbccd8e816b5/finding/0ec437473c147f649d1437f94d615224\",\n            \"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/guardduty\",\n            \"ProductName\": \"GuardDuty\",\n            \"CompanyName\": \"Amazon\",\n            \"Region\": \"us-east-1\",\n            \"GeneratorId\": \"arn:aws:guardduty:us-east-1:XXXXXXXXXXX:detector/68b6db88cfef1e59333ecbccd8e816b5\",\n            \"AwsAccountId\": \"XXXXXXXXXXX\",\n            \"Types\": [\n                \"TTPs/PrivilegeEscalation/PrivilegeEscalation:Kubernetes-PrivilegedContainer\"\n            ],\n            ...\n            \"Severity\": {\n                \"Product\": 5,\n                \"Label\": \"MEDIUM\",\n                \"Normalized\": 50\n            },\n            \"Title\": \"Privileged container with root level access launched on the EKS Cluster.\",\n            \"Description\": \"A privileged container with root level access was launched on EKS Cluster guardduty-blueprint. If this behavior is not expected, it may indicate that your credentials are compromised.\",\n            \"SourceUrl\": \"https://us-east-1.console.aws.amazon.com/guardduty/home?region=us-east-1#/findings?macros=current&amp;fId=0ec437473c147f649d1437f94d615224\",\n            \"ProductFields\": {\n                ...\n            },\n            \"Resources\": [\n                { ... }\n            ],\n            \"WorkflowState\": \"NEW\",\n            \"Workflow\": {\n                \"Status\": \"NEW\"\n            },\n            \"RecordState\": \"ACTIVE\",\n            \"FindingProviderFields\": {\n                \"Severity\": {\n                    \"Label\": \"MEDIUM\"\n                },\n                \"Types\": [\n                    \"TTPs/PrivilegeEscalation/PrivilegeEscalation:Kubernetes-PrivilegedContainer\"\n                ]\n            },\n            \"Sample\": false\n        }\n    ]\n}\n</code></pre> <p>If you deployed the Amazon GuardDuty Protection EKS Blueprints pattern to the same account and region where you enabled AWS Security Hub, you should see Amazon GuardDuty findings like the one above. The sample workload deployed with the Amazon GuardDuty pattern which contains a privileged container is detected by Amazon GuardDuty and generates the <code>Kubernetes-PrivilegedContainer</code> finding. Amazon GuardDuty automatically sent this finding to AWS Security Hub where it can be viewed and triaged.</p>"}]}